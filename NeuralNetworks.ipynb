{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural Networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with the Kaggle Titanic dataset. The dataset is a list of Titanic passengers with features such as class, age, sex and fare. We'll use these features to train a model, and use the model to predict whether or not each passenger survived. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from time import time\n",
    "\n",
    "np.random.seed(1337)\n",
    "\n",
    "df = pd.read_csv('titanicNN.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Class</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>7.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>71.2833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>7.9250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>53.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>8.0500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Class  Sex   Age     Fare\n",
       "0         0      3    1  22.0   7.2500\n",
       "1         1      1    0  38.0  71.2833\n",
       "2         1      3    0  26.0   7.9250\n",
       "3         1      1    0  35.0  53.1000\n",
       "4         0      3    1  35.0   8.0500"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit_transform?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data 80-20 into training and test sets. In addition, we scale the data so that each column has mean 0 and standard deviation 1, and create one-hot vectors with the labels (analogous to dummy variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.iloc[:712, :]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features = ['Class', 'Sex', 'Age', 'Fare']\n",
    "\n",
    "X_train = scaler.fit_transform(df_train[features].values)\n",
    "y_train = df_train['Survived'].values\n",
    "y_train_onehot = pd.get_dummies(df_train['Survived']).values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.83290956,  0.74926865, -0.61259594, -0.51933199],\n",
       "       [-1.55353553, -1.33463478,  0.6184268 ,  0.79718222],\n",
       "       [ 0.83290956, -1.33463478, -0.30484025, -0.5054541 ],\n",
       "       [-1.55353553, -1.33463478,  0.38761004,  0.42333654],\n",
       "       [ 0.83290956,  0.74926865,  0.38761004, -0.50288412]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_onehot[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df.iloc[712:, :]\n",
    "\n",
    "X_test = scaler.transform(df_test[features].values)\n",
    "y_test = df_test['Survived'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " ## Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a basis for comparison, we train a Random Forest model and record the accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForestClassifier?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 10\n",
      "building tree 2 of 10\n",
      "building tree 3 of 10\n",
      "building tree 4 of 10\n",
      "building tree 5 of 10\n",
      "building tree 6 of 10\n",
      "building tree 7 of 10\n",
      "building tree 8 of 10\n",
      "building tree 9 of 10\n",
      "building tree 10 of 10\n",
      "\n",
      "accuracy 0.8100558659217877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rajku\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(random_state=0, verbose=3)\n",
    "model = model.fit(X_train, y_train)\n",
    "\n",
    "y_prediction = model.predict(X_test)\n",
    "print(\"\\naccuracy\", np.sum(y_prediction == y_test) / float(len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1-layer Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the input, we have a vector of length 4 that represents each passenger's features. As an example, we consider the first passenger in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.83290956  0.74926865 -0.61259594 -0.51933199]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the output, we want a vector of length 2 to represent the survival probabilities. A simple way to create this mapping is by using a 2x4 matrix. To start off, we generate a random matrix representing feature weights and apply the matrix to our input. We'll also add a bias term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00262025 0.00158684 0.00278127 0.00459317]\n",
      " [0.00321001 0.00518393 0.00261943 0.00976085]]\n"
     ]
    }
   ],
   "source": [
    "W = np.random.rand(2, 4) * 0.01\n",
    "\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00732815 0.00115274]\n"
     ]
    }
   ],
   "source": [
    "b = np.random.rand(2,) * 0.01\n",
    "\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00661037 0.00103677]\n"
     ]
    }
   ],
   "source": [
    "result = np.dot(W, X_train[0]) + b\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the output vector to sum to 1, we apply a softmax mapping. The first element would now represent the probability that the passenger did not survive, and the second element represents the probability the passenger survives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.exp(x).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5013934 0.4986066]\n"
     ]
    }
   ],
   "source": [
    "result = softmax(result)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then compare the output vector to the actual label. We would have a 'good' model if the probability for the correct label was close to 1, and a 'bad' one if it was close to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_train_onehot[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "label_index = np.argmax(y_train_onehot[0])\n",
    "\n",
    "print(label_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted label-0 probability 0.5013933977819183\n"
     ]
    }
   ],
   "source": [
    "print(\"predicted label-0 probability\", result[label_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define loss to be the negative logarithm of the probability of the correct label. Taking the logarithm penalizes the model for having a high probability associated with the wrong label. Here we have the loss associated with the first passenger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for first passenger 0.6903642609116132\n"
     ]
    }
   ],
   "source": [
    "loss = -np.log(result[label_index])\n",
    "\n",
    "print(\"loss for first passenger\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the calculation through all the passengers in the training set, and divide the total loss by the number of passengers to obtain the average loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss across all passengers 0.6938940425068784\n"
     ]
    }
   ],
   "source": [
    "for j in range(X_train.shape[0]):\n",
    "    result = np.dot(W, X_train[j]) + b\n",
    "    result = softmax(result)\n",
    "        \n",
    "    label_index = np.argmax(y_train_onehot[j])\n",
    "    loss += -np.log(result[label_index])\n",
    "\n",
    "loss = loss / float(X_train.shape[0])\n",
    "\n",
    "print(\"average loss across all passengers\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we iterate through 1000 iterations for random values of W and b, and keep the pair which minimizes the average loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.711 accuracy 0.317 loop 0\n",
      "loss 0.698 accuracy 0.393 loop 1\n",
      "loss 0.67 accuracy 0.772 loop 2\n",
      "loss 0.663 accuracy 0.785 loop 4\n",
      "loss 0.661 accuracy 0.749 loop 156\n",
      "loss 0.659 accuracy 0.725 loop 452\n",
      "loss 0.658 accuracy 0.787 loop 715\n",
      "\n",
      "time taken 21.83057689666748 seconds\n"
     ]
    }
   ],
   "source": [
    "min_loss = 1000\n",
    "best_weights = ()\n",
    "\n",
    "start = time()\n",
    "\n",
    "for i in range(1000):\n",
    "    W = np.random.rand(2, 4) / 10\n",
    "    b = np.random.rand(2,) / 10\n",
    "\n",
    "    scores = []\n",
    "    loss = 0\n",
    "    \n",
    "    for j in range(X_train.shape[0]):\n",
    "        result = np.dot(W, X_train[j]) + b\n",
    "        result = softmax(result)\n",
    "        scores.append(list(result))\n",
    "        \n",
    "        label_index = np.argmax(y_train_onehot[j])\n",
    "        loss += -np.log(result[label_index])\n",
    "\n",
    "    loss = loss / float(X_train.shape[0])\n",
    "    y_prediction = np.argmax(np.array(scores), axis=1)\n",
    "    accuracy = np.sum(y_prediction == y_train) / float(len(y_train))\n",
    "    \n",
    "    if loss < min_loss:\n",
    "        min_loss = loss\n",
    "        best_weights = (W, b)\n",
    "        print(\"loss %s accuracy %s loop %s\" % (round(loss, 3), round(accuracy, 3), i))\n",
    "\n",
    "print(\"\\ntime taken %s seconds\" % str(time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7821229050279329\n"
     ]
    }
   ],
   "source": [
    "W, b = best_weights\n",
    "scores = []\n",
    "\n",
    "for j in range(X_test.shape[0]):\n",
    "    result = np.dot(W, X_test[j]) + b\n",
    "    result = softmax(result)\n",
    "    scores.append(list(result))\n",
    "\n",
    "y_prediction = np.argmax(np.array(scores), axis=1)\n",
    "\n",
    "print(\"accuracy\", np.sum(y_prediction == y_test) / float(len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each passenger, predictions for the test set were made by selecting the label with the highest probability. Despite the naïve approach, we obtain a prediction accuracy of 78%!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2-layer Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the 1-layer neural network, we had a 2x4 weight matrix. To create more degrees of freedom, we can introduce intermediary matrices or 'layers'. For example, instead of having a mapping from a vector of length 4 to a vector of length 2, we'll have two mappings - first from 4 to 100, followed by 100 to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_1 = np.random.rand(100, 4) * 0.01\n",
    "b_1 = np.random.rand(100,) * 0.01\n",
    "W_2 = np.random.rand(2, 100) * 0.01\n",
    "b_2 = np.random.rand(2,) * 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.dot(W_1, X_train[0]) + b_1\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.dot(W_2, result) + b_2\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before we iterate through 1000 iterations of random values for W_1, b_1, W_2 and b_2, and keep the one which minimizes loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_loss = 1000\n",
    "best_weights = ()\n",
    "\n",
    "start = time()\n",
    "\n",
    "for i in range(1000):\n",
    "    W_1 = np.random.rand(100, 4) * 0.01\n",
    "    b_1 = np.random.rand(100,) * 0.01\n",
    "    W_2 = np.random.rand(2, 100) * 0.01\n",
    "    b_2 = np.random.rand(2,) * 0.01\n",
    "    \n",
    "    scores = []\n",
    "    loss = 0\n",
    "\n",
    "    for j in range(X_train.shape[0]):\n",
    "        result = np.dot(W_1, X_train[j]) + b_1\n",
    "        result = np.dot(W_2, result) + b_2\n",
    "        result = softmax(result)\n",
    "        scores.append(list(result))\n",
    "        \n",
    "        label_index = np.argmax(y_train_onehot[j])\n",
    "        loss += -np.log(result[label_index])\n",
    "\n",
    "    loss = loss / float(X_train.shape[0])\n",
    "    y_prediction = np.argmax(np.array(scores), axis=1)\n",
    "    accuracy = np.sum(y_prediction == y_train) / float(len(y_train))\n",
    "        \n",
    "    if loss < min_loss:\n",
    "        min_loss = loss\n",
    "        best_weights = (W_1, b_1, W_2, b_2)\n",
    "        print(\"loss %s accuracy %s loop %s\" % (round(loss, 3), round(accuracy, 3), i))\n",
    "\n",
    "print(\"\\ntime taken %s seconds\" % str(time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_1, b_1, W_2, b_2 = best_weights\n",
    "scores = []\n",
    "\n",
    "for j in range(X_test.shape[0]):\n",
    "    result = np.dot(W_1, X_test[j]) + b_1\n",
    "    result = np.dot(W_2, result) + b_2\n",
    "    result = softmax(result)\n",
    "    scores.append(list(result))\n",
    "    \n",
    "y_prediction = np.argmax(np.array(scores), axis=1)\n",
    "\n",
    "print(\"accuracy\", np.sum(y_prediction == y_test) / float(len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the greater degree of freedom, we get an accuracy score of only 64%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3-layer Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take this a step further by adding an additional layer, and similarly review model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_loss = 1000\n",
    "best_weights = ()\n",
    "\n",
    "start = time()\n",
    "\n",
    "for i in range(1000):\n",
    "    W_1 = np.random.rand(100, 4) * 0.01\n",
    "    b_1 = np.random.rand(100,) * 0.01\n",
    "    W_2 = np.random.rand(100, 100) * 0.01\n",
    "    b_2 = np.random.rand(100,) * 0.01\n",
    "    W_3 = np.random.rand(2, 100) * 0.01\n",
    "    b_3 = np.random.rand(2,) * 0.01\n",
    "    \n",
    "    scores = []\n",
    "    loss = 0\n",
    "\n",
    "    for j in range(X_train.shape[0]):\n",
    "        result = np.dot(W_1, X_train[j]) + b_1\n",
    "        result = np.dot(W_2, result) + b_2\n",
    "        result = np.dot(W_3, result) + b_3\n",
    "        result = softmax(result)\n",
    "        scores.append(list(result))\n",
    "        \n",
    "        label_index = np.argmax(y_train_onehot[j])\n",
    "        loss += -np.log(result[label_index])\n",
    "        \n",
    "    loss = loss / float(X_train.shape[0])\n",
    "    y_prediction = np.argmax(np.array(scores), axis=1)\n",
    "    accuracy = np.sum(y_prediction == y_train) / float(len(y_train))          \n",
    "        \n",
    "    if loss < min_loss:\n",
    "        min_loss = loss\n",
    "        best_weights = (W_1, b_1, W_2, b_2, W_3, b_3)\n",
    "        print(\"loss %s accuracy %s loop %s\" % (round(loss, 3), round(accuracy, 3), i))\n",
    "\n",
    "print(\"\\ntime taken %s seconds\" % str(time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_1, b_1, W_2, b_2, W_3, b_3 = best_weights\n",
    "scores = []\n",
    "\n",
    "for j in range(X_test.shape[0]):\n",
    "    result = np.dot(W_1, X_test[j]) + b_1\n",
    "    result = np.dot(W_2, result) + b_2\n",
    "    result = np.dot(W_3, result) + b_3    \n",
    "    result = softmax(result)\n",
    "    scores.append(list(result))\n",
    "    \n",
    "y_prediction = np.argmax(np.array(scores), axis=1)\n",
    "\n",
    "print(\"accuracy\", np.sum(y_prediction == y_test) / float(len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run into the same problem of suboptimal model performance, but this is not unexpected given the simplistic approach taken for illustrative purposes. We look to build on this in the next section by taking a systematic approach to optimizing the weight matrices and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Neural Network - Titanic Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, we simply iterated through randomly-generated matrices and chose the best-performing one. We build on this approach by reducing loss in a systematic way via stochastic gradient descent. In particular, we'll be using TensorFlow, an open source library developed by Google, and Keras, a high-level wrapper on top of TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from time import time\n",
    "\n",
    "np.random.seed(1337)\n",
    "\n",
    "df = pd.read_csv('titanicNN.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.iloc[:712, :]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features = ['Class', 'Sex', 'Age', 'Fare']\n",
    "\n",
    "X_train = scaler.fit_transform(df_train[features].values)\n",
    "y_train = df_train['Survived'].values\n",
    "y_train_onehot = pd.get_dummies(df_train['Survived']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df.iloc[712:, :]\n",
    "\n",
    "X_test = scaler.transform(df_test[features].values)\n",
    "y_test = df_test['Survived'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(random_state=0, verbose=3)\n",
    "model = model.fit(X_train, df_train['Survived'].values)\n",
    "\n",
    "y_prediction = model.predict(X_test)\n",
    "print(\"\\naccuracy\", np.sum(y_prediction == y_test) / float(len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1-layer Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of generating a linear stack of layers with Numpy, we'll be implementing our model using Keras. We initialize our model, add a layer that inputs vectors of length 4 and outputs vectors of length 2, and finally add a softmax layer. We configure the learning process in the compilation step by specifying the optimizer, loss function and performance metric.\n",
    "\n",
    "Stochastic gradient descent acts by changing the weights gradually in the 'direction' that decreases the average loss. In other words, a particular weight would be increased if acts to decrease loss, or the weight decreased if it acts to increase loss. TensorFlow does the heavy-lifting by efficiently handling these numerical computations under the hood. A simple example of stochastic gradient descent is illustrated in the Appendix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(input_dim=4, output_dim=2))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train_onehot)\n",
    "\n",
    "print('\\ntime taken %s seconds' % str(time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Using cached https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl\n",
      "Collecting keras-applications>=1.0.6 (from keras)\n",
      "  Using cached https://files.pythonhosted.org/packages/90/85/64c82949765cfb246bbdaf5aca2d55f400f792655927a017710a78445def/Keras_Applications-1.0.7-py2.py3-none-any.whl\n",
      "Collecting keras-preprocessing>=1.0.5 (from keras)\n",
      "  Using cached https://files.pythonhosted.org/packages/c0/bf/0315ef6a9fd3fc2346e85b0ff1f5f83ca17073f2c31ac719ab2e4da0d4a3/Keras_Preprocessing-1.0.9-py2.py3-none-any.whl\n",
      "Requirement already satisfied: pyyaml in c:\\users\\rajku\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from keras) (3.13)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\rajku\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from keras) (1.15.4)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\rajku\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from keras) (1.12.0)\n",
      "Requirement already satisfied: h5py in c:\\users\\rajku\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from keras) (2.8.0)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\rajku\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from keras) (1.1.0)\n",
      "Installing collected packages: keras-applications, keras-preprocessing, keras\n",
      "Successfully installed keras-2.2.4 keras-applications-1.0.7 keras-preprocessing-1.0.9\n"
     ]
    }
   ],
   "source": [
    "!pip install keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached https://files.pythonhosted.org/packages/7b/14/e4538c2bc3ae9f4ce6f6ce7ef1180da05abc4a617afba798268232b01d0d/tensorflow-1.13.1-cp37-cp37m-win_amd64.whl\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\rajku\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from tensorflow) (0.32.3)\n",
      "Collecting grpcio>=1.8.6 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/2a/22/bd327063dd0bdf9d8d640b3185b760707842160e69df909db3fcaab5b758/grpcio-1.20.1-cp37-cp37m-win_amd64.whl\n",
      "Collecting gast>=0.2.0 (from tensorflow)\n",
      "Collecting astor>=0.6.0 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/35/6b/11530768cac581a12952a2aad00e1526b89d242d0b9f59534ef6e6a1752f/astor-0.7.1-py2.py3-none-any.whl\n",
      "Collecting tensorboard<1.14.0,>=1.13.0 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\users\\rajku\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from tensorflow) (1.0.9)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in c:\\users\\rajku\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from tensorflow) (1.0.7)\n",
      "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\rajku\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.4)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\rajku\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.0)\n",
      "Collecting protobuf>=3.6.1 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/34/ef/f020691889031a8e1d8cb20711daa43cfe999e0768ff6903c4bf70c2eecd/protobuf-3.7.1-cp37-cp37m-win_amd64.whl\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "Collecting absl-py>=0.1.6 (from tensorflow)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\rajku\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow) (0.14.1)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<1.14.0,>=1.13.0->tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/f5/e4/d8c18f2555add57ff21bf25af36d827145896a07607486cc79a2aea641af/Markdown-3.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: h5py in c:\\users\\rajku\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from keras-applications>=1.0.6->tensorflow) (2.8.0)\n",
      "Collecting mock>=2.0.0 (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/42/b4/f9afb3de9bd92d165e94a81f4048b373825009be9234f588a69afc64e7a1/mock-3.0.4-py2.py3-none-any.whl\n",
      "Requirement already satisfied: setuptools in c:\\users\\rajku\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from protobuf>=3.6.1->tensorflow) (40.6.3)\n",
      "Installing collected packages: grpcio, gast, astor, protobuf, markdown, absl-py, tensorboard, mock, tensorflow-estimator, termcolor, tensorflow\n",
      "Successfully installed absl-py-0.7.1 astor-0.7.1 gast-0.2.2 grpcio-1.20.1 markdown-3.1 mock-3.0.4 protobuf-3.7.1 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0 termcolor-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction = model.predict_classes(X_test)\n",
    "print(\"\\n\\naccuracy\", np.sum(y_prediction == y_test) / float(len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the loss reduces systematically as the model 'learns' from the data. The rate of loss reduction, however, seems to indicate that loss could be further reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2-layer Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(input_dim=4,  output_dim=100))\n",
    "model.add(Dense(output_dim=2))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train_onehot)\n",
    "\n",
    "print('\\ntime taken %s seconds' % str(time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction = model.predict_classes(X_test)\n",
    "print(\"\\n\\naccuracy\", np.sum(y_prediction == y_test) / float(len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss reduction 'flattens out' more compared to the 1-layer example, and the accuracy improves to 81%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3-layer Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(input_dim=4, output_dim=100))\n",
    "model.add(Dense(output_dim=100))\n",
    "model.add(Dense(output_dim=2))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train_onehot)\n",
    "\n",
    "print('\\ntime taken %s seconds' % str(time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction = model.predict_classes(X_test)\n",
    "print(\"\\n\\naccuracy\", np.sum(y_prediction == y_test) / float(len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we're able to reduce loss on the training set a little further, the best performance obtained is merely comparable to our benchmark. Since the dataset is small, there isn't as much for the model to 'learn' from (or for that matter, predict on). We'll apply techniques developed so far on a much larger dataset in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Neural Network - MNIST Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll apply techniques developed so far on the MNIST dataset. The MNIST dataset consists of hand-drawn digits from zero to nine. Each image is 28 pixels in height and 28 pixels in width, with the pixel value an integer between 0 and 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from itertools import izip\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from time import time\n",
    "\n",
    "np.random.seed(1337)\n",
    "\n",
    "df = pd.read_csv('mnist.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For illustrative purposes, the first example is shown with pixel values between 0 and 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]\n",
      "[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]\n",
      "[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]\n",
      "[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]\n",
      "[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,7.230769230769231,9.807692307692308,3.6153846153846154,0.0,0.0,0.0,0.0,0.0]\n",
      "[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,7.346153846153846,9.615384615384615,9.73076923076923,3.576923076923077,0.0,0.0,0.0,0.0,0.0]\n",
      "[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,4.730769230769231,9.538461538461538,9.73076923076923,6.423076923076923,0.38461538461538464,0.0,0.0,0.0,0.0,0.0]\n",
      "[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,3.076923076923077,9.5,9.73076923076923,8.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0]\n",
      "[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.1153846153846154,7.961538461538462,9.73076923076923,9.038461538461538,2.9615384615384617,0.0,0.0,0.0,0.0,0.0,0.0,0.0]\n",
      "[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2.076923076923077,8.038461538461538,9.73076923076923,9.73076923076923,3.3846153846153846,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]\n",
      "[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,3.576923076923077,9.76923076923077,9.73076923076923,9.153846153846153,6.538461538461538,0.6538461538461539,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]\n",
      "[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.8846153846153846,8.076923076923077,9.76923076923077,9.73076923076923,6.115384615384615,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]\n",
      "[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6153846153846154,8.038461538461538,9.73076923076923,9.76923076923077,9.23076923076923,3.1153846153846154,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]\n",
      "[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0384615384615385,9.73076923076923,9.73076923076923,9.76923076923077,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]\n",
      "[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.7692307692307693,7.923076923076923,9.76923076923077,9.76923076923077,7.615384615384615,0.2692307692307692,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]\n",
      "[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,6.461538461538462,9.73076923076923,9.73076923076923,7.538461538461538,0.2692307692307692,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]\n",
      "[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.7692307692307693,7.8076923076923075,9.73076923076923,9.538461538461538,2.923076923076923,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]\n",
      "[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.8461538461538461,7.230769230769231,9.73076923076923,9.423076923076923,3.576923076923077,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]\n",
      "[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,3.9615384615384617,9.73076923076923,9.73076923076923,7.346153846153846,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]\n",
      "[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,3.423076923076923,9.23076923076923,9.73076923076923,7.5,0.9615384615384616,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]\n",
      "[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5769230769230769,8.461538461538462,9.73076923076923,9.73076923076923,3.076923076923077,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]\n",
      "[0.0,0.0,0.0,0.0,0.0,0.0,0.0,3.6153846153846154,9.73076923076923,9.73076923076923,9.73076923076923,3.6153846153846154,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]\n",
      "[0.0,0.0,0.0,0.0,0.0,0.0,0.0,3.423076923076923,9.653846153846153,9.73076923076923,9.615384615384615,5.038461538461538,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]\n",
      "[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,8.23076923076923,8.384615384615385,3.6538461538461537,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]\n",
      "[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]\n",
      "[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]\n",
      "[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]\n",
      "[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]\n"
     ]
    }
   ],
   "source": [
    "for item in df.iloc[0, 1:].values.reshape(28,28)/26:\n",
    "    print(''.join(str(list(item)).split(' ')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A hand-drawn 1 can clearly be seen from the visualization in grayscale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip3 install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(df.iloc[0, 1:].values.reshape(28,28), cmap=plt.get_cmap('gray', 5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 42,000 images. We similarly split the images 80:20 into training and test sets, and scale the data through division by 255. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.iloc[:33600, :]\n",
    "\n",
    "X_train = df_train.iloc[:, 1:].values / 255.\n",
    "y_train = df_train['label'].values\n",
    "y_train_onehot = pd.get_dummies(df_train['label']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df.iloc[33600:, :]\n",
    "\n",
    "X_test = df_test.iloc[:, 1:].values / 255.\n",
    "y_test = df_test['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rajku\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.4s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 2 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    1.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 3 of 10\n",
      "building tree 4 of 10\n",
      "building tree 5 of 10\n",
      "building tree 6 of 10\n",
      "building tree 7 of 10\n",
      "building tree 8 of 10\n",
      "building tree 9 of 10\n",
      "building tree 10 of 10\n",
      "\n",
      "accuracy 0.9379761904761905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    4.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(random_state=0, verbose=3)\n",
    "model = model.fit(X_train, df_train['label'].values)\n",
    "\n",
    "y_prediction = model.predict(X_test)\n",
    "print(\"\\naccuracy\", np.sum(y_prediction == df_test['label'].values) / float(len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1-layer Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy.core._multiarray_umath'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy.core._multiarray_umath'"
     ]
    },
    {
     "ename": "SystemError",
     "evalue": "<class '_frozen_importlib._ModuleLockManager'> returned a result with an error set",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;31mImportError\u001b[0m: numpy.core.multiarray failed to import",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;31mSystemError\u001b[0m: <class '_frozen_importlib._ModuleLockManager'> returned a result with an error set"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core._multiarray_umath failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;31mImportError\u001b[0m: numpy.core._multiarray_umath failed to import"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.umath failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;31mImportError\u001b[0m: numpy.core.umath failed to import"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "start = time()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(input_dim=784, output_dim=10))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train_onehot)\n",
    "\n",
    "print('\\ntime taken %s seconds' % str(time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction = model.predict_classes(X_test)\n",
    "print(\"\\n\\naccuracy\", np.sum(y_prediction == y_test) / float(len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-layer Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(input_dim=784, output_dim=100))\n",
    "model.add(Dense(output_dim=10))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train_onehot)\n",
    "\n",
    "print('\\ntime taken %s seconds' % str(time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction = model.predict_classes(X_test)\n",
    "print(\"\\n\\naccuracy\", np.sum(y_prediction == y_test) / float(len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-layer Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(input_dim=784, output_dim=100))\n",
    "model.add(Dense(output_dim=100))\n",
    "model.add(Dense(output_dim=10))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train_onehot)\n",
    "\n",
    "print('\\ntime taken %s seconds' % str(time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction = model.predict_classes(X_test)\n",
    "print(\"\\n\\naccuracy\", np.sum(y_prediction == y_test) / float(len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous section, we were able to drive loss down even further with additional layers. While we see improvements in accuracy, it's still not enough to beat the benchmark. We'll look into more advanced techniques to enhance model performance in next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll introduce two additional layers to our model. The first is called the rectified linear unit (hereafter, ReLU), which helps introduce non-linearity into the network. The second is called the dropout layer, which acts to regularize the network and prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from time import time\n",
    "\n",
    "np.random.seed(1337)\n",
    "\n",
    "df = pd.read_csv('mnist.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.iloc[:33600, :]\n",
    "\n",
    "X_train = df_train.iloc[:, 1:].values / 255.\n",
    "y_train = df_train['label'].values\n",
    "y_train_onehot = pd.get_dummies(df_train['label']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df.iloc[33600:, :]\n",
    "\n",
    "X_test = df_test.iloc[:, 1:].values / 255.\n",
    "y_test = df_test['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(random_state=0, verbose=3)\n",
    "model = model.fit(X_train, df_train['label'].values)\n",
    "\n",
    "y_prediction = model.predict(X_test)\n",
    "print(\"\\naccuracy\", np.sum(y_prediction == df_test['label'].values) / float(len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While matrix operations are linear, there could be a non-linear relationship between the features and the label. Introducing a ReLU layer, defined as f(x) = max(0, x), can help the model capture this interaction. ReLU is widely used as its simplicity allows for much faster training without a high cost to accuracy.\n",
    "\n",
    "The dropout layer can be thought of as a form of sampling, where output values are randomly set to zero by a pre-specified probability. This creates a more robust network as the process prevents interdependence, and as such the model is less likely to overfit on the training data. It is surprisingly effective, which has made it an active area of research.\n",
    "\n",
    "Following Andrej Karpathy's advice of \"don't be a hero\", the example shown here is from the Keras repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/fchollet/keras/blob/master/examples/mnist_mlp.py\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "\n",
    "start = time()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(784,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train_onehot)\n",
    "\n",
    "print('\\ntime taken %s seconds' % str(time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction = model.predict_classes(X_test)\n",
    "print(\"\\n\\naccuracy\", np.sum(y_prediction == y_test) / float(len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introducing ReLU and dropout layers have enabled the model to outperform the benchmark by 2%! In the next section, we introduce new layers that take advantage of the 2D structure of the image to further improve model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approach we've taken so far treats each image a single 'flat' vector of length 784. In this section, we'll introduce layers that take advantage the 2D structure of each 28x28 MNIST image, helping simplify computation and improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from time import time\n",
    "\n",
    "np.random.seed(1337)\n",
    "\n",
    "df = pd.read_csv('mnist.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.iloc[:33600, :]\n",
    "\n",
    "X_train = df_train.iloc[:, 1:].values / 255.\n",
    "y_train = df_train['label'].values\n",
    "y_train_onehot = pd.get_dummies(df_train['label']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df.iloc[33600:, :]\n",
    "\n",
    "X_test = df_test.iloc[:, 1:].values / 255.\n",
    "y_test = df_test['label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(random_state=0, verbose=3)\n",
    "model = model.fit(X_train, df_train['label'].values)\n",
    "\n",
    "y_prediction = model.predict(X_test)\n",
    "print(\"\\naccuracy\", np.sum(y_prediction == df_test['label'].values) / float(len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first layer we'll introduce is called the convolutional layer. Instead of having a weight matrix of shape (output length x input length), we'll instead consider a 3x3 weight matrix called a filter or kernel. We take the vector product of the filter with each (overlapping) 3x3 grid in the 28x28 image.\n",
    "\n",
    "Since there are 26x26 such grids, a single filter results in a 26x26 output. 32 filters gives us an output 'volume' of shape 26x26x32. By making the 26x26x32 volume go through another convolutional layer, we end up with an output volume of shape 24x24x32.\n",
    "\n",
    "The second new layer we'll use is called the pooling layer. Here we divide up the input into non-overlapping grids of size 2x2, and take the maximum value from each grid. For an 24x24x32 input, this results in 12x12x32 output. This volume is then 'flattened' to a vector of length 4,608, which we manipulate the same way as any vector.\n",
    "\n",
    "The use of filters constraints the architecture of the network as each filter only focuses on a specific aspect of the data. This allows our model to scale better and be more translation-invariant. The pooling layer also reduces the number of parameters, helping reduce computation and limit overfitting.\n",
    "\n",
    "An excellent detailed discussion, with architectural variations and helpful illustrations, can be found on Stanford's CS231n course notes:\n",
    "\n",
    "http://cs231n.github.io/convolutional-networks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Convolution2D, MaxPooling2D, Flatten\n",
    "\n",
    "start = time()\n",
    "\n",
    "img_rows, img_cols = 28, 28\n",
    "nb_filters = 32\n",
    "pool_size = (2, 2)\n",
    "kernel_size = (3, 3)\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1],\n",
    "                        border_mode='valid',\n",
    "                        input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=pool_size))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train_onehot)\n",
    "\n",
    "print('\\ntime taken %s seconds' % str(time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction = model.predict_classes(X_test)\n",
    "print(\"\\naccuracy\", np.sum(y_prediction == y_test) / float(len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our best result yet! It is worth experimenting with different network architectures to see how they affect loss and accuracy. In the next section, we'll continue to take advantage of inherent data structures but apply it to text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've been using stochastic gradient descent for model training. In this section we'll discuss briefly how it works. For simplicity, we'll focus only on the features Sex and Fare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from time import time\n",
    "\n",
    "np.random.seed(1337)\n",
    "\n",
    "df = pd.read_csv('titanicNN.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.iloc[:712, :]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features = ['Sex', 'Fare']\n",
    "\n",
    "X_train = scaler.fit_transform(df_train[features].values)\n",
    "y_train = df_train['Survived'].values\n",
    "y_train_onehot = pd.get_dummies(df_train['Survived']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df.iloc[712:, :]\n",
    "\n",
    "X_test = scaler.transform(df_test[features].values)\n",
    "y_test = df_test['Survived'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.exp(x).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using the random matrix we started with, we make slight adjustments to the weights at every step. We track the values for the weights and biases at the start and end, and evaluate model accuracy at these points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_loss = 1000\n",
    "best_weights = ()\n",
    "\n",
    "W = np.random.rand(2, 2) * 0.01\n",
    "b = np.random.rand(2,) * 0.01\n",
    "\n",
    "scores = []\n",
    "loss = 0\n",
    "\n",
    "W_start = W\n",
    "b_start = b\n",
    "metrics = []\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "for j in range(X_train.shape[0]):\n",
    "    result = np.dot(W, X_train[j]) + b        \n",
    "    W = W - learning_rate * np.dot((result - y_train_onehot[j]).reshape(2,1), X_train[j].reshape(1,2))\n",
    "        \n",
    "    result = softmax(result)\n",
    "    scores.append(list(result))\n",
    "        \n",
    "    label_index = np.argmax(y_train_onehot[j])\n",
    "    loss += -np.log(result[label_index])\n",
    "    \n",
    "    metrics.append([W[1, 0], W[1, 1], loss/(j+1)])\n",
    "    \n",
    "W_end = W\n",
    "b_end = b\n",
    "metrics = np.array(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(W, b):\n",
    "    scores = []\n",
    "\n",
    "    for j in range(X_test.shape[0]):\n",
    "        result = np.dot(W, X_test[j]) + b\n",
    "        result = softmax(result)\n",
    "        scores.append(list(result))\n",
    "\n",
    "    y_prediction = np.argmax(np.array(scores), axis=1)\n",
    "    return np.sum(y_prediction == y_test) / float(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"accuracy at start\", accuracy(W_start, b_start))\n",
    "print(\"accuracy at end\", accuracy(W_end, b_end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"label-1 weights at start\", W_start[1, :])\n",
    "print(\"label-1 weights at end\", W_end[1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that accuracy has improved significantly by these adjustments. Note that the label-1 weight for Sex goes from a positive value at initialization to a negative value. The model has 'learned' that the higher the value of this feature, the less likely that passenger would survive. This is indeed consistent with the data.\n",
    "\n",
    "We visualize how the label-1 weights change over time, changing from red to green with the number of steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "border = 0.0025\n",
    "\n",
    "fig1 = plt.figure(figsize=(50,10))\n",
    "ax1 = plt.subplot(131)\n",
    "\n",
    "alphas = np.linspace(0, 1, 356)\n",
    "ones = np.ones(356)\n",
    "rgba_colors = np.zeros((712,4))\n",
    "rgba_colors[:, 0] = np.concatenate([ones, ones-alphas])\n",
    "rgba_colors[:, 1] = np.concatenate([alphas, ones])\n",
    "rgba_colors[:, 3] = np.ones(712)\n",
    "\n",
    "ax1.scatter(metrics[:, 0], metrics[:, 1], c=rgba_colors, edgecolor=rgba_colors)\n",
    "ax1.set_xlim([metrics[:, 0].min()-border, metrics[:, 0].max()+border])\n",
    "ax1.set_ylim([metrics[:, 1].min()-border, metrics[:, 1].max()+border])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Linear Regression with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset\n",
    "trX = np.linspace(-1, 1, 101)\n",
    "trY = 2 * trX + np.random.randn(*trX.shape) * 0.33 # create a y value which is approximately linear but with some random noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression model\n",
    "model = Sequential()\n",
    "model.add(Dense(input_dim=1, output_dim=1, init='uniform', activation='linear'))\n",
    "model.compile(optimizer='sgd', loss='mse') ## Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print initial weights\n",
    "weights = model.layers[0].get_weights()\n",
    "w_init = weights[0][0][0]\n",
    "b_init = weights[1][0]\n",
    "print('Linear regression model is initialized with weight w: %.2f, b: %.2f' % (w_init, b_init))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "model.fit(trX, trY, nb_epoch=100, verbose=1)\n",
    "\n",
    "# Print trained weights\n",
    "weights = model.layers[0].get_weights()\n",
    "w = weights[0][0][0]\n",
    "b = weights[1][0]\n",
    "print('Linear regression model is trained with weight w: %.2f, b: %.2f' % (w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(trX, trY, label='data')\n",
    "plt.plot(trX, w_init*trX + b_init, label='init')\n",
    "plt.plot(trX, w*trX + b, label='prediction')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression Settings\n",
    "batch_size = 128\n",
    "nb_classes = 10\n",
    "nb_epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset and split for train and test\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "Y_Train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_Test = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression model\n",
    "model = Sequential()\n",
    "model.add(Dense(output_dim=10, input_shape=(784,), init='normal', activation='softmax'))\n",
    "model.compile(optimizer=SGD(lr=0.05), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "history = model.fit(X_train, Y_Train, nb_epoch=nb_epoch, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "evaluation = model.evaluate(X_test, Y_Test, verbose=1)\n",
    "print('Summary: Loss over the test dataset: %.2f, Accuracy: %.2f' % (evaluation[0], evaluation[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode test images \n",
    "# decoded_imgs = model.predict(X_test)\n",
    "decoded_imgs = model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_test[1].reshape(28, 28))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decoded_imgs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoEncoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input\n",
    "from keras.datasets import mnist\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder settings - Hyper parameters\n",
    "batch_size = 128\n",
    "nb_epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters (size) for MNIST dataset\n",
    "img_rows, img_cols = 28, 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for denoising autoencoder\n",
    "nb_visible = img_rows * img_cols\n",
    "nb_hidden = 500\n",
    "corruption_level = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST Dataset\n",
    "(x_train, _), (x_test, _) = mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add random noise\n",
    "x_train_noisy = x_train + corruption_level * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)\n",
    "x_test_noisy = x_test + corruption_level * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)\n",
    "x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n",
    "x_test_noisy = np.clip(x_test_noisy, 0., 1.)\n",
    "print(x_train_noisy.shape)\n",
    "print(x_test_noisy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build autoencoder model\n",
    "input_img = Input(shape=(nb_visible,))\n",
    "encoded = Dense(nb_hidden, activation='relu')(input_img)\n",
    "decoded = Dense(nb_visible, activation='sigmoid')(encoded)\n",
    "\n",
    "autoencoder = Model(input=input_img, output=decoded)\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "autoencoder.fit(x_train_noisy, x_train,\n",
    "                nb_epoch=20, batch_size=batch_size, shuffle=True, verbose=1,\n",
    "                validation_data=(x_test_noisy, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "evaluation = autoencoder.evaluate(x_test_noisy, x_test, batch_size=batch_size, verbose=1)\n",
    "print('\\nSummary: Loss over the test dataset: %.2f' % (evaluation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode test images \n",
    "decoded_imgs = autoencoder.predict(x_test_noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10  # how many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test_noisy[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "name": "_merged",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "226.0749969482422px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
