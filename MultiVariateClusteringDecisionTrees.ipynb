{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Analysis, Clustering and Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Multivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import the libraries for multivariate analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from scipy import stats\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# figures inline in notebook\n",
    "%matplotlib inline\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "DISPLAY_MAX_ROWS = 20  # number of max rows to print for a DataFrame\n",
    "pd.set_option('display.max_rows', DISPLAY_MAX_ROWS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reading Multivariate Data into Python\n",
    "\n",
    "For this portion of the class we will be using the [Python Data Analysis Library](http://pandas.pydata.org) (pandas, imported as `pd`), which you are already familiar with.\n",
    "\n",
    "Our first data file is from UCI Machine Learning Library,  http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data, contains data on concentrations of 13 different chemicals in wines grown in the same region in Italy that are derived from three different cultivars. The data set looks like this:\n",
    "\n",
    "```\n",
    "1,14.23,1.71,2.43,15.6,127,2.8,3.06,.28,2.29,5.64,1.04,3.92,1065\n",
    "1,13.2,1.78,2.14,11.2,100,2.65,2.76,.26,1.28,4.38,1.05,3.4,1050\n",
    "1,13.16,2.36,2.67,18.6,101,2.8,3.24,.3,2.81,5.68,1.03,3.17,1185\n",
    "1,14.37,1.95,2.5,16.8,113,3.85,3.49,.24,2.18,7.8,.86,3.45,1480\n",
    "1,13.24,2.59,2.87,21,118,2.8,2.69,.39,1.82,4.32,1.04,2.93,735\n",
    "...\n",
    "```\n",
    "\n",
    "There is one row per wine sample. The first column contains the cultivar of a wine sample (labelled 1, 2 or 3), and the following thirteen columns contain the concentrations of the 13 different chemicals in that sample. The columns are separated by commas, i.e. it is a comma-separated (csv) file without a header row.\n",
    "\n",
    "The data can be read in a pandas dataframe using the `read_csv()` function. The argument `header=None` tells the function that there is no header in the beginning of the file.<a id='read_data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.640000</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.380000</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.680000</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.800000</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.320000</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>14.20</td>\n",
       "      <td>1.76</td>\n",
       "      <td>2.45</td>\n",
       "      <td>15.2</td>\n",
       "      <td>112</td>\n",
       "      <td>3.27</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.97</td>\n",
       "      <td>6.750000</td>\n",
       "      <td>1.05</td>\n",
       "      <td>2.85</td>\n",
       "      <td>1450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>14.39</td>\n",
       "      <td>1.87</td>\n",
       "      <td>2.45</td>\n",
       "      <td>14.6</td>\n",
       "      <td>96</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.52</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.98</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>1.02</td>\n",
       "      <td>3.58</td>\n",
       "      <td>1290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>14.06</td>\n",
       "      <td>2.15</td>\n",
       "      <td>2.61</td>\n",
       "      <td>17.6</td>\n",
       "      <td>121</td>\n",
       "      <td>2.60</td>\n",
       "      <td>2.51</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1.25</td>\n",
       "      <td>5.050000</td>\n",
       "      <td>1.06</td>\n",
       "      <td>3.58</td>\n",
       "      <td>1295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>14.83</td>\n",
       "      <td>1.64</td>\n",
       "      <td>2.17</td>\n",
       "      <td>14.0</td>\n",
       "      <td>97</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.98</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.98</td>\n",
       "      <td>5.200000</td>\n",
       "      <td>1.08</td>\n",
       "      <td>2.85</td>\n",
       "      <td>1045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>13.86</td>\n",
       "      <td>1.35</td>\n",
       "      <td>2.27</td>\n",
       "      <td>16.0</td>\n",
       "      <td>98</td>\n",
       "      <td>2.98</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1.85</td>\n",
       "      <td>7.220000</td>\n",
       "      <td>1.01</td>\n",
       "      <td>3.55</td>\n",
       "      <td>1045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>3</td>\n",
       "      <td>13.58</td>\n",
       "      <td>2.58</td>\n",
       "      <td>2.69</td>\n",
       "      <td>24.5</td>\n",
       "      <td>105</td>\n",
       "      <td>1.55</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.54</td>\n",
       "      <td>8.660000</td>\n",
       "      <td>0.74</td>\n",
       "      <td>1.80</td>\n",
       "      <td>750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>3</td>\n",
       "      <td>13.40</td>\n",
       "      <td>4.60</td>\n",
       "      <td>2.86</td>\n",
       "      <td>25.0</td>\n",
       "      <td>112</td>\n",
       "      <td>1.98</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.11</td>\n",
       "      <td>8.500000</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1.92</td>\n",
       "      <td>630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>3</td>\n",
       "      <td>12.20</td>\n",
       "      <td>3.03</td>\n",
       "      <td>2.32</td>\n",
       "      <td>19.0</td>\n",
       "      <td>96</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.73</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>0.66</td>\n",
       "      <td>1.83</td>\n",
       "      <td>510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>3</td>\n",
       "      <td>12.77</td>\n",
       "      <td>2.39</td>\n",
       "      <td>2.28</td>\n",
       "      <td>19.5</td>\n",
       "      <td>86</td>\n",
       "      <td>1.39</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.64</td>\n",
       "      <td>9.899999</td>\n",
       "      <td>0.57</td>\n",
       "      <td>1.63</td>\n",
       "      <td>470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>3</td>\n",
       "      <td>14.16</td>\n",
       "      <td>2.51</td>\n",
       "      <td>2.48</td>\n",
       "      <td>20.0</td>\n",
       "      <td>91</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1.24</td>\n",
       "      <td>9.700000</td>\n",
       "      <td>0.62</td>\n",
       "      <td>1.71</td>\n",
       "      <td>660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>3</td>\n",
       "      <td>13.71</td>\n",
       "      <td>5.65</td>\n",
       "      <td>2.45</td>\n",
       "      <td>20.5</td>\n",
       "      <td>95</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.06</td>\n",
       "      <td>7.700000</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.74</td>\n",
       "      <td>740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>3</td>\n",
       "      <td>13.40</td>\n",
       "      <td>3.91</td>\n",
       "      <td>2.48</td>\n",
       "      <td>23.0</td>\n",
       "      <td>102</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.41</td>\n",
       "      <td>7.300000</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.56</td>\n",
       "      <td>750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>3</td>\n",
       "      <td>13.27</td>\n",
       "      <td>4.28</td>\n",
       "      <td>2.26</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.35</td>\n",
       "      <td>10.200000</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1.56</td>\n",
       "      <td>835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>3</td>\n",
       "      <td>13.17</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.37</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.46</td>\n",
       "      <td>9.300000</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.62</td>\n",
       "      <td>840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>3</td>\n",
       "      <td>14.13</td>\n",
       "      <td>4.10</td>\n",
       "      <td>2.74</td>\n",
       "      <td>24.5</td>\n",
       "      <td>96</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.35</td>\n",
       "      <td>9.200000</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.60</td>\n",
       "      <td>560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    V1     V2    V3    V4    V5   V6    V7    V8    V9   V10        V11   V12  \\\n",
       "0    1  14.23  1.71  2.43  15.6  127  2.80  3.06  0.28  2.29   5.640000  1.04   \n",
       "1    1  13.20  1.78  2.14  11.2  100  2.65  2.76  0.26  1.28   4.380000  1.05   \n",
       "2    1  13.16  2.36  2.67  18.6  101  2.80  3.24  0.30  2.81   5.680000  1.03   \n",
       "3    1  14.37  1.95  2.50  16.8  113  3.85  3.49  0.24  2.18   7.800000  0.86   \n",
       "4    1  13.24  2.59  2.87  21.0  118  2.80  2.69  0.39  1.82   4.320000  1.04   \n",
       "5    1  14.20  1.76  2.45  15.2  112  3.27  3.39  0.34  1.97   6.750000  1.05   \n",
       "6    1  14.39  1.87  2.45  14.6   96  2.50  2.52  0.30  1.98   5.250000  1.02   \n",
       "7    1  14.06  2.15  2.61  17.6  121  2.60  2.51  0.31  1.25   5.050000  1.06   \n",
       "8    1  14.83  1.64  2.17  14.0   97  2.80  2.98  0.29  1.98   5.200000  1.08   \n",
       "9    1  13.86  1.35  2.27  16.0   98  2.98  3.15  0.22  1.85   7.220000  1.01   \n",
       "..  ..    ...   ...   ...   ...  ...   ...   ...   ...   ...        ...   ...   \n",
       "168  3  13.58  2.58  2.69  24.5  105  1.55  0.84  0.39  1.54   8.660000  0.74   \n",
       "169  3  13.40  4.60  2.86  25.0  112  1.98  0.96  0.27  1.11   8.500000  0.67   \n",
       "170  3  12.20  3.03  2.32  19.0   96  1.25  0.49  0.40  0.73   5.500000  0.66   \n",
       "171  3  12.77  2.39  2.28  19.5   86  1.39  0.51  0.48  0.64   9.899999  0.57   \n",
       "172  3  14.16  2.51  2.48  20.0   91  1.68  0.70  0.44  1.24   9.700000  0.62   \n",
       "173  3  13.71  5.65  2.45  20.5   95  1.68  0.61  0.52  1.06   7.700000  0.64   \n",
       "174  3  13.40  3.91  2.48  23.0  102  1.80  0.75  0.43  1.41   7.300000  0.70   \n",
       "175  3  13.27  4.28  2.26  20.0  120  1.59  0.69  0.43  1.35  10.200000  0.59   \n",
       "176  3  13.17  2.59  2.37  20.0  120  1.65  0.68  0.53  1.46   9.300000  0.60   \n",
       "177  3  14.13  4.10  2.74  24.5   96  2.05  0.76  0.56  1.35   9.200000  0.61   \n",
       "\n",
       "      V13   V14  \n",
       "0    3.92  1065  \n",
       "1    3.40  1050  \n",
       "2    3.17  1185  \n",
       "3    3.45  1480  \n",
       "4    2.93   735  \n",
       "5    2.85  1450  \n",
       "6    3.58  1290  \n",
       "7    3.58  1295  \n",
       "8    2.85  1045  \n",
       "9    3.55  1045  \n",
       "..    ...   ...  \n",
       "168  1.80   750  \n",
       "169  1.92   630  \n",
       "170  1.83   510  \n",
       "171  1.63   470  \n",
       "172  1.71   660  \n",
       "173  1.74   740  \n",
       "174  1.56   750  \n",
       "175  1.56   835  \n",
       "176  1.62   840  \n",
       "177  1.60   560  \n",
       "\n",
       "[178 rows x 14 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\", header=None)\n",
    "data.columns = [\"V\"+str(i) for i in range(1, len(data.columns)+1)]  # rename column names to be similar to R naming convention\n",
    "data.V1 = data.V1.astype(str)\n",
    "X = data.loc[:, \"V2\":]  # independent variables data\n",
    "y = data.V1  # dependednt variable data\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the data on 178 samples of wine has been read into the variable `data`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plotting Multivariate Data\n",
    "\n",
    "Once you have read a multivariate data set into python, the next step is usually to make a plot of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A Matrix Scatterplot\n",
    "\n",
    "One common way of plotting multivariate data is to make a *matrix scatterplot*, showing each pair of variables plotted against each other. We can use the `scatter_matrix()` function from the `pandas.tools.plotting` package to do this.\n",
    "\n",
    "To use the `scatter_matrix()` function, we need to give it as its input the variables that you want included in the plot. Say for example, that we just want to include the variables corresponding to the concentrations of the first five chemicals. These are stored in columns V2-V6 of the variable `data`. The parameter `diagonal` allows us to specify whether to plot a histogram (`\"hist\"`) or a Kernel Density Estimation (`\"kde\"`) for the variable. We can extract just these columns from the variable `data` by typing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14.20</td>\n",
       "      <td>1.76</td>\n",
       "      <td>2.45</td>\n",
       "      <td>15.2</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14.39</td>\n",
       "      <td>1.87</td>\n",
       "      <td>2.45</td>\n",
       "      <td>14.6</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>14.06</td>\n",
       "      <td>2.15</td>\n",
       "      <td>2.61</td>\n",
       "      <td>17.6</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14.83</td>\n",
       "      <td>1.64</td>\n",
       "      <td>2.17</td>\n",
       "      <td>14.0</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>13.86</td>\n",
       "      <td>1.35</td>\n",
       "      <td>2.27</td>\n",
       "      <td>16.0</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>13.58</td>\n",
       "      <td>2.58</td>\n",
       "      <td>2.69</td>\n",
       "      <td>24.5</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>13.40</td>\n",
       "      <td>4.60</td>\n",
       "      <td>2.86</td>\n",
       "      <td>25.0</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>12.20</td>\n",
       "      <td>3.03</td>\n",
       "      <td>2.32</td>\n",
       "      <td>19.0</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>12.77</td>\n",
       "      <td>2.39</td>\n",
       "      <td>2.28</td>\n",
       "      <td>19.5</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>14.16</td>\n",
       "      <td>2.51</td>\n",
       "      <td>2.48</td>\n",
       "      <td>20.0</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>13.71</td>\n",
       "      <td>5.65</td>\n",
       "      <td>2.45</td>\n",
       "      <td>20.5</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>13.40</td>\n",
       "      <td>3.91</td>\n",
       "      <td>2.48</td>\n",
       "      <td>23.0</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>13.27</td>\n",
       "      <td>4.28</td>\n",
       "      <td>2.26</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>13.17</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.37</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>14.13</td>\n",
       "      <td>4.10</td>\n",
       "      <td>2.74</td>\n",
       "      <td>24.5</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        V2    V3    V4    V5   V6\n",
       "0    14.23  1.71  2.43  15.6  127\n",
       "1    13.20  1.78  2.14  11.2  100\n",
       "2    13.16  2.36  2.67  18.6  101\n",
       "3    14.37  1.95  2.50  16.8  113\n",
       "4    13.24  2.59  2.87  21.0  118\n",
       "5    14.20  1.76  2.45  15.2  112\n",
       "6    14.39  1.87  2.45  14.6   96\n",
       "7    14.06  2.15  2.61  17.6  121\n",
       "8    14.83  1.64  2.17  14.0   97\n",
       "9    13.86  1.35  2.27  16.0   98\n",
       "..     ...   ...   ...   ...  ...\n",
       "168  13.58  2.58  2.69  24.5  105\n",
       "169  13.40  4.60  2.86  25.0  112\n",
       "170  12.20  3.03  2.32  19.0   96\n",
       "171  12.77  2.39  2.28  19.5   86\n",
       "172  14.16  2.51  2.48  20.0   91\n",
       "173  13.71  5.65  2.45  20.5   95\n",
       "174  13.40  3.91  2.48  23.0  102\n",
       "175  13.27  4.28  2.26  20.0  120\n",
       "176  13.17  2.59  2.37  20.0  120\n",
       "177  14.13  4.10  2.74  24.5   96\n",
       "\n",
       "[178 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[:, \"V2\":\"V6\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make a matrix scatterplot of just these 5 variables using the `scatter_matrix()` function we type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 25 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.plotting.scatter_matrix(data.loc[:, \"V2\":\"V6\"], diagonal=\"kde\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this matrix scatterplot, the diagonal cells show histograms of each of the variables, in this case the concentrations of the first five chemicals (variables V2, V3, V4, V5, V6).\n",
    "\n",
    "Each of the off-diagonal cells is a scatterplot of two of the five chemicals, for example, the second cell in the first row is a scatterplot of V2 (y-axis) against V3 (x-axis)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A Scatterplot with the Data Points Labelled by their Group\n",
    "\n",
    "If you see an interesting scatterplot for two variables in the matrix scatterplot, you may want to plot that scatterplot in more detail, with the data points labelled by their group (their cultivar in this case).\n",
    "\n",
    "For example, in the matrix scatterplot above, the cell in the third column of the fourth row down is a scatterplot of V5 (x-axis) against V4 (y-axis). If you look at this scatterplot, it appears that there may be a positive relationship between V5 and V4.\n",
    "\n",
    "We may therefore decide to examine the relationship between `V5` and `V4` more closely, by plotting a scatterplot of these two variables, with the data points labelled by their group (their cultivar). To plot a scatterplot of two variables, we can use the `lmplot` function from the `seaborn` package. The V4 and V5 variables are stored in the columns V4 and V5 of the variable `data`. The first two parameters in the `lmplot()` function are the columns to be plotted against each other in x-y, the third parameter specifies the data, the `hue` parameter is the column name used for the labels of the datapoints, i.e. the classes they belong to, lastly, the `fit_reg` parameter is set to `False` when we do not want to plot a regression model relating to the x-y variables. Therefore, to plot the scatterplot, we type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 402.375x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lmplot(\"V4\", \"V5\", data, hue=\"V1\", fit_reg=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the scatterplot of V4 versus V5 that the wines from cultivar 2 seem to have lower values of V4 compared to the wines of cultivar 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### An overlay plot\n",
    "\n",
    "Another type of plot that is useful is a *profile plot*, which shows the variation in each of the variables, by plotting the value of each of the variables for each of the samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = data[[\"V2\",\"V3\",\"V4\",\"V5\",\"V6\"]].plot()\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear from the overlay plot that the mean and standard deviation for V6 is quite a lot higher than that for the other variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculating Summary Statistics for Multivariate Data\n",
    "\n",
    "We are starting to implement what we learned in the earlier sessions... Another thing that you are likely to want to do is to calculate summary statistics such as the mean and standard deviation for each of the variables in your multivariate data set.\n",
    "\n",
    "This is easy to do, using the `mean()` and `std()` functions in `numpy` and applying them to the dataframe using its member function `apply`.\n",
    "\n",
    "<p><div class=\"alert alert-success\">\n",
    "Pandas allows to do simple operations directly calling them as methods, for example we could do compute the means of a dataframe `df` by calling `df.mean()`.\n",
    "\n",
    "An alternative option is to use the `apply` method of the `pandas.DataFrame` class, which applies the passed argument function along the input axis of the DataFrame. This method is powerful as it allows passing any function we want to be applied in our data.\n",
    "</div></p>\n",
    "\n",
    "For example, say we want to calculate the mean and standard deviations of each of the 13 chemical concentrations in the wine samples. These are stored in columns V2-V14 of the variable `data`, which has been previously assigned to `X` for convenience. So we type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "V2      13.000618\n",
       "V3       2.336348\n",
       "V4       2.366517\n",
       "V5      19.494944\n",
       "V6      99.741573\n",
       "V7       2.295112\n",
       "V8       2.029270\n",
       "V9       0.361854\n",
       "V10      1.590899\n",
       "V11      5.058090\n",
       "V12      0.957449\n",
       "V13      2.611685\n",
       "V14    746.893258\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.apply(np.mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that the mean of variable V2 is 13.000618, the mean of V3 is 2.336348, and so on.\n",
    "\n",
    "Similarly, to get the standard deviations of the 13 chemical concentrations, we type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "V2       0.809543\n",
       "V3       1.114004\n",
       "V4       0.273572\n",
       "V5       3.330170\n",
       "V6      14.242308\n",
       "V7       0.624091\n",
       "V8       0.996049\n",
       "V9       0.124103\n",
       "V10      0.570749\n",
       "V11      2.311765\n",
       "V12      0.227929\n",
       "V13      0.707993\n",
       "V14    314.021657\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.apply(np.std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that it would make sense to standardise in order to compare the variables because the variables have very different standard deviations - the standard deviation of V14 is 314.021657, while the standard deviation of V9 is just 0.124103. Thus, in order to compare the variables, we need to standardise each variable so that it has a sample variance of 1 and sample mean of 0. We will explain below how to standardise the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Means and Variances Per Group\n",
    "\n",
    "It is often interesting to calculate the means and standard deviations for just the samples from a particular group, for example, for the wine samples from each cultivar. The cultivar is stored in the column V1 of the variable `data`, which has been previously assigned to `y` for convenience.\n",
    "\n",
    "To extract out the data for just cultivar 2, we can type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71, 14)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cultivar2data = data[y==\"2\"]\n",
    "cultivar2data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then calculate the mean and standard deviations of the 13 chemicals' concentrations, for just the cultivar 2 samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "V2      12.278732\n",
       "V3       1.932676\n",
       "V4       2.244789\n",
       "V5      20.238028\n",
       "V6      94.549296\n",
       "V7       2.258873\n",
       "V8       2.080845\n",
       "V9       0.363662\n",
       "V10      1.630282\n",
       "V11      3.086620\n",
       "V12      1.056282\n",
       "V13      2.785352\n",
       "V14    519.507042\n",
       "dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cultivar2data.loc[:, \"V2\":].apply(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "V2       0.534162\n",
       "V3       1.008391\n",
       "V4       0.313238\n",
       "V5       3.326097\n",
       "V6      16.635097\n",
       "V7       0.541507\n",
       "V8       0.700713\n",
       "V9       0.123085\n",
       "V10      0.597813\n",
       "V11      0.918393\n",
       "V12      0.201503\n",
       "V13      0.493064\n",
       "V14    156.100173\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cultivar2data.loc[:, \"V2\":].apply(np.std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can calculate the mean and standard deviation of the 13 chemicals' concentrations for just cultivar 1 samples, or for just cultivar 3 samples, in a similar way.\n",
    "\n",
    "However, for convenience, you might want to use the function `printMeanAndSdByGroup()` below, which prints out the mean and standard deviation of the variables for each group in your data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.744746</td>\n",
       "      <td>2.010678</td>\n",
       "      <td>2.455593</td>\n",
       "      <td>17.037288</td>\n",
       "      <td>106.338983</td>\n",
       "      <td>2.840169</td>\n",
       "      <td>2.982373</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>1.899322</td>\n",
       "      <td>5.528305</td>\n",
       "      <td>1.062034</td>\n",
       "      <td>3.157797</td>\n",
       "      <td>1115.711864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.278732</td>\n",
       "      <td>1.932676</td>\n",
       "      <td>2.244789</td>\n",
       "      <td>20.238028</td>\n",
       "      <td>94.549296</td>\n",
       "      <td>2.258873</td>\n",
       "      <td>2.080845</td>\n",
       "      <td>0.363662</td>\n",
       "      <td>1.630282</td>\n",
       "      <td>3.086620</td>\n",
       "      <td>1.056282</td>\n",
       "      <td>2.785352</td>\n",
       "      <td>519.507042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.153750</td>\n",
       "      <td>3.333750</td>\n",
       "      <td>2.437083</td>\n",
       "      <td>21.416667</td>\n",
       "      <td>99.312500</td>\n",
       "      <td>1.678750</td>\n",
       "      <td>0.781458</td>\n",
       "      <td>0.447500</td>\n",
       "      <td>1.153542</td>\n",
       "      <td>7.396250</td>\n",
       "      <td>0.682708</td>\n",
       "      <td>1.683542</td>\n",
       "      <td>629.895833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           V2        V3        V4         V5          V6        V7        V8  \\\n",
       "V1                                                                             \n",
       "1   13.744746  2.010678  2.455593  17.037288  106.338983  2.840169  2.982373   \n",
       "2   12.278732  1.932676  2.244789  20.238028   94.549296  2.258873  2.080845   \n",
       "3   13.153750  3.333750  2.437083  21.416667   99.312500  1.678750  0.781458   \n",
       "\n",
       "          V9       V10       V11       V12       V13          V14  \n",
       "V1                                                                 \n",
       "1   0.290000  1.899322  5.528305  1.062034  3.157797  1115.711864  \n",
       "2   0.363662  1.630282  3.086620  1.056282  2.785352   519.507042  \n",
       "3   0.447500  1.153542  7.396250  0.682708  1.683542   629.895833  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.458192</td>\n",
       "      <td>0.682689</td>\n",
       "      <td>0.225233</td>\n",
       "      <td>2.524651</td>\n",
       "      <td>10.409595</td>\n",
       "      <td>0.336077</td>\n",
       "      <td>0.394111</td>\n",
       "      <td>0.069453</td>\n",
       "      <td>0.408602</td>\n",
       "      <td>1.228032</td>\n",
       "      <td>0.115491</td>\n",
       "      <td>0.354038</td>\n",
       "      <td>219.635449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.534162</td>\n",
       "      <td>1.008391</td>\n",
       "      <td>0.313238</td>\n",
       "      <td>3.326097</td>\n",
       "      <td>16.635097</td>\n",
       "      <td>0.541507</td>\n",
       "      <td>0.700713</td>\n",
       "      <td>0.123085</td>\n",
       "      <td>0.597813</td>\n",
       "      <td>0.918393</td>\n",
       "      <td>0.201503</td>\n",
       "      <td>0.493064</td>\n",
       "      <td>156.100173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.524689</td>\n",
       "      <td>1.076514</td>\n",
       "      <td>0.182756</td>\n",
       "      <td>2.234515</td>\n",
       "      <td>10.776433</td>\n",
       "      <td>0.353233</td>\n",
       "      <td>0.290431</td>\n",
       "      <td>0.122840</td>\n",
       "      <td>0.404555</td>\n",
       "      <td>2.286743</td>\n",
       "      <td>0.113243</td>\n",
       "      <td>0.269262</td>\n",
       "      <td>113.891805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          V2        V3        V4        V5         V6        V7        V8  \\\n",
       "V1                                                                          \n",
       "1   0.458192  0.682689  0.225233  2.524651  10.409595  0.336077  0.394111   \n",
       "2   0.534162  1.008391  0.313238  3.326097  16.635097  0.541507  0.700713   \n",
       "3   0.524689  1.076514  0.182756  2.234515  10.776433  0.353233  0.290431   \n",
       "\n",
       "          V9       V10       V11       V12       V13         V14  \n",
       "V1                                                                \n",
       "1   0.069453  0.408602  1.228032  0.115491  0.354038  219.635449  \n",
       "2   0.123085  0.597813  0.918393  0.201503  0.493064  156.100173  \n",
       "3   0.122840  0.404555  2.286743  0.113243  0.269262  113.891805  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "V1\n",
       "1    59\n",
       "2    71\n",
       "3    48\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dat_groupby=X.groupby(y)\n",
    "display(dat_groupby.apply(np.mean))\n",
    "display(dat_groupby.apply(np.std))\n",
    "display(dat_groupby.apply(len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's write a simple function to get information about data\n",
    "def printMeanAndSdByGroup(variables, groupvariable):\n",
    "    data_groupby = variables.groupby(groupvariable)\n",
    "    print(\"## Means:\")\n",
    "    display(data_groupby.apply(np.mean))\n",
    "    print(\"\\n## Standard deviations:\")\n",
    "    display(data_groupby.apply(np.std))\n",
    "    print(\"\\n## Sample sizes:\")\n",
    "    display(pd.DataFrame(data_groupby.apply(len)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arguments of the function are the variables that you want to calculate means and standard deviations for (`X`), and the variable containing the group of each sample (`y`). For example, to calculate the mean and standard deviation for each of the 13 chemical concentrations, for each of the three different wine cultivars, we type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Means:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.744746</td>\n",
       "      <td>2.010678</td>\n",
       "      <td>2.455593</td>\n",
       "      <td>17.037288</td>\n",
       "      <td>106.338983</td>\n",
       "      <td>2.840169</td>\n",
       "      <td>2.982373</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>1.899322</td>\n",
       "      <td>5.528305</td>\n",
       "      <td>1.062034</td>\n",
       "      <td>3.157797</td>\n",
       "      <td>1115.711864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.278732</td>\n",
       "      <td>1.932676</td>\n",
       "      <td>2.244789</td>\n",
       "      <td>20.238028</td>\n",
       "      <td>94.549296</td>\n",
       "      <td>2.258873</td>\n",
       "      <td>2.080845</td>\n",
       "      <td>0.363662</td>\n",
       "      <td>1.630282</td>\n",
       "      <td>3.086620</td>\n",
       "      <td>1.056282</td>\n",
       "      <td>2.785352</td>\n",
       "      <td>519.507042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.153750</td>\n",
       "      <td>3.333750</td>\n",
       "      <td>2.437083</td>\n",
       "      <td>21.416667</td>\n",
       "      <td>99.312500</td>\n",
       "      <td>1.678750</td>\n",
       "      <td>0.781458</td>\n",
       "      <td>0.447500</td>\n",
       "      <td>1.153542</td>\n",
       "      <td>7.396250</td>\n",
       "      <td>0.682708</td>\n",
       "      <td>1.683542</td>\n",
       "      <td>629.895833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           V2        V3        V4         V5          V6        V7        V8  \\\n",
       "V1                                                                             \n",
       "1   13.744746  2.010678  2.455593  17.037288  106.338983  2.840169  2.982373   \n",
       "2   12.278732  1.932676  2.244789  20.238028   94.549296  2.258873  2.080845   \n",
       "3   13.153750  3.333750  2.437083  21.416667   99.312500  1.678750  0.781458   \n",
       "\n",
       "          V9       V10       V11       V12       V13          V14  \n",
       "V1                                                                 \n",
       "1   0.290000  1.899322  5.528305  1.062034  3.157797  1115.711864  \n",
       "2   0.363662  1.630282  3.086620  1.056282  2.785352   519.507042  \n",
       "3   0.447500  1.153542  7.396250  0.682708  1.683542   629.895833  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Standard deviations:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.458192</td>\n",
       "      <td>0.682689</td>\n",
       "      <td>0.225233</td>\n",
       "      <td>2.524651</td>\n",
       "      <td>10.409595</td>\n",
       "      <td>0.336077</td>\n",
       "      <td>0.394111</td>\n",
       "      <td>0.069453</td>\n",
       "      <td>0.408602</td>\n",
       "      <td>1.228032</td>\n",
       "      <td>0.115491</td>\n",
       "      <td>0.354038</td>\n",
       "      <td>219.635449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.534162</td>\n",
       "      <td>1.008391</td>\n",
       "      <td>0.313238</td>\n",
       "      <td>3.326097</td>\n",
       "      <td>16.635097</td>\n",
       "      <td>0.541507</td>\n",
       "      <td>0.700713</td>\n",
       "      <td>0.123085</td>\n",
       "      <td>0.597813</td>\n",
       "      <td>0.918393</td>\n",
       "      <td>0.201503</td>\n",
       "      <td>0.493064</td>\n",
       "      <td>156.100173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.524689</td>\n",
       "      <td>1.076514</td>\n",
       "      <td>0.182756</td>\n",
       "      <td>2.234515</td>\n",
       "      <td>10.776433</td>\n",
       "      <td>0.353233</td>\n",
       "      <td>0.290431</td>\n",
       "      <td>0.122840</td>\n",
       "      <td>0.404555</td>\n",
       "      <td>2.286743</td>\n",
       "      <td>0.113243</td>\n",
       "      <td>0.269262</td>\n",
       "      <td>113.891805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          V2        V3        V4        V5         V6        V7        V8  \\\n",
       "V1                                                                          \n",
       "1   0.458192  0.682689  0.225233  2.524651  10.409595  0.336077  0.394111   \n",
       "2   0.534162  1.008391  0.313238  3.326097  16.635097  0.541507  0.700713   \n",
       "3   0.524689  1.076514  0.182756  2.234515  10.776433  0.353233  0.290431   \n",
       "\n",
       "          V9       V10       V11       V12       V13         V14  \n",
       "V1                                                                \n",
       "1   0.069453  0.408602  1.228032  0.115491  0.354038  219.635449  \n",
       "2   0.123085  0.597813  0.918393  0.201503  0.493064  156.100173  \n",
       "3   0.122840  0.404555  2.286743  0.113243  0.269262  113.891805  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Sample sizes:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V1</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0\n",
       "V1    \n",
       "1   59\n",
       "2   71\n",
       "3   48"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "printMeanAndSdByGroup(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `printMeanAndSdByGroup()` also prints out the number of samples in each group. In this case, we see that there are 59 samples of cultivar 1, 71 of cultivar 2, and 48 of cultivar 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Between-groups Variance and Within-groups Variance for a Variable\n",
    "\n",
    "If we want to calculate the within-groups variance for a particular variable (for example, for a particular chemical’s concentration), we can use the function `calcWithinGroupsVariance()` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcWithinGroupsVariance(variable, groupvariable):\n",
    "    # find out how many values the group variable can take\n",
    "    levels = sorted(set(groupvariable))\n",
    "    numlevels = len(levels)\n",
    "    # get the mean and standard deviation for each group:\n",
    "    numtotal = 0\n",
    "    denomtotal = 0\n",
    "    for leveli in levels:\n",
    "        levelidata = variable[groupvariable==leveli]\n",
    "        levelilength = len(levelidata)\n",
    "        # get the standard deviation for group i:\n",
    "        sdi = np.std(levelidata)\n",
    "        numi = (levelilength)*sdi**2\n",
    "        denomi = levelilength\n",
    "        numtotal = numtotal + numi\n",
    "        denomtotal = denomtotal + denomi\n",
    "    # calculate the within-groups variance\n",
    "    withinGroupVariance = numtotal / (denomtotal - numlevels)\n",
    "    return withinGroupVariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `variable` parameter of the function `calcWithinGroupsVariance()` is the input variable for which we wish to compute its within-groups variance for the groups given in `groupvariable`.\n",
    "\n",
    "So for example, to calculate the within-groups variance of the variable V2 (the concentration of the first chemical), we type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2620524691539065"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calcWithinGroupsVariance(X.V2, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the within-groups variance for V2 is 0.2620525.\n",
    "\n",
    "We can calculate the between-groups variance for a particular variable (eg. V2) using the function `calcBetweenGroupsVariance()` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcBetweenGroupsVariance(variable, groupvariable):\n",
    "    # find out how many values the group variable can take\n",
    "    levels = sorted(set((groupvariable)))\n",
    "    numlevels = len(levels)\n",
    "    # calculate the overall grand mean:\n",
    "    grandmean = np.mean(variable)\n",
    "    # get the mean and standard deviation for each group:\n",
    "    numtotal = 0\n",
    "    denomtotal = 0\n",
    "    for leveli in levels:\n",
    "        levelidata = variable[groupvariable==leveli]\n",
    "        levelilength = len(levelidata)\n",
    "        # get the mean and standard deviation for group i:\n",
    "        meani = np.mean(levelidata)\n",
    "        sdi = np.std(levelidata)\n",
    "        numi = levelilength * ((meani - grandmean)**2)\n",
    "        denomi = levelilength\n",
    "        numtotal = numtotal + numi\n",
    "        denomtotal = denomtotal + denomi\n",
    "    # calculate the between-groups variance\n",
    "    betweenGroupsVariance = numtotal / (numlevels - 1)\n",
    "    return(betweenGroupsVariance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to the parameters of the function `calcWithinGroupsVariance()`, the `variable` parameter of the function `calcBetweenGroupsVariance()` is the input variable for which we wish to compute its between-groups variance for the groups given in `groupvariable`.\n",
    "\n",
    "So for example, to calculate the between-groups variance of the variable V2 (the concentration of the first chemical), we type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35.397424960269106"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calcBetweenGroupsVariance(X.V2, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the between-groups variance of V2 is 35.397425.\n",
    "\n",
    "We can calculate the *separation* achieved by a variable as its between-groups variance devided by its within-groups variance. Thus, the separation achieved by V2 is calculated as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135.07762424279917"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 35.397424960269106 / 0.2620524691539065\n",
    "calcBetweenGroupsVariance(X.V2, y) / calcWithinGroupsVariance(X.V2, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to calculate the separations achieved by all of the variables in a multivariate data set, you can use the function `calcSeparations()` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcSeparations(variables, groupvariable):\n",
    "    # calculate the separation for each variable\n",
    "    for variablename in variables:\n",
    "        variablei = variables[variablename]\n",
    "        withinGroup = calcWithinGroupsVariance(variablei, groupvariable)\n",
    "        betweenGroups = calcBetweenGroupsVariance(variablei, groupvariable)\n",
    "        sep = betweenGroups/withinGroup\n",
    "        print(\"variable\", variablename, \"withinGroup=\", withinGroup, \"betweenGroups=\", betweenGroups, \"separation=\", sep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, to calculate the separations for each of the 13 chemical concentrations, we type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable V2 withinGroup= 0.2620524691539065 betweenGroups= 35.397424960269106 separation= 135.07762424279917\n",
      "variable V3 withinGroup= 0.8875467967465813 betweenGroups= 32.789018486921364 separation= 36.94342496318368\n",
      "variable V4 withinGroup= 0.0660721013425184 betweenGroups= 0.8796113572487572 separation= 13.312901199991257\n",
      "variable V5 withinGroup= 8.006811181211566 betweenGroups= 286.41674636308926 separation= 35.77163740730921\n",
      "variable V6 withinGroup= 180.65777316441023 betweenGroups= 2245.50102788939 separation= 12.429584338149898\n",
      "variable V7 withinGroup= 0.1912704752242267 betweenGroups= 17.92835729428464 separation= 93.73300962036717\n",
      "variable V8 withinGroup= 0.274707514337437 betweenGroups= 64.26119502356416 separation= 233.92587268154927\n",
      "variable V9 withinGroup= 0.011911702213279677 betweenGroups= 0.32847015746162356 separation= 27.575417146965858\n",
      "variable V10 withinGroup= 0.2461729437955417 betweenGroups= 7.451995507777756 separation= 30.27138317022764\n",
      "variable V11 withinGroup= 2.2849230813335426 betweenGroups= 275.7080008223043 separation= 120.66401844100315\n",
      "variable V12 withinGroup= 0.024487646943241362 betweenGroups= 2.4810099149382907 separation= 101.31679539030002\n",
      "variable V13 withinGroup= 0.16077872956098171 betweenGroups= 30.543508354425363 separation= 189.97232057888928\n",
      "variable V14 withinGroup= 29707.68187051689 betweenGroups= 6176832.322284826 separation= 207.92037390217797\n"
     ]
    }
   ],
   "source": [
    "calcSeparations(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the individual variable which gives the greatest separations between the groups (the wine cultivars) is V8 (separation 233.9). \n",
    "\n",
    "As we will discuss below, the purpose of linear discriminant analysis (LDA) is to find the linear combination of the individual variables that will give the greatest separation between the groups (cultivars here). This hopefully will give a better separation than the best separation achievable by any individual variable (233.9 for V8 here)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Between-groups Covariance and Within-groups Covariance for Two Variables\n",
    "\n",
    "If you have a multivariate data set with several variables describing sampling units from different groups, such as the wine samples from different cultivars, it is often of interest to calculate the within-groups covariance and between-groups variance for pairs of the variables.\n",
    "\n",
    "This can be done using the following functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcWithinGroupsCovariance(variable1, variable2, groupvariable):\n",
    "    levels = sorted(set(groupvariable))\n",
    "    numlevels = len(levels)\n",
    "    covarianceWithin = 0.0\n",
    "    # get the covariance of variable 1 and variable 2 for each group:\n",
    "    for leveli in levels:\n",
    "        levelidata1 = variable1[groupvariable==leveli]\n",
    "        levelidata2 = variable2[groupvariable==leveli]\n",
    "        mean1 = np.mean(levelidata1)\n",
    "        mean2 = np.mean(levelidata2)\n",
    "        levelilength = len(levelidata1)\n",
    "        # get the covariance for this group:\n",
    "        term1 = 0.0\n",
    "        for levelidata1j, levelidata2j in zip(levelidata1, levelidata2):\n",
    "            term1 += (levelidata1j - mean1)*(levelidata2j - mean2)\n",
    "        groupCovariance = term1 # covariance for this group\n",
    "        covarianceWithin += groupCovariance\n",
    "    totallength = len(variable1)\n",
    "    covarianceWithin /= totallength - numlevels\n",
    "    return covarianceWithin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, to calculate the within-groups covariance for variables V8 and V11, we type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28667830215140183"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calcWithinGroupsCovariance(X.V8, X.V11, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcBetweenGroupsCovariance(variable1, variable2, groupvariable):\n",
    "    # find out how many values the group variable can take\n",
    "    levels = sorted(set(groupvariable))\n",
    "    numlevels = len(levels)\n",
    "    # calculate the grand means\n",
    "    variable1mean = np.mean(variable1)\n",
    "    variable2mean = np.mean(variable2)\n",
    "    # calculate the between-groups covariance\n",
    "    covarianceBetweenGroups = 0.0\n",
    "    for leveli in levels:\n",
    "        levelidata1 = variable1[groupvariable==leveli]\n",
    "        levelidata2 = variable2[groupvariable==leveli]\n",
    "        mean1 = np.mean(levelidata1)\n",
    "        mean2 = np.mean(levelidata2)\n",
    "        levelilength = len(levelidata1)\n",
    "        term1 = (mean1 - variable1mean) * (mean2 - variable2mean) * levelilength\n",
    "        covarianceBetweenGroups += term1\n",
    "    covarianceBetweenGroups /= numlevels - 1\n",
    "    return covarianceBetweenGroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, to calculate the between-groups covariance for variables V8 and V11, we type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-60.4107748359163"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calcBetweenGroupsCovariance(X.V8, X.V11, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, for V8 and V11, the between-groups covariance is -60.41 and the within-groups covariance is 0.29. \n",
    "\n",
    "Since the within-groups covariance is positive (0.29), it means V8 and V11 are positively related within groups: for individuals from the same group, individuals with a high value of V8 tend to have a high value of V11, and vice versa. \n",
    "    \n",
    "Since the between-groups covariance is negative (-60.41), V8 and V11 are negatively related between groups: groups with a high mean value of V8 tend to have a low mean value of V11, and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculating Correlations for Multivariate Data\n",
    "\n",
    "It is often of interest to investigate whether any of the variables in a multivariate data set are significantly correlated.\n",
    "\n",
    "To calculate the linear (Pearson) correlation coefficient for a pair of variables, you can use the `pearsonr()` function from `scipy.stats` package. For example, to calculate the correlation coefficient for the first two chemicals’ concentrations, V2 and V3, we type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value:   0.21008198597074534\n",
      "cor:       0.094396940910414\n"
     ]
    }
   ],
   "source": [
    "corr = stats.pearsonr(X.V2, X.V3)\n",
    "print(\"p-value:  \", corr[1])\n",
    "print(\"cor:      \", corr[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that the correlation coefficient is about 0.094, which is a very weak correlation. Furthermore, the *p-value* for the statistical test of whether the correlation coefficient is significantly different from zero is 0.21. This is much greater than 0.05 (which we can use here as a cutoff for statistical significance), so there is very weak evidence that that the correlation is non-zero.\n",
    "\n",
    "If you have a lot of variables, you can use the `pandas.DataFrame` method `corr()` to calculate a correlation matrix that shows the correlation coefficient for each pair of variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>V2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.094397</td>\n",
       "      <td>0.211545</td>\n",
       "      <td>-0.310235</td>\n",
       "      <td>0.270798</td>\n",
       "      <td>0.289101</td>\n",
       "      <td>0.236815</td>\n",
       "      <td>-0.155929</td>\n",
       "      <td>0.136698</td>\n",
       "      <td>0.546364</td>\n",
       "      <td>-0.071747</td>\n",
       "      <td>0.072343</td>\n",
       "      <td>0.643720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V3</th>\n",
       "      <td>0.094397</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.164045</td>\n",
       "      <td>0.288500</td>\n",
       "      <td>-0.054575</td>\n",
       "      <td>-0.335167</td>\n",
       "      <td>-0.411007</td>\n",
       "      <td>0.292977</td>\n",
       "      <td>-0.220746</td>\n",
       "      <td>0.248985</td>\n",
       "      <td>-0.561296</td>\n",
       "      <td>-0.368710</td>\n",
       "      <td>-0.192011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V4</th>\n",
       "      <td>0.211545</td>\n",
       "      <td>0.164045</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.443367</td>\n",
       "      <td>0.286587</td>\n",
       "      <td>0.128980</td>\n",
       "      <td>0.115077</td>\n",
       "      <td>0.186230</td>\n",
       "      <td>0.009652</td>\n",
       "      <td>0.258887</td>\n",
       "      <td>-0.074667</td>\n",
       "      <td>0.003911</td>\n",
       "      <td>0.223626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V5</th>\n",
       "      <td>-0.310235</td>\n",
       "      <td>0.288500</td>\n",
       "      <td>0.443367</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.083333</td>\n",
       "      <td>-0.321113</td>\n",
       "      <td>-0.351370</td>\n",
       "      <td>0.361922</td>\n",
       "      <td>-0.197327</td>\n",
       "      <td>0.018732</td>\n",
       "      <td>-0.273955</td>\n",
       "      <td>-0.276769</td>\n",
       "      <td>-0.440597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V6</th>\n",
       "      <td>0.270798</td>\n",
       "      <td>-0.054575</td>\n",
       "      <td>0.286587</td>\n",
       "      <td>-0.083333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.214401</td>\n",
       "      <td>0.195784</td>\n",
       "      <td>-0.256294</td>\n",
       "      <td>0.236441</td>\n",
       "      <td>0.199950</td>\n",
       "      <td>0.055398</td>\n",
       "      <td>0.066004</td>\n",
       "      <td>0.393351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V7</th>\n",
       "      <td>0.289101</td>\n",
       "      <td>-0.335167</td>\n",
       "      <td>0.128980</td>\n",
       "      <td>-0.321113</td>\n",
       "      <td>0.214401</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.864564</td>\n",
       "      <td>-0.449935</td>\n",
       "      <td>0.612413</td>\n",
       "      <td>-0.055136</td>\n",
       "      <td>0.433681</td>\n",
       "      <td>0.699949</td>\n",
       "      <td>0.498115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V8</th>\n",
       "      <td>0.236815</td>\n",
       "      <td>-0.411007</td>\n",
       "      <td>0.115077</td>\n",
       "      <td>-0.351370</td>\n",
       "      <td>0.195784</td>\n",
       "      <td>0.864564</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.537900</td>\n",
       "      <td>0.652692</td>\n",
       "      <td>-0.172379</td>\n",
       "      <td>0.543479</td>\n",
       "      <td>0.787194</td>\n",
       "      <td>0.494193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V9</th>\n",
       "      <td>-0.155929</td>\n",
       "      <td>0.292977</td>\n",
       "      <td>0.186230</td>\n",
       "      <td>0.361922</td>\n",
       "      <td>-0.256294</td>\n",
       "      <td>-0.449935</td>\n",
       "      <td>-0.537900</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.365845</td>\n",
       "      <td>0.139057</td>\n",
       "      <td>-0.262640</td>\n",
       "      <td>-0.503270</td>\n",
       "      <td>-0.311385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V10</th>\n",
       "      <td>0.136698</td>\n",
       "      <td>-0.220746</td>\n",
       "      <td>0.009652</td>\n",
       "      <td>-0.197327</td>\n",
       "      <td>0.236441</td>\n",
       "      <td>0.612413</td>\n",
       "      <td>0.652692</td>\n",
       "      <td>-0.365845</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.025250</td>\n",
       "      <td>0.295544</td>\n",
       "      <td>0.519067</td>\n",
       "      <td>0.330417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V11</th>\n",
       "      <td>0.546364</td>\n",
       "      <td>0.248985</td>\n",
       "      <td>0.258887</td>\n",
       "      <td>0.018732</td>\n",
       "      <td>0.199950</td>\n",
       "      <td>-0.055136</td>\n",
       "      <td>-0.172379</td>\n",
       "      <td>0.139057</td>\n",
       "      <td>-0.025250</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.521813</td>\n",
       "      <td>-0.428815</td>\n",
       "      <td>0.316100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V12</th>\n",
       "      <td>-0.071747</td>\n",
       "      <td>-0.561296</td>\n",
       "      <td>-0.074667</td>\n",
       "      <td>-0.273955</td>\n",
       "      <td>0.055398</td>\n",
       "      <td>0.433681</td>\n",
       "      <td>0.543479</td>\n",
       "      <td>-0.262640</td>\n",
       "      <td>0.295544</td>\n",
       "      <td>-0.521813</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.565468</td>\n",
       "      <td>0.236183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V13</th>\n",
       "      <td>0.072343</td>\n",
       "      <td>-0.368710</td>\n",
       "      <td>0.003911</td>\n",
       "      <td>-0.276769</td>\n",
       "      <td>0.066004</td>\n",
       "      <td>0.699949</td>\n",
       "      <td>0.787194</td>\n",
       "      <td>-0.503270</td>\n",
       "      <td>0.519067</td>\n",
       "      <td>-0.428815</td>\n",
       "      <td>0.565468</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.312761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V14</th>\n",
       "      <td>0.643720</td>\n",
       "      <td>-0.192011</td>\n",
       "      <td>0.223626</td>\n",
       "      <td>-0.440597</td>\n",
       "      <td>0.393351</td>\n",
       "      <td>0.498115</td>\n",
       "      <td>0.494193</td>\n",
       "      <td>-0.311385</td>\n",
       "      <td>0.330417</td>\n",
       "      <td>0.316100</td>\n",
       "      <td>0.236183</td>\n",
       "      <td>0.312761</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           V2        V3        V4        V5        V6        V7        V8  \\\n",
       "V2   1.000000  0.094397  0.211545 -0.310235  0.270798  0.289101  0.236815   \n",
       "V3   0.094397  1.000000  0.164045  0.288500 -0.054575 -0.335167 -0.411007   \n",
       "V4   0.211545  0.164045  1.000000  0.443367  0.286587  0.128980  0.115077   \n",
       "V5  -0.310235  0.288500  0.443367  1.000000 -0.083333 -0.321113 -0.351370   \n",
       "V6   0.270798 -0.054575  0.286587 -0.083333  1.000000  0.214401  0.195784   \n",
       "V7   0.289101 -0.335167  0.128980 -0.321113  0.214401  1.000000  0.864564   \n",
       "V8   0.236815 -0.411007  0.115077 -0.351370  0.195784  0.864564  1.000000   \n",
       "V9  -0.155929  0.292977  0.186230  0.361922 -0.256294 -0.449935 -0.537900   \n",
       "V10  0.136698 -0.220746  0.009652 -0.197327  0.236441  0.612413  0.652692   \n",
       "V11  0.546364  0.248985  0.258887  0.018732  0.199950 -0.055136 -0.172379   \n",
       "V12 -0.071747 -0.561296 -0.074667 -0.273955  0.055398  0.433681  0.543479   \n",
       "V13  0.072343 -0.368710  0.003911 -0.276769  0.066004  0.699949  0.787194   \n",
       "V14  0.643720 -0.192011  0.223626 -0.440597  0.393351  0.498115  0.494193   \n",
       "\n",
       "           V9       V10       V11       V12       V13       V14  \n",
       "V2  -0.155929  0.136698  0.546364 -0.071747  0.072343  0.643720  \n",
       "V3   0.292977 -0.220746  0.248985 -0.561296 -0.368710 -0.192011  \n",
       "V4   0.186230  0.009652  0.258887 -0.074667  0.003911  0.223626  \n",
       "V5   0.361922 -0.197327  0.018732 -0.273955 -0.276769 -0.440597  \n",
       "V6  -0.256294  0.236441  0.199950  0.055398  0.066004  0.393351  \n",
       "V7  -0.449935  0.612413 -0.055136  0.433681  0.699949  0.498115  \n",
       "V8  -0.537900  0.652692 -0.172379  0.543479  0.787194  0.494193  \n",
       "V9   1.000000 -0.365845  0.139057 -0.262640 -0.503270 -0.311385  \n",
       "V10 -0.365845  1.000000 -0.025250  0.295544  0.519067  0.330417  \n",
       "V11  0.139057 -0.025250  1.000000 -0.521813 -0.428815  0.316100  \n",
       "V12 -0.262640  0.295544 -0.521813  1.000000  0.565468  0.236183  \n",
       "V13 -0.503270  0.519067 -0.428815  0.565468  1.000000  0.312761  \n",
       "V14 -0.311385  0.330417  0.316100  0.236183  0.312761  1.000000  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrmat = X.corr()\n",
    "corrmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A better graphical representation of the correlation matrix is via a correlation matrix plot in the form of a *heatmap*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(corrmat, vmax=1., square=False).xaxis.tick_top()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or an alternative nice visualization is via a Hinton diagram. The color of the boxes determines the sign of the correlation, in this case red for positive and blue for negative correlations; while the size of the boxes determines their magnitude, the bigger the box the higher the magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# adapted from http://matplotlib.org/examples/specialty_plots/hinton_demo.html\n",
    "def hinton(matrix, max_weight=None, ax=None):\n",
    "    \"\"\"Draw Hinton diagram for visualizing a weight matrix.\"\"\"\n",
    "    ax = ax if ax is not None else plt.gca()\n",
    "\n",
    "    if not max_weight:\n",
    "        max_weight = 2**np.ceil(np.log(np.abs(matrix).max())/np.log(2))\n",
    "\n",
    "    ax.patch.set_facecolor('lightgray')\n",
    "    ax.set_aspect('equal', 'box')\n",
    "    ax.xaxis.set_major_locator(plt.NullLocator())\n",
    "    ax.yaxis.set_major_locator(plt.NullLocator())\n",
    "\n",
    "    for (x, y), w in np.ndenumerate(matrix):\n",
    "        color = 'red' if w > 0 else 'blue'\n",
    "        size = np.sqrt(np.abs(w))\n",
    "        rect = plt.Rectangle([x - size / 2, y - size / 2], size, size,\n",
    "                             facecolor=color, edgecolor=color)\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    nticks = matrix.shape[0]\n",
    "    ax.xaxis.tick_top()\n",
    "    ax.set_xticks(range(nticks))\n",
    "    ax.set_xticklabels(list(matrix.columns), rotation=90)\n",
    "    ax.set_yticks(range(nticks))\n",
    "    ax.set_yticklabels(matrix.columns)\n",
    "    ax.grid(False)\n",
    "\n",
    "    ax.autoscale_view()\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "hinton(corrmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the correlation matrix and diagrams are useful for quickly looking to identify the strongest correlations, they still require labor work to find the top `N` strongest correlations. For this you can use the function `mosthighlycorrelated()` below.\n",
    "\n",
    "The function `mosthighlycorrelated()` will print out the linear correlation coefficients for each pair of variables in your data set, in order of the correlation coefficient. This lets you see very easily which pair of variables are most highly correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mosthighlycorrelated(mydataframe, numtoreport):\n",
    "    # find the correlations\n",
    "    cormatrix = mydataframe.corr()\n",
    "    # set the correlations on the diagonal or lower triangle to zero,\n",
    "    # so they will not be reported as the highest ones:\n",
    "    cormatrix *= np.tri(*cormatrix.values.shape, k=-1).T\n",
    "    # find the top n correlations\n",
    "    cormatrix = cormatrix.stack()\n",
    "    cormatrix = cormatrix.reindex(cormatrix.abs().sort_values(ascending=False).index).reset_index()\n",
    "    # assign human-friendly names\n",
    "    cormatrix.columns = [\"FirstVariable\", \"SecondVariable\", \"Correlation\"]\n",
    "    return cormatrix.head(numtoreport)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arguments of the function are the variables that you want to calculate the correlations for, and the number of top correlation coefficients to print out (for example, you can tell it to print out the largest 10 correlation coefficients, or the largest 20).\n",
    "\n",
    "For example, to calculate correlation coefficients between the concentrations of the 13 chemicals in the wine samples, and to print out the top 10 pairwise correlation coefficients, you can type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FirstVariable</th>\n",
       "      <th>SecondVariable</th>\n",
       "      <th>Correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>V7</td>\n",
       "      <td>V8</td>\n",
       "      <td>0.864564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>V8</td>\n",
       "      <td>V13</td>\n",
       "      <td>0.787194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>V7</td>\n",
       "      <td>V13</td>\n",
       "      <td>0.699949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>V8</td>\n",
       "      <td>V10</td>\n",
       "      <td>0.652692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>V2</td>\n",
       "      <td>V14</td>\n",
       "      <td>0.643720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>V7</td>\n",
       "      <td>V10</td>\n",
       "      <td>0.612413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>V12</td>\n",
       "      <td>V13</td>\n",
       "      <td>0.565468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>V3</td>\n",
       "      <td>V12</td>\n",
       "      <td>-0.561296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>V2</td>\n",
       "      <td>V11</td>\n",
       "      <td>0.546364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>V8</td>\n",
       "      <td>V12</td>\n",
       "      <td>0.543479</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  FirstVariable SecondVariable  Correlation\n",
       "0            V7             V8     0.864564\n",
       "1            V8            V13     0.787194\n",
       "2            V7            V13     0.699949\n",
       "3            V8            V10     0.652692\n",
       "4            V2            V14     0.643720\n",
       "5            V7            V10     0.612413\n",
       "6           V12            V13     0.565468\n",
       "7            V3            V12    -0.561296\n",
       "8            V2            V11     0.546364\n",
       "9            V8            V12     0.543479"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mosthighlycorrelated(X, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that the pair of variables with the highest linear correlation coefficient are V7 and V8 (correlation = 0.86 approximately)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Standardizing Variables\n",
    "\n",
    "If you want to compare different variables that have different units, are very different variances, it is a good idea to first standardize these variables.\n",
    "\n",
    "For example, we found above that the concentrations of the 13 chemicals in the wine samples show a wide range of standard deviations, from 0.124103 for V9 (variance 0.015402) to 314.021657 for V14 (variance 98609.60). This is a range of approximately 6million+-fold in the variances.\n",
    "\n",
    "As a result, it is not a good idea to use the unstandardised chemical concentrations as the input for a principal component analysis of the wine samples. If you did that, the first principal component would be dominated by the variables which show the largest variances, such as V14.\n",
    "\n",
    "Thus, it would be a better idea to first standardize the variables so that they all have variance 1 and mean 0, and to then carry out the principal component analysis on the standardized data. This would allow us to find the principal components that provide the best low-dimensional representation of the variation in the original data, without being overly biased by those variables that show the most variance in the original data.\n",
    "\n",
    "You can standardise variables by using the `scale()` function from the package `sklearn.preprocessing`.\n",
    "\n",
    "For example, to standardise the concentrations of the 13 chemicals in the wine samples, we type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rajku\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by the scale function.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "standardizedX = scale(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.518613</td>\n",
       "      <td>-0.562250</td>\n",
       "      <td>0.232053</td>\n",
       "      <td>-1.169593</td>\n",
       "      <td>1.913905</td>\n",
       "      <td>0.808997</td>\n",
       "      <td>1.034819</td>\n",
       "      <td>-0.659563</td>\n",
       "      <td>1.224884</td>\n",
       "      <td>0.251717</td>\n",
       "      <td>0.362177</td>\n",
       "      <td>1.847920</td>\n",
       "      <td>1.013009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.246290</td>\n",
       "      <td>-0.499413</td>\n",
       "      <td>-0.827996</td>\n",
       "      <td>-2.490847</td>\n",
       "      <td>0.018145</td>\n",
       "      <td>0.568648</td>\n",
       "      <td>0.733629</td>\n",
       "      <td>-0.820719</td>\n",
       "      <td>-0.544721</td>\n",
       "      <td>-0.293321</td>\n",
       "      <td>0.406051</td>\n",
       "      <td>1.113449</td>\n",
       "      <td>0.965242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.196879</td>\n",
       "      <td>0.021231</td>\n",
       "      <td>1.109334</td>\n",
       "      <td>-0.268738</td>\n",
       "      <td>0.088358</td>\n",
       "      <td>0.808997</td>\n",
       "      <td>1.215533</td>\n",
       "      <td>-0.498407</td>\n",
       "      <td>2.135968</td>\n",
       "      <td>0.269020</td>\n",
       "      <td>0.318304</td>\n",
       "      <td>0.788587</td>\n",
       "      <td>1.395148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.691550</td>\n",
       "      <td>-0.346811</td>\n",
       "      <td>0.487926</td>\n",
       "      <td>-0.809251</td>\n",
       "      <td>0.930918</td>\n",
       "      <td>2.491446</td>\n",
       "      <td>1.466525</td>\n",
       "      <td>-0.981875</td>\n",
       "      <td>1.032155</td>\n",
       "      <td>1.186068</td>\n",
       "      <td>-0.427544</td>\n",
       "      <td>1.184071</td>\n",
       "      <td>2.334574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.295700</td>\n",
       "      <td>0.227694</td>\n",
       "      <td>1.840403</td>\n",
       "      <td>0.451946</td>\n",
       "      <td>1.281985</td>\n",
       "      <td>0.808997</td>\n",
       "      <td>0.663351</td>\n",
       "      <td>0.226796</td>\n",
       "      <td>0.401404</td>\n",
       "      <td>-0.319276</td>\n",
       "      <td>0.362177</td>\n",
       "      <td>0.449601</td>\n",
       "      <td>-0.037874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.481555</td>\n",
       "      <td>-0.517367</td>\n",
       "      <td>0.305159</td>\n",
       "      <td>-1.289707</td>\n",
       "      <td>0.860705</td>\n",
       "      <td>1.562093</td>\n",
       "      <td>1.366128</td>\n",
       "      <td>-0.176095</td>\n",
       "      <td>0.664217</td>\n",
       "      <td>0.731870</td>\n",
       "      <td>0.406051</td>\n",
       "      <td>0.336606</td>\n",
       "      <td>2.239039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.716255</td>\n",
       "      <td>-0.418624</td>\n",
       "      <td>0.305159</td>\n",
       "      <td>-1.469878</td>\n",
       "      <td>-0.262708</td>\n",
       "      <td>0.328298</td>\n",
       "      <td>0.492677</td>\n",
       "      <td>-0.498407</td>\n",
       "      <td>0.681738</td>\n",
       "      <td>0.083015</td>\n",
       "      <td>0.274431</td>\n",
       "      <td>1.367689</td>\n",
       "      <td>1.729520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.308617</td>\n",
       "      <td>-0.167278</td>\n",
       "      <td>0.890014</td>\n",
       "      <td>-0.569023</td>\n",
       "      <td>1.492625</td>\n",
       "      <td>0.488531</td>\n",
       "      <td>0.482637</td>\n",
       "      <td>-0.417829</td>\n",
       "      <td>-0.597284</td>\n",
       "      <td>-0.003499</td>\n",
       "      <td>0.449924</td>\n",
       "      <td>1.367689</td>\n",
       "      <td>1.745442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.259772</td>\n",
       "      <td>-0.625086</td>\n",
       "      <td>-0.718336</td>\n",
       "      <td>-1.650049</td>\n",
       "      <td>-0.192495</td>\n",
       "      <td>0.808997</td>\n",
       "      <td>0.954502</td>\n",
       "      <td>-0.578985</td>\n",
       "      <td>0.681738</td>\n",
       "      <td>0.061386</td>\n",
       "      <td>0.537671</td>\n",
       "      <td>0.336606</td>\n",
       "      <td>0.949319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.061565</td>\n",
       "      <td>-0.885409</td>\n",
       "      <td>-0.352802</td>\n",
       "      <td>-1.049479</td>\n",
       "      <td>-0.122282</td>\n",
       "      <td>1.097417</td>\n",
       "      <td>1.125176</td>\n",
       "      <td>-1.143031</td>\n",
       "      <td>0.453967</td>\n",
       "      <td>0.935177</td>\n",
       "      <td>0.230557</td>\n",
       "      <td>1.325316</td>\n",
       "      <td>0.949319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.715690</td>\n",
       "      <td>0.218717</td>\n",
       "      <td>1.182441</td>\n",
       "      <td>1.502943</td>\n",
       "      <td>0.369212</td>\n",
       "      <td>-1.193917</td>\n",
       "      <td>-1.193987</td>\n",
       "      <td>0.226796</td>\n",
       "      <td>-0.089179</td>\n",
       "      <td>1.558078</td>\n",
       "      <td>-0.954024</td>\n",
       "      <td>-1.146459</td>\n",
       "      <td>0.009893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>0.493343</td>\n",
       "      <td>2.031997</td>\n",
       "      <td>1.803849</td>\n",
       "      <td>1.653086</td>\n",
       "      <td>0.860705</td>\n",
       "      <td>-0.504914</td>\n",
       "      <td>-1.073511</td>\n",
       "      <td>-0.740141</td>\n",
       "      <td>-0.842575</td>\n",
       "      <td>1.488867</td>\n",
       "      <td>-1.261138</td>\n",
       "      <td>-0.976966</td>\n",
       "      <td>-0.372246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>-0.988975</td>\n",
       "      <td>0.622666</td>\n",
       "      <td>-0.170035</td>\n",
       "      <td>-0.148624</td>\n",
       "      <td>-0.262708</td>\n",
       "      <td>-1.674617</td>\n",
       "      <td>-1.545376</td>\n",
       "      <td>0.307374</td>\n",
       "      <td>-1.508367</td>\n",
       "      <td>0.191157</td>\n",
       "      <td>-1.305011</td>\n",
       "      <td>-1.104086</td>\n",
       "      <td>-0.754385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>-0.284874</td>\n",
       "      <td>0.048161</td>\n",
       "      <td>-0.316249</td>\n",
       "      <td>0.001518</td>\n",
       "      <td>-0.964842</td>\n",
       "      <td>-1.450290</td>\n",
       "      <td>-1.525296</td>\n",
       "      <td>0.951998</td>\n",
       "      <td>-1.666055</td>\n",
       "      <td>2.094465</td>\n",
       "      <td>-1.699872</td>\n",
       "      <td>-1.386574</td>\n",
       "      <td>-0.881765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>1.432144</td>\n",
       "      <td>0.155881</td>\n",
       "      <td>0.414820</td>\n",
       "      <td>0.151661</td>\n",
       "      <td>-0.613775</td>\n",
       "      <td>-0.985614</td>\n",
       "      <td>-1.334543</td>\n",
       "      <td>0.629686</td>\n",
       "      <td>-0.614804</td>\n",
       "      <td>2.007951</td>\n",
       "      <td>-1.480505</td>\n",
       "      <td>-1.273579</td>\n",
       "      <td>-0.276711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>0.876275</td>\n",
       "      <td>2.974543</td>\n",
       "      <td>0.305159</td>\n",
       "      <td>0.301803</td>\n",
       "      <td>-0.332922</td>\n",
       "      <td>-0.985614</td>\n",
       "      <td>-1.424900</td>\n",
       "      <td>1.274310</td>\n",
       "      <td>-0.930179</td>\n",
       "      <td>1.142811</td>\n",
       "      <td>-1.392758</td>\n",
       "      <td>-1.231206</td>\n",
       "      <td>-0.021952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>0.493343</td>\n",
       "      <td>1.412609</td>\n",
       "      <td>0.414820</td>\n",
       "      <td>1.052516</td>\n",
       "      <td>0.158572</td>\n",
       "      <td>-0.793334</td>\n",
       "      <td>-1.284344</td>\n",
       "      <td>0.549108</td>\n",
       "      <td>-0.316950</td>\n",
       "      <td>0.969783</td>\n",
       "      <td>-1.129518</td>\n",
       "      <td>-1.485445</td>\n",
       "      <td>0.009893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>0.332758</td>\n",
       "      <td>1.744744</td>\n",
       "      <td>-0.389355</td>\n",
       "      <td>0.151661</td>\n",
       "      <td>1.422412</td>\n",
       "      <td>-1.129824</td>\n",
       "      <td>-1.344582</td>\n",
       "      <td>0.549108</td>\n",
       "      <td>-0.422075</td>\n",
       "      <td>2.224236</td>\n",
       "      <td>-1.612125</td>\n",
       "      <td>-1.485445</td>\n",
       "      <td>0.280575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>0.209232</td>\n",
       "      <td>0.227694</td>\n",
       "      <td>0.012732</td>\n",
       "      <td>0.151661</td>\n",
       "      <td>1.422412</td>\n",
       "      <td>-1.033684</td>\n",
       "      <td>-1.354622</td>\n",
       "      <td>1.354888</td>\n",
       "      <td>-0.229346</td>\n",
       "      <td>1.834923</td>\n",
       "      <td>-1.568252</td>\n",
       "      <td>-1.400699</td>\n",
       "      <td>0.296498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>1.395086</td>\n",
       "      <td>1.583165</td>\n",
       "      <td>1.365208</td>\n",
       "      <td>1.502943</td>\n",
       "      <td>-0.262708</td>\n",
       "      <td>-0.392751</td>\n",
       "      <td>-1.274305</td>\n",
       "      <td>1.596623</td>\n",
       "      <td>-0.422075</td>\n",
       "      <td>1.791666</td>\n",
       "      <td>-1.524378</td>\n",
       "      <td>-1.428948</td>\n",
       "      <td>-0.595160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           V2        V3        V4        V5        V6        V7        V8  \\\n",
       "0    1.518613 -0.562250  0.232053 -1.169593  1.913905  0.808997  1.034819   \n",
       "1    0.246290 -0.499413 -0.827996 -2.490847  0.018145  0.568648  0.733629   \n",
       "2    0.196879  0.021231  1.109334 -0.268738  0.088358  0.808997  1.215533   \n",
       "3    1.691550 -0.346811  0.487926 -0.809251  0.930918  2.491446  1.466525   \n",
       "4    0.295700  0.227694  1.840403  0.451946  1.281985  0.808997  0.663351   \n",
       "5    1.481555 -0.517367  0.305159 -1.289707  0.860705  1.562093  1.366128   \n",
       "6    1.716255 -0.418624  0.305159 -1.469878 -0.262708  0.328298  0.492677   \n",
       "7    1.308617 -0.167278  0.890014 -0.569023  1.492625  0.488531  0.482637   \n",
       "8    2.259772 -0.625086 -0.718336 -1.650049 -0.192495  0.808997  0.954502   \n",
       "9    1.061565 -0.885409 -0.352802 -1.049479 -0.122282  1.097417  1.125176   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "168  0.715690  0.218717  1.182441  1.502943  0.369212 -1.193917 -1.193987   \n",
       "169  0.493343  2.031997  1.803849  1.653086  0.860705 -0.504914 -1.073511   \n",
       "170 -0.988975  0.622666 -0.170035 -0.148624 -0.262708 -1.674617 -1.545376   \n",
       "171 -0.284874  0.048161 -0.316249  0.001518 -0.964842 -1.450290 -1.525296   \n",
       "172  1.432144  0.155881  0.414820  0.151661 -0.613775 -0.985614 -1.334543   \n",
       "173  0.876275  2.974543  0.305159  0.301803 -0.332922 -0.985614 -1.424900   \n",
       "174  0.493343  1.412609  0.414820  1.052516  0.158572 -0.793334 -1.284344   \n",
       "175  0.332758  1.744744 -0.389355  0.151661  1.422412 -1.129824 -1.344582   \n",
       "176  0.209232  0.227694  0.012732  0.151661  1.422412 -1.033684 -1.354622   \n",
       "177  1.395086  1.583165  1.365208  1.502943 -0.262708 -0.392751 -1.274305   \n",
       "\n",
       "           V9       V10       V11       V12       V13       V14  \n",
       "0   -0.659563  1.224884  0.251717  0.362177  1.847920  1.013009  \n",
       "1   -0.820719 -0.544721 -0.293321  0.406051  1.113449  0.965242  \n",
       "2   -0.498407  2.135968  0.269020  0.318304  0.788587  1.395148  \n",
       "3   -0.981875  1.032155  1.186068 -0.427544  1.184071  2.334574  \n",
       "4    0.226796  0.401404 -0.319276  0.362177  0.449601 -0.037874  \n",
       "5   -0.176095  0.664217  0.731870  0.406051  0.336606  2.239039  \n",
       "6   -0.498407  0.681738  0.083015  0.274431  1.367689  1.729520  \n",
       "7   -0.417829 -0.597284 -0.003499  0.449924  1.367689  1.745442  \n",
       "8   -0.578985  0.681738  0.061386  0.537671  0.336606  0.949319  \n",
       "9   -1.143031  0.453967  0.935177  0.230557  1.325316  0.949319  \n",
       "..        ...       ...       ...       ...       ...       ...  \n",
       "168  0.226796 -0.089179  1.558078 -0.954024 -1.146459  0.009893  \n",
       "169 -0.740141 -0.842575  1.488867 -1.261138 -0.976966 -0.372246  \n",
       "170  0.307374 -1.508367  0.191157 -1.305011 -1.104086 -0.754385  \n",
       "171  0.951998 -1.666055  2.094465 -1.699872 -1.386574 -0.881765  \n",
       "172  0.629686 -0.614804  2.007951 -1.480505 -1.273579 -0.276711  \n",
       "173  1.274310 -0.930179  1.142811 -1.392758 -1.231206 -0.021952  \n",
       "174  0.549108 -0.316950  0.969783 -1.129518 -1.485445  0.009893  \n",
       "175  0.549108 -0.422075  2.224236 -1.612125 -1.485445  0.280575  \n",
       "176  1.354888 -0.229346  1.834923 -1.568252 -1.400699  0.296498  \n",
       "177  1.596623 -0.422075  1.791666 -1.524378 -1.428948 -0.595160  \n",
       "\n",
       "[178 rows x 13 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standardizedX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178, 13)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardizedX = pd.DataFrame(standardizedX, index=X.index, columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "V2    -8.619821e-16\n",
       "V3    -8.357859e-17\n",
       "V4    -8.657245e-16\n",
       "V5    -1.160121e-16\n",
       "V6    -1.995907e-17\n",
       "V7    -2.972030e-16\n",
       "V8    -4.016762e-16\n",
       "V9     4.079134e-16\n",
       "V10   -1.699639e-16\n",
       "V11   -1.247442e-18\n",
       "V12    3.717376e-16\n",
       "V13    2.919013e-16\n",
       "V14   -7.484650e-18\n",
       "dtype: float64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standardizedX.apply(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "V2     1.0\n",
       "V3     1.0\n",
       "V4     1.0\n",
       "V5     1.0\n",
       "V6     1.0\n",
       "V7     1.0\n",
       "V8     1.0\n",
       "V9     1.0\n",
       "V10    1.0\n",
       "V11    1.0\n",
       "V12    1.0\n",
       "V13    1.0\n",
       "V14    1.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standardizedX.apply(np.std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis\n",
    "\n",
    "The purpose of principal component analysis is to find the best low-dimensional representation of the variation in a multivariate data set. For example, in the case of the wine data set, we have 13 chemical concentrations describing wine samples from three different cultivars. We can carry out a principal component analysis to investigate whether we can capture most of the variation between samples using a smaller number of new variables (principal components), where each of these new variables is a linear combination of all or some of the 13 chemical concentrations.\n",
    "\n",
    "To carry out a principal component analysis (PCA) on a multivariate data set, the first step is often to standardise the variables under study using the `scale()` function (see above). This is necessary if the input variables have very different variances, which is true in this case as the concentrations of the 13 chemicals have very different variances (see above).\n",
    "\n",
    "Once you have standardized your variables, you can carry out a principal component analysis using the `PCA` class from `sklearn.decomposition` package and its `fit` method, which fits the model with the data `X`. The default `solver` is Singular Value Decomposition (\"svd\"). For more information you can type `help(PCA)` in the python console.\n",
    "\n",
    "For example, to standardize the concentrations of the 13 chemicals in the wine samples, and carry out a principal components analysis on the standardizsed concentrations, we type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA().fit(standardizedX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.36198848 0.1920749  0.11123631 0.0706903  0.06563294 0.04935823\n",
      " 0.04238679 0.02680749 0.02222153 0.01930019 0.01736836 0.01298233\n",
      " 0.00795215]\n"
     ]
    }
   ],
   "source": [
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get a summary of the principal component analysis results using the `pca_summary()` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_summary(pca, standardized_data, out=True):\n",
    "    names = [\"PC\"+str(i) for i in range(1, len(pca.explained_variance_ratio_)+1)]\n",
    "    a = list(np.std(pca.transform(standardized_data), axis=0))\n",
    "    b = list(pca.explained_variance_ratio_)\n",
    "    c = [np.sum(pca.explained_variance_ratio_[:i]) for i in range(1, len(pca.explained_variance_ratio_)+1)]\n",
    "    columns = pd.MultiIndex.from_tuples([(\"sdev\", \"Standard deviation\"), (\"varprop\", \"Proportion of Variance\"), (\"cumprop\", \"Cumulative Proportion\")])\n",
    "    summary = pd.DataFrame(list(zip(a, b, c)), index=names, columns=columns)\n",
    "    \n",
    "    if out:\n",
    "        print(\"Importance of components:\")\n",
    "        display(summary)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters of the `print_pca_summary` function are:\n",
    "* `pca`: A PCA object\n",
    "* `standardised_data`: The standardised data\n",
    "* `out (True)`: Print to standard output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importance of components:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>sdev</th>\n",
       "      <th>varprop</th>\n",
       "      <th>cumprop</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Standard deviation</th>\n",
       "      <th>Proportion of Variance</th>\n",
       "      <th>Cumulative Proportion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PC1</th>\n",
       "      <td>2.169297</td>\n",
       "      <td>0.361988</td>\n",
       "      <td>0.361988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC2</th>\n",
       "      <td>1.580182</td>\n",
       "      <td>0.192075</td>\n",
       "      <td>0.554063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC3</th>\n",
       "      <td>1.202527</td>\n",
       "      <td>0.111236</td>\n",
       "      <td>0.665300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC4</th>\n",
       "      <td>0.958631</td>\n",
       "      <td>0.070690</td>\n",
       "      <td>0.735990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC5</th>\n",
       "      <td>0.923704</td>\n",
       "      <td>0.065633</td>\n",
       "      <td>0.801623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC6</th>\n",
       "      <td>0.801035</td>\n",
       "      <td>0.049358</td>\n",
       "      <td>0.850981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC7</th>\n",
       "      <td>0.742313</td>\n",
       "      <td>0.042387</td>\n",
       "      <td>0.893368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC8</th>\n",
       "      <td>0.590337</td>\n",
       "      <td>0.026807</td>\n",
       "      <td>0.920175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC9</th>\n",
       "      <td>0.537476</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.942397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC10</th>\n",
       "      <td>0.500902</td>\n",
       "      <td>0.019300</td>\n",
       "      <td>0.961697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC11</th>\n",
       "      <td>0.475172</td>\n",
       "      <td>0.017368</td>\n",
       "      <td>0.979066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC12</th>\n",
       "      <td>0.410817</td>\n",
       "      <td>0.012982</td>\n",
       "      <td>0.992048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC13</th>\n",
       "      <td>0.321524</td>\n",
       "      <td>0.007952</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   sdev                varprop               cumprop\n",
       "     Standard deviation Proportion of Variance Cumulative Proportion\n",
       "PC1            2.169297               0.361988              0.361988\n",
       "PC2            1.580182               0.192075              0.554063\n",
       "PC3            1.202527               0.111236              0.665300\n",
       "PC4            0.958631               0.070690              0.735990\n",
       "PC5            0.923704               0.065633              0.801623\n",
       "PC6            0.801035               0.049358              0.850981\n",
       "PC7            0.742313               0.042387              0.893368\n",
       "PC8            0.590337               0.026807              0.920175\n",
       "PC9            0.537476               0.022222              0.942397\n",
       "PC10           0.500902               0.019300              0.961697\n",
       "PC11           0.475172               0.017368              0.979066\n",
       "PC12           0.410817               0.012982              0.992048\n",
       "PC13           0.321524               0.007952              1.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary = pca_summary(pca, standardizedX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us the standard deviation of each component, and the proportion of variance explained by each component. The standard deviation of the components is stored in a named row called `sdev` of the output variable made by the `pca_summary` function and stored in the `summary` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Standard deviation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PC1</th>\n",
       "      <td>2.169297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC2</th>\n",
       "      <td>1.580182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC3</th>\n",
       "      <td>1.202527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC4</th>\n",
       "      <td>0.958631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC5</th>\n",
       "      <td>0.923704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC6</th>\n",
       "      <td>0.801035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC7</th>\n",
       "      <td>0.742313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC8</th>\n",
       "      <td>0.590337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC9</th>\n",
       "      <td>0.537476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC10</th>\n",
       "      <td>0.500902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC11</th>\n",
       "      <td>0.475172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC12</th>\n",
       "      <td>0.410817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC13</th>\n",
       "      <td>0.321524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Standard deviation\n",
       "PC1             2.169297\n",
       "PC2             1.580182\n",
       "PC3             1.202527\n",
       "PC4             0.958631\n",
       "PC5             0.923704\n",
       "PC6             0.801035\n",
       "PC7             0.742313\n",
       "PC8             0.590337\n",
       "PC9             0.537476\n",
       "PC10            0.500902\n",
       "PC11            0.475172\n",
       "PC12            0.410817\n",
       "PC13            0.321524"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary.sdev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total variance explained by the components is the sum of the variances of the components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Standard deviation    13.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(summary.sdev**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we see that the total variance is 13, which is equal to the number of standardised variables (13 variables). This is because for standardised data, the variance of each standardised variable is 1. The total variance is equal to the sum of the variances of the individual variables, and since the variance of each standardised variable is 1, the total variance should be equal to the number of variables (13 here)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Deciding How Many Principal Components to Retain\n",
    "\n",
    "In order to decide how many principal components should be retained, it is common to summarize the results of a principal components analysis by making a scree plot, which we can do using the `screeplot()` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def screeplot(pca, standardized_values):\n",
    "    y = np.std(pca.transform(standardized_values), axis=0)**2\n",
    "    x = np.arange(len(y)) + 1\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.plot(x, y, \"o-\")\n",
    "    plt.xticks(x, [\"Comp.\"+str(i) for i in x], rotation=60)\n",
    "    plt.ylabel(\"Variance\")\n",
    "    plt.show()\n",
    "\n",
    "screeplot(pca, standardizedX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most obvious change in slope in the scree plot occurs at component 4, which is the \"elbow\" of the scree plot. Therefore, it cound be argued based on the basis of the scree plot that the first three components should be retained.\n",
    "\n",
    "Another way of deciding how many components to retain is to use *Kaiser’s criterion*: that we should only retain principal components for which the variance is above 1 (when principal component analysis was applied to standardised data). We can check this by finding the variance of each of the principal components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Standard deviation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PC1</th>\n",
       "      <td>4.705850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC2</th>\n",
       "      <td>2.496974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC3</th>\n",
       "      <td>1.446072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC4</th>\n",
       "      <td>0.918974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC5</th>\n",
       "      <td>0.853228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC6</th>\n",
       "      <td>0.641657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC7</th>\n",
       "      <td>0.551028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC8</th>\n",
       "      <td>0.348497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC9</th>\n",
       "      <td>0.288880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC10</th>\n",
       "      <td>0.250902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC11</th>\n",
       "      <td>0.225789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC12</th>\n",
       "      <td>0.168770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC13</th>\n",
       "      <td>0.103378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Standard deviation\n",
       "PC1             4.705850\n",
       "PC2             2.496974\n",
       "PC3             1.446072\n",
       "PC4             0.918974\n",
       "PC5             0.853228\n",
       "PC6             0.641657\n",
       "PC7             0.551028\n",
       "PC8             0.348497\n",
       "PC9             0.288880\n",
       "PC10            0.250902\n",
       "PC11            0.225789\n",
       "PC12            0.168770\n",
       "PC13            0.103378"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary.sdev**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the variance is above 1 for principal components 1, 2, and 3 (which have variances 4.71, 2.50, and 1.45, respectively). Therefore, using Kaiser’s criterion, we would retain the first three principal components.\n",
    "\n",
    "A third way to decide how many principal components to retain is to decide to keep the number of components required to explain at least some minimum amount of the total variance. For example, if it is important to explain at least 80% of the variance, we would retain the first five principal components, as we can see from cumulative proportions (`summary.cumprop`) that the first five principal components explain 80.2% of the variance (while the first four components explain just 73.6%, so are not sufficient)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loadings for the Principal Components\n",
    "\n",
    "The loadings for the principal components are stored in a named element `components_` of the variable returned by `PCA().fit()`. This contains a matrix with the loadings of each principal component, where the first column in the matrix contains the loadings for the first principal component, the second column contains the loadings for the second principal component, and so on.\n",
    "\n",
    "Therefore, to obtain the loadings for the first principal component in our analysis of the 13 chemical concentrations in wine samples, we type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.1443294 , -0.24518758, -0.00205106, -0.23932041,  0.14199204,\n",
       "        0.39466085,  0.4229343 , -0.2985331 ,  0.31342949, -0.0886167 ,\n",
       "        0.29671456,  0.37616741,  0.28675223])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.components_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that the first principal component is a linear combination of the variables:\n",
    "```\n",
    "-0.144*Z2 + 0.245*Z3 + 0.002*Z4 + 0.239*Z5 - 0.142*Z6 - 0.395*Z7 - 0.423*Z8 + 0.299*Z9 -0.313*Z10 + 0.089*Z11 - 0.297*Z12 - 0.376*Z13 - 0.287*Z14\n",
    "```\n",
    "\n",
    "where Z2, Z3, Z4, ..., Z14 are the standardised versions of the variables V2, V3, V4, ..., V14 (that each have mean of 0 and variance of 1).\n",
    "\n",
    "Note that the square of the loadings sum to 1, as this is a constraint used in calculating the loadings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999993"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(pca.components_[0]**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the values of the first principal component, we can define our own function to calculate a principal component given the loadings and the input variables' values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcpc(variables, loadings):\n",
    "    # find the number of samples in the data set and the number of variables\n",
    "    numsamples, numvariables = variables.shape\n",
    "    # make a vector to store the component\n",
    "    pc = np.zeros(numsamples)\n",
    "    # calculate the value of the component for each sample\n",
    "    for i in range(numsamples):\n",
    "        valuei = 0\n",
    "        for j in range(numvariables):\n",
    "            valueij = variables.iloc[i, j]\n",
    "            loadingj = loadings[j]\n",
    "            valuei = valuei + (valueij * loadingj)\n",
    "        pc[i] = valuei\n",
    "    return pc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use the function to calculate the values of the first principal component for each sample in our wine data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.31675081,  2.20946492,  2.51674015,  3.75706561,  1.00890849,\n",
       "        3.05025392,  2.44908967,  2.05943687,  2.5108743 ,  2.75362819,\n",
       "        3.47973668,  1.7547529 ,  2.11346234,  3.45815682,  4.31278391,\n",
       "        2.3051882 ,  2.17195527,  1.89897118,  3.54198508,  2.0845222 ,\n",
       "        3.12440254,  1.08657007,  2.53522408,  1.64498834,  1.76157587,\n",
       "        0.9900791 ,  1.77527763,  1.23542396,  2.18840633,  2.25610898,\n",
       "        2.50022003,  2.67741105,  1.62857912,  1.90269086,  1.41038853,\n",
       "        1.90382623,  1.38486223,  1.12220741,  1.5021945 ,  2.52980109,\n",
       "        2.58809543,  0.66848199,  3.07080699,  0.46220914,  2.10135193,\n",
       "        1.13616618,  2.72660096,  2.82133927,  2.00985085,  2.7074913 ,\n",
       "        3.21491747,  2.85895983,  3.50560436,  2.22479138,  2.14698782,\n",
       "        2.46932948,  2.74151791,  2.17374092,  3.13938015, -0.92858197,\n",
       "       -1.54248014, -1.83624976,  0.03060683,  2.05026161, -0.60968083,\n",
       "        0.90022784,  2.24850719,  0.18338403, -0.81280503,  1.9756205 ,\n",
       "       -1.57221622,  1.65768181, -0.72537239,  2.56222717,  1.83256757,\n",
       "       -0.8679929 ,  0.3700144 , -1.45737704,  1.26293085,  0.37615037,\n",
       "        0.7620639 ,  1.03457797, -0.49487676, -2.53897708,  0.83532015,\n",
       "        0.78790461, -0.80683216, -0.55804262, -1.11511104, -0.55572283,\n",
       "       -1.34928528, -1.56448261, -1.93255561,  0.74666594,  0.95745536,\n",
       "        2.54386518, -0.54395259,  1.03104975,  2.25190942,  1.41021602,\n",
       "        0.79771979, -0.54953173, -0.16117374, -0.65979494,  0.39235441,\n",
       "       -1.77249908, -0.36626736, -1.62067257,  0.08253578,  1.57827507,\n",
       "        1.42056925, -0.27870275, -1.30314497, -0.45707187, -0.49418585,\n",
       "        0.48207441, -0.25288888, -0.10722764, -2.4330126 , -0.55108954,\n",
       "        0.73962193,  1.33632173, -1.177087  , -0.46233501,  0.97847408,\n",
       "       -0.09680973,  0.03848715, -1.5971585 , -0.47956492, -1.79283347,\n",
       "       -1.32710166, -2.38450083, -2.9369401 , -2.14681113, -2.36986949,\n",
       "       -3.06384157, -3.91575378, -3.93646339, -3.09427612, -2.37447163,\n",
       "       -2.77881295, -2.28656128, -2.98563349, -2.3751947 , -2.20986553,\n",
       "       -2.625621  , -4.28063878, -3.58264137, -2.80706372, -2.89965933,\n",
       "       -2.32073698, -2.54983095, -1.81254128, -2.76014464, -2.7371505 ,\n",
       "       -3.60486887, -2.889826  , -3.39215608, -1.0481819 , -1.60991228,\n",
       "       -3.14313097, -2.2401569 , -2.84767378, -2.59749706, -2.94929937,\n",
       "       -3.53003227, -2.40611054, -2.92908473, -2.18141278, -2.38092779,\n",
       "       -3.21161722, -3.67791872, -2.4655558 , -3.37052415, -2.60195585,\n",
       "       -2.67783946, -2.38701709, -3.20875816])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calcpc(standardizedX, pca.components_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, the values of the first principal component are computed with the following, so we can compare those values to the ones that we calculated, and they should agree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.transform(standardizedX)[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that they do agree.\n",
    "\n",
    "The first principal component has highest (in absolute value) loadings for \n",
    "    - V8 (-0.423), \n",
    "    - V7 (-0.395), \n",
    "    - V13 (-0.376), \n",
    "    - V10 (-0.313), \n",
    "    - V12 (-0.297), \n",
    "    - V14 (-0.287), \n",
    "    - V9 (0.299), \n",
    "    - V3 (0.245), and \n",
    "    - V5 (0.239). \n",
    "    \n",
    "The loadings for V8, V7, V13, V10, V12 and V14 are negative, while those for V9, V3, and V5 are positive. \n",
    "\n",
    "Therefore, an interpretation of the first principal component is that it represents a contrast between the concentrations of V8, V7, V13, V10, V12, and V14, and the concentrations of V9, V3 and V5.\n",
    "\n",
    "Similarly, we can obtain the loadings for the second principal component by typing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that the second principal component is a linear combination of the variables: \n",
    "\n",
    "```\n",
    "0.484*Z2 + 0.225*Z3 + 0.316*Z4 - 0.011*Z5 + 0.300*Z6 + 0.065*Z7 - 0.003*Z8 + 0.029*Z9 + 0.039*Z10 + 0.530*Z11 - 0.279*Z12 - 0.164*Z13 + 0.365*Z14\n",
    "```\n",
    "\n",
    "where Z1, Z2, Z3, ..., Z14 are the standardized versions of variables V2, V3, ..., V14 that each have mean 0 and variance 1.\n",
    "\n",
    "Note again that the square of the loadings sum to 1, as above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(pca.components_[1]**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second principal component has highest loadings for \n",
    "    - V11 (0.530), \n",
    "    - V2 (0.484), \n",
    "    - V14 (0.365), \n",
    "    - V4 (0.316), \n",
    "    - V6 (0.300), \n",
    "    - V12 (-0.279), and \n",
    "    - V3 (0.225). \n",
    "    \n",
    "The loadings for V11, V2, V14, V4, V6 and V3 are positive, while the loading for V12 is negative. \n",
    "\n",
    "Therefore, an interpretation of the second principal component is that it represents a contrast between the concentrations of V11, V2, V14, V4, V6 and V3, and the concentration of V12. Note that the loadings for V11 (0.530) and V2 (0.484) are the largest, so the contrast is mainly between the concentrations of V11 and V2, and the concentration of V12."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scatterplots of the Principal Components\n",
    "\n",
    "\n",
    "The values of the principal components can be computed by the `transform()` (or `fit_transform()`) method of the `PCA` class. It returns a matrix with the principal components, where the first column in the matrix contains the first principal component, the second column the second component, and so on.\n",
    "\n",
    "Thus, in our example, `pca.transform(standardizedX)[:, 0]` contains the first principal component, and `pca.transform(standardizedX)[:, 1]` contains the second principal component.\n",
    "\n",
    "We can make a scatterplot of the first two principal components, and label the data points with the cultivar that the wine samples come from, by typing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_scatter(pca, standardized_values, classifications):\n",
    "    foo = pca.transform(standardized_values)\n",
    "    bar = pd.DataFrame(list(zip(foo[:, 0], foo[:, 1], classifications)), columns=[\"PC1\", \"PC2\", \"Class\"])\n",
    "    sns.lmplot(\"PC1\", \"PC2\", bar, hue=\"Class\", fit_reg=False)\n",
    "\n",
    "pca_scatter(pca, standardizedX, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatterplot shows the first principal component on the x-axis, and the second principal component on the y-axis. We can see from the scatterplot that wine samples of cultivar 1 have much lower values of the first principal component than wine samples of cultivar 3. Therefore, the first principal component separates wine samples of cultivars 1 from those of cultivar 3.\n",
    "\n",
    "We can also see that wine samples of cultivar 2 have much higher values of the second principal component than wine samples of cultivars 1 and 3. Therefore, the second principal component separates samples of cultivar 2 from samples of cultivars 1 and 3.\n",
    "\n",
    "Therefore, the first two principal components are reasonably useful for distinguishing wine samples of the three different cultivars.\n",
    "\n",
    "Above, we interpreted the first principal component as a contrast between the concentrations of V8, V7, V13, V10, V12, and V14, and the concentrations of V9, V3 and V5. We can check whether this makes sense in terms of the concentrations of these chemicals in the different cultivars, by printing out the means of the standardized concentration variables in each cultivar, using the `printMeanAndSdByGroup()` function (see above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Means:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.919195</td>\n",
       "      <td>-0.292342</td>\n",
       "      <td>0.325604</td>\n",
       "      <td>-0.737997</td>\n",
       "      <td>0.463226</td>\n",
       "      <td>0.873362</td>\n",
       "      <td>0.956884</td>\n",
       "      <td>-0.578985</td>\n",
       "      <td>0.540383</td>\n",
       "      <td>0.203401</td>\n",
       "      <td>0.458847</td>\n",
       "      <td>0.771351</td>\n",
       "      <td>1.174501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.891720</td>\n",
       "      <td>-0.362362</td>\n",
       "      <td>-0.444958</td>\n",
       "      <td>0.223137</td>\n",
       "      <td>-0.364567</td>\n",
       "      <td>-0.058067</td>\n",
       "      <td>0.051780</td>\n",
       "      <td>0.014569</td>\n",
       "      <td>0.069002</td>\n",
       "      <td>-0.852799</td>\n",
       "      <td>0.433611</td>\n",
       "      <td>0.245294</td>\n",
       "      <td>-0.724110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.189159</td>\n",
       "      <td>0.895331</td>\n",
       "      <td>0.257945</td>\n",
       "      <td>0.577065</td>\n",
       "      <td>-0.030127</td>\n",
       "      <td>-0.987617</td>\n",
       "      <td>-1.252761</td>\n",
       "      <td>0.690119</td>\n",
       "      <td>-0.766287</td>\n",
       "      <td>1.011418</td>\n",
       "      <td>-1.205382</td>\n",
       "      <td>-1.310950</td>\n",
       "      <td>-0.372578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          V2        V3        V4        V5        V6        V7        V8  \\\n",
       "V1                                                                         \n",
       "1   0.919195 -0.292342  0.325604 -0.737997  0.463226  0.873362  0.956884   \n",
       "2  -0.891720 -0.362362 -0.444958  0.223137 -0.364567 -0.058067  0.051780   \n",
       "3   0.189159  0.895331  0.257945  0.577065 -0.030127 -0.987617 -1.252761   \n",
       "\n",
       "          V9       V10       V11       V12       V13       V14  \n",
       "V1                                                              \n",
       "1  -0.578985  0.540383  0.203401  0.458847  0.771351  1.174501  \n",
       "2   0.014569  0.069002 -0.852799  0.433611  0.245294 -0.724110  \n",
       "3   0.690119 -0.766287  1.011418 -1.205382 -1.310950 -0.372578  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Standard deviations:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.565989</td>\n",
       "      <td>0.612825</td>\n",
       "      <td>0.823302</td>\n",
       "      <td>0.758115</td>\n",
       "      <td>0.730892</td>\n",
       "      <td>0.538506</td>\n",
       "      <td>0.395674</td>\n",
       "      <td>0.559639</td>\n",
       "      <td>0.715905</td>\n",
       "      <td>0.531210</td>\n",
       "      <td>0.506699</td>\n",
       "      <td>0.500058</td>\n",
       "      <td>0.699428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.659832</td>\n",
       "      <td>0.905196</td>\n",
       "      <td>1.144991</td>\n",
       "      <td>0.998777</td>\n",
       "      <td>1.168006</td>\n",
       "      <td>0.867674</td>\n",
       "      <td>0.703493</td>\n",
       "      <td>0.991797</td>\n",
       "      <td>1.047418</td>\n",
       "      <td>0.397269</td>\n",
       "      <td>0.884060</td>\n",
       "      <td>0.696425</td>\n",
       "      <td>0.497100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.648130</td>\n",
       "      <td>0.966347</td>\n",
       "      <td>0.668036</td>\n",
       "      <td>0.670991</td>\n",
       "      <td>0.756649</td>\n",
       "      <td>0.565996</td>\n",
       "      <td>0.291583</td>\n",
       "      <td>0.989818</td>\n",
       "      <td>0.708814</td>\n",
       "      <td>0.989176</td>\n",
       "      <td>0.496834</td>\n",
       "      <td>0.380317</td>\n",
       "      <td>0.362688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          V2        V3        V4        V5        V6        V7        V8  \\\n",
       "V1                                                                         \n",
       "1   0.565989  0.612825  0.823302  0.758115  0.730892  0.538506  0.395674   \n",
       "2   0.659832  0.905196  1.144991  0.998777  1.168006  0.867674  0.703493   \n",
       "3   0.648130  0.966347  0.668036  0.670991  0.756649  0.565996  0.291583   \n",
       "\n",
       "          V9       V10       V11       V12       V13       V14  \n",
       "V1                                                              \n",
       "1   0.559639  0.715905  0.531210  0.506699  0.500058  0.699428  \n",
       "2   0.991797  1.047418  0.397269  0.884060  0.696425  0.497100  \n",
       "3   0.989818  0.708814  0.989176  0.496834  0.380317  0.362688  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Sample sizes:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V1</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0\n",
       "V1    \n",
       "1   59\n",
       "2   71\n",
       "3   48"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "printMeanAndSdByGroup(standardizedX, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does it make sense that the first principal component can separate cultivar 1 from cultivar 3? \n",
    "    * In cultivar 1, the mean values of V8 (0.954), V7 (0.871), V13 (0.769), V10 (0.539), V12 (0.458) and V14 (1.171) are very high compared to the mean values of V9 (-0.577), V3 (-0.292) and V5 (-0.736). \n",
    "    * In cultivar 3, the mean values of V8 (-1.249), V7 (-0.985), V13 (-1.307), V10 (-0.764), V12 (-1.202) and V14 (-0.372) are very low compared to the mean values of V9 (0.688), V3 (0.893) and V5 (0.575). \n",
    "\n",
    "Therefore, it does make sense that principal component 1 is a contrast between the concentrations of V8, V7, V13, V10, V12, and V14, and the concentrations of V9, V3 and V5; and that principal component 1 can separate cultivar 1 from cultivar 3.\n",
    "\n",
    "Above, we intepreted the second principal component as a contrast between the concentrations of V11, V2, V14, V4, V6 and V3, and the concentration of V12. In the light of the mean values of these variables in the different cultivars, does it make sense that the second principal component can separate cultivar 2 from cultivars 1 and 3? \n",
    "    * In cultivar 1, the mean values of V11 (0.203), V2 (0.917), V14 (1.171), V4 (0.325), V6 (0.462) and V3 (-0.292) are not very different from the mean value of V12 (0.458). \n",
    "    * In cultivar 3, the mean values of V11 (1.009), V2 (0.189), V14 (-0.372), V4 (0.257), V6 (-0.030) and V3 (0.893) are also not very different from the mean value of V12 (-1.202). In contrast, in cultivar 2, the mean values of V11 (-0.850), V2 (-0.889), V14 (-0.722), V4 (-0.444), V6 (-0.364) and V3 (-0.361) are much less than the mean value of V12 (0.432). \n",
    "\n",
    "Therefore, it makes sense that principal component is a contrast between the concentrations of V11, V2, V14, V4, V6 and V3, and the concentration of V12; and that principal component 2 can separate cultivar 2 from cultivars 1 and 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Linear Discriminant Analysis\n",
    "\n",
    "The purpose of principal component analysis is to find the best low-dimensional representation of the variation in a multivariate data set. For example, in the wine data set, we have 13 chemical concentrations describing wine samples from three cultivars. By carrying out a principal component analysis, we found that most of the variation in the chemical concentrations between the samples can be captured using the first two principal components, where each of the principal components is a particular linear combination of the 13 chemical concentrations.\n",
    "\n",
    "The purpose of linear discriminant analysis (LDA) is to find the linear combinations of the original variables (the 13 chemical concentrations here) that gives the best possible separation between the groups (wine cultivars here) in our data set. *Linear discriminant analysis* is also known as *canonical discriminant analysis*, or simply *discriminant analysis*.\n",
    "\n",
    "If we want to separate the wines by cultivar, the wines come from three different cultivars, so the number of groups (G) is 3, and the number of variables is 13 (13 chemicals’ concentrations; p = 13). The maximum number of useful discriminant functions that can separate the wines by cultivar is the minimum of G-1 and p, and so in this case it is the minimum of 2 and 13, which is 2. Thus, we can find at most 2 useful discriminant functions to separate the wines by cultivar, using the 13 chemical concentration variables.\n",
    "\n",
    "You can carry out a linear discriminant analysis by using the `LinearDiscriminantAnalysis` class model from the module `sklearn.discriminant_analysis` and using its method `fit()` to fit our `X, y` data.\n",
    "\n",
    "For example, to carry out a linear discriminant analysis using the 13 chemical concentrations in the wine samples, we type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LinearDiscriminantAnalysis().fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loadings for the Discriminant Functions\n",
    "\n",
    "The values of the loadings of the discriminant functions for the wine data are stored in the `scalings_` member of the `lda` object model. For a pretty print we can type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients of linear discriminants:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LD1</th>\n",
       "      <th>LD2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>V2</th>\n",
       "      <td>-0.403400</td>\n",
       "      <td>0.871793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V3</th>\n",
       "      <td>0.165255</td>\n",
       "      <td>0.305380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V4</th>\n",
       "      <td>-0.369075</td>\n",
       "      <td>2.345850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V5</th>\n",
       "      <td>0.154798</td>\n",
       "      <td>-0.146381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V6</th>\n",
       "      <td>-0.002163</td>\n",
       "      <td>-0.000463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V7</th>\n",
       "      <td>0.618052</td>\n",
       "      <td>-0.032213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V8</th>\n",
       "      <td>-1.661191</td>\n",
       "      <td>-0.491998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V9</th>\n",
       "      <td>-1.495818</td>\n",
       "      <td>-1.630954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V10</th>\n",
       "      <td>0.134093</td>\n",
       "      <td>-0.307088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V11</th>\n",
       "      <td>0.355056</td>\n",
       "      <td>0.253231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V12</th>\n",
       "      <td>-0.818036</td>\n",
       "      <td>-1.515634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V13</th>\n",
       "      <td>-1.157559</td>\n",
       "      <td>0.051184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V14</th>\n",
       "      <td>-0.002691</td>\n",
       "      <td>0.002853</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          LD1       LD2\n",
       "V2  -0.403400  0.871793\n",
       "V3   0.165255  0.305380\n",
       "V4  -0.369075  2.345850\n",
       "V5   0.154798 -0.146381\n",
       "V6  -0.002163 -0.000463\n",
       "V7   0.618052 -0.032213\n",
       "V8  -1.661191 -0.491998\n",
       "V9  -1.495818 -1.630954\n",
       "V10  0.134093 -0.307088\n",
       "V11  0.355056  0.253231\n",
       "V12 -0.818036 -1.515634\n",
       "V13 -1.157559  0.051184\n",
       "V14 -0.002691  0.002853"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def pretty_scalings(lda, X, out=False):\n",
    "    ret = pd.DataFrame(lda.scalings_, index=X.columns, columns=[\"LD\"+str(i+1) for i in range(lda.scalings_.shape[1])])\n",
    "    if out:\n",
    "        print(\"Coefficients of linear discriminants:\")\n",
    "        display(ret)\n",
    "    return ret\n",
    "\n",
    "pretty_scalings_ = pretty_scalings(lda, X, out=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that the first discriminant function is a linear combination of the variables:\n",
    "```\n",
    "-0.403*V2 + 0.165*V3 - 0.369*V4 + 0.155*V5 - 0.002*V6 + 0.618*V7 - 1.661*V8 - 1.496*V9 + 0.134*V10 + 0.355*V11 - 0.818*V12 - 1.158*V13 - 0.003*V14\n",
    "```\n",
    "where V2, V3, ..., V14 are the concentrations of the 14 chemicals found in the wine samples. For convenience, the value for each discriminant function (eg. the first discriminant function) are scaled so that their mean value is zero (see below).\n",
    "\n",
    "Note that these loadings are calculated so that the within-group variance of each discriminant function for each group (cultivar) is equal to 1.\n",
    "\n",
    "As mentioned above, these scalings are stored in the named member `scalings_` of the object variable returned by `LinearDiscriminantAnalysis().fit(X, y)`. This element contains a numpy array, in which the first column contains the loadings for the first discriminant function, the second column contains the loadings for the second discriminant function and so on. For example, to extract the loadings for the first discriminant function, we can type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.40339978,  0.1652546 , -0.36907526,  0.15479789, -0.0021635 ,\n",
       "        0.61805207, -1.66119123, -1.49581844,  0.13409263,  0.35505571,\n",
       "       -0.81803607, -1.15755938, -0.00269121])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.scalings_[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or for \"prettier\" print, use the dataframe variable created above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "V2    -0.403400\n",
       "V3     0.165255\n",
       "V4    -0.369075\n",
       "V5     0.154798\n",
       "V6    -0.002163\n",
       "V7     0.618052\n",
       "V8    -1.661191\n",
       "V9    -1.495818\n",
       "V10    0.134093\n",
       "V11    0.355056\n",
       "V12   -0.818036\n",
       "V13   -1.157559\n",
       "V14   -0.002691\n",
       "Name: LD1, dtype: float64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretty_scalings_.LD1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the values of the first discriminant function, we can define our own function `calclda()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateLDA(variables, loadings):\n",
    "    # find the number of samples in the data set and the number of variables\n",
    "    numsamples, numvariables = variables.shape\n",
    "    # make a vector to store the discriminant function\n",
    "    ld = np.zeros(numsamples)\n",
    "    # calculate the value of the discriminant function for each sample\n",
    "    for i in range(numsamples):\n",
    "        valuei = 0\n",
    "        for j in range(numvariables):\n",
    "            valueij = variables.iloc[i, j]\n",
    "            loadingj = loadings[j]\n",
    "            valuei = valuei + (valueij * loadingj)\n",
    "        ld[i] = valuei\n",
    "    # standardise the discriminant function so that its mean value is 0:\n",
    "    ld = scale(ld, with_std=False)\n",
    "    return ld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `calculateLDA()` simply calculates the value of a discriminant function for each sample in the data set, for example, for the first disriminant function, for each sample we calculate the value using the equation:\n",
    "\n",
    "```\n",
    "-0.403*V2 - 0.165*V3 - 0.369*V4 + 0.155*V5 - 0.002*V6 + 0.618*V7 - 1.661*V8 - 1.496*V9 + 0.134*V10 + 0.355*V11 - 0.818*V12 - 1.158*V13 - 0.003*V14\n",
    "```\n",
    "\n",
    "Furthermore, the `scale()` command is used within the `calclda()` function in order to standardise the value of a discriminant function (eg. the first discriminant function) so that its mean value (over all the wine samples) is 0.\n",
    "\n",
    "We can use the function `calculateLDA()` to calculate the values of the first discriminant function for each sample in our wine data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculateLDA(X, lda.scalings_[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, the values of the first linear discriminant function can be calculated using the `transform(X)` or `fit_transform(X, y)` methods of the LDA object, so we can compare those to the ones that we calculated, and they should agree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try either, they produce the same result, use help() for more info\n",
    "# lda.transform(X)[:, 0]\n",
    "lda.fit_transform(X, y)[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that they do agree.\n",
    "\n",
    "It doesn’t matter whether the input variables for linear discriminant analysis are standardized or not, unlike for principal components analysis in which it is often necessary to standardize the input variables. However, using standardized variables in linear discriminant analysis makes it easier to interpret the loadings in a linear discriminant function.\n",
    "\n",
    "In linear discriminant analysis, the standardized version of an input variable is defined so that it has mean zero and within-groups variance of 1. Thus, we can calculate the “group-standardised” variable by subtracting the mean from each value of the variable, and dividing by the within-groups standard deviation. To calculate the group-standardized version of a set of variables, we can use the function `groupStandardize()` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupStandardize(variables, groupvariable):\n",
    "    # find the number of samples in the data set and the number of variables\n",
    "    numsamples, numvariables = variables.shape\n",
    "    # find the variable names\n",
    "    variablenames = variables.columns\n",
    "    # calculate the group-standardised version of each variable\n",
    "    variables_new = pd.DataFrame()\n",
    "    for i in range(numvariables):\n",
    "        variable_name = variablenames[i]\n",
    "        variablei = variables[variable_name]\n",
    "        variablei_Vw = calcWithinGroupsVariance(variablei, groupvariable)\n",
    "        variablei_mean = np.mean(variablei)\n",
    "        variablei_new = (variablei - variablei_mean)/(np.sqrt(variablei_Vw))\n",
    "        variables_new[variable_name] = variablei_new\n",
    "    return variables_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we can use the `groupStandardize()` function to calculate the group-standardized versions of the chemical concentrations in wine samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupstandardizedX = groupStandardize(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use the `LinearDiscriminantAnalysis().fit()` method to perform linear disriminant analysis on the group-standardised variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda2 = LinearDiscriminantAnalysis().fit(groupstandardizedX, y)\n",
    "pretty_scalings(lda2, groupstandardizedX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It makes sense to interpret the loadings calculated using the group-standardized variables rather than the loadings for the original (unstandardized) variables.\n",
    "\n",
    "In the first discriminant function calculated for the group-standardized variables, the largest loadings (in absolute) value are given to V8 (-0.871), V11 (0.537), V13 (-0.464), V14 (-0.464), and V5 (0.438). The loadings for V8, V13 and V14 are negative, while those for V11 and V5 are positive. Therefore, the discriminant function seems to represent a contrast between the concentrations of V8, V13 and V14, and the concentrations of V11 and V5.\n",
    "\n",
    "We saw above that the individual variables which gave the greatest separations between the groups were V8 (separation 233.93), V14 (207.92), V13 (189.97), V2 (135.08) and V11 (120.66). These were mostly the same variables that had the largest loadings in the linear discriminant function (loading for V8: -0.871, for V14: -0.464, for V13: -0.464, for V11: 0.537).\n",
    "\n",
    "We found above that variables V8 and V11 have a negative between-groups covariance (-60.41) and a positive within-groups covariance (0.29). When the between-groups covariance and within-groups covariance for two variables have opposite signs, it indicates that a better separation between groups can be obtained by using a linear combination of those two variables than by using either variable on its own.\n",
    "\n",
    "Thus, given that the two variables V8 and V11 have between-groups and within-groups covariances of opposite signs, and that these are two of the variables that gave the greatest separations between groups when used individually, it is not surprising that these are the two variables that have the largest loadings in the first discriminant function.\n",
    "\n",
    "Note that although the loadings for the group-standardized variables are easier to interpret than the loadings for the unstandardized variables, the values of the discriminant function are the same regardless of whether we standardize the input variables or not. For example, for wine data, we can calculate the value of the first discriminant function calculated using the unstandardized and group-standardised variables by typing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.fit_transform(X, y)[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda2.fit_transform(groupstandardizedX, y)[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that although the loadings are different for the first discriminant functions calculated using unstandardized and group-standardized data, the actual values of the first discriminant function are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Separation Achieved by the Discriminant Functions\n",
    "\n",
    "To calculate the separation achieved by each discriminant function, we first need to calculate the value of each discriminant function, by substituting the values of the variables into the linear combination for the discriminant function (eg. `-0.403*V2 - 0.165*V3 - 0.369*V4 + 0.155*V5 - 0.002*V6 + 0.618*V7 - 1.661*V8 - 1.496*V9 + 0.134*V10 + 0.355*V11 - 0.818*V12 - 1.158*V13 - 0.003*V14` for the first discriminant function), and then scaling the values of the discriminant function so that their mean is zero.\n",
    "\n",
    "As mentioned above, we can do this using the `rpredict()` function which simulates the output of the `predict()` function in R. For example, to calculate the value of the discriminant functions for the wine data, we type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rpredict(lda, X, y, out=False):\n",
    "    ret = {\"class\": lda.predict(X),\n",
    "           \"posterior\": pd.DataFrame(lda.predict_proba(X), columns=lda.classes_)}\n",
    "    ret[\"x\"] = pd.DataFrame(lda.fit_transform(X, y))\n",
    "    ret[\"x\"].columns = [\"LD\"+str(i+1) for i in range(ret[\"x\"].shape[1])]\n",
    "    if out:\n",
    "        print(\"class\")\n",
    "        print(ret[\"class\"])\n",
    "        print()\n",
    "        print(\"posterior\")\n",
    "        print(ret[\"posterior\"])\n",
    "        print()\n",
    "        print(\"x\")\n",
    "        print(ret[\"x\"])\n",
    "    return ret\n",
    "\n",
    "lda_values = rpredict(lda, standardizedX, y, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned variable has a named element `x` which is a matrix containing the linear discriminant functions: the first column of `x` contains the first discriminant function, the second column of `x` contains the second discriminant function, and so on (if there are more discriminant functions).\n",
    "\n",
    "We can therefore calculate the separations achieved by the two linear discriminant functions for the wine data by using the `calcSeparations()` function (see above), which calculates the separation as the ratio of the between-groups variance to the within-groups variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calcSeparations(lda_values[\"x\"], y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, the loadings for each discriminant function are calculated in such a way that the within-group variance for each group (wine cultivar here) is equal to 1, as we see in the output from `calcSeparations()` above.\n",
    "\n",
    "The output from `calcSeparations()` tells us that the separation achieved by the first (best) discriminant function is 794.7, and the separation achieved by the second (second best) discriminant function is 361.2.\n",
    "\n",
    "Therefore, the total separation is the sum of these, which is (`794.652200566216+361.241041493455=1155.893`) 1155.89, rounded to two decimal places. Therefore, the *percentage separation* achieved by the first discriminant function is (`794.652200566216*100/1155.893=`) 68.75%, and the percentage separation achieved by the second discriminant function is (`361.241041493455*100/1155.893=`) 31.25%.\n",
    "\n",
    "The *proportion of trace* (as reported in R by the `lda()` model) is the percentage separation achieved by each discriminant function. For example, for the wine data we get the same values as just calculated (68.75% and 31.25%). Note that in `sklearn` the proportion of trace is reported as `explained_variance_ratio_` in a `LinearDiscriminantAnalysis` model and is computed only for an \"eigen\" solver, while so far we have been using the default one, which is \"svd\" (Singular Value Decomposition):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proportion_of_trace(lda):\n",
    "    ret = pd.DataFrame([round(i, 4) for i in lda.explained_variance_ratio_ if round(i, 4) > 0], columns=[\"ExplainedVariance\"])\n",
    "    ret.index = [\"LD\"+str(i+1) for i in range(ret.shape[0])]\n",
    "    ret = ret.transpose()\n",
    "    print(\"Proportion of trace:\")\n",
    "    print(ret.to_string(index=False))\n",
    "    return ret\n",
    "\n",
    "proportion_of_trace(LinearDiscriminantAnalysis(solver=\"eigen\").fit(X, y));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the first discriminant function does achieve a good separation between the three groups (three cultivars), but the second discriminant function does improve the separation of the groups by quite a large amount, so is it worth using the second discriminant function as well. Therefore, to achieve a good separation of the groups (cultivars), it is necessary to use both of the first two discriminant functions.\n",
    "\n",
    "We found above that the largest separation achieved for any of the individual variables (individual chemical concentrations) was 233.9 for V8, which is quite a lot less than 794.7, the separation achieved by the first discriminant function. Therefore, the effect of using more than one variable to calculate the discriminant function is that we can find a discriminant function that achieves a far greater separation between groups than achieved by any one variable alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A Stacked Histogram of the LDA Values\n",
    "\n",
    "A nice way of displaying the results of a linear discriminant analysis (LDA) is to make a stacked histogram of the values of the discriminant function for the samples from different groups (different wine cultivars in our example).\n",
    "\n",
    "We can do this using the `ldahist()` function defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ldahist(data, g, sep=False):\n",
    "    xmin = np.trunc(np.min(data)) - 1\n",
    "    xmax = np.trunc(np.max(data)) + 1\n",
    "    ncol = len(set(g))\n",
    "    binwidth = 0.5\n",
    "    bins=np.arange(xmin, xmax + binwidth, binwidth)\n",
    "    if sep:\n",
    "        fig, axl = plt.subplots(ncol, 1, sharey=True, sharex=True)\n",
    "    else:\n",
    "        fig, axl = plt.subplots(1, 1, sharey=True, sharex=True)\n",
    "        axl = [axl]*ncol\n",
    "    for ax, (group, gdata) in zip(axl, data.groupby(g)):\n",
    "        sns.distplot(gdata.values, bins, ax=ax, label=\"group \"+str(group))\n",
    "        ax.set_xlim([xmin, xmax])\n",
    "        if sep:\n",
    "            ax.set_xlabel(\"group\"+str(group))\n",
    "        else:\n",
    "            ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, to make a stacked histogram of the first discriminant function’s values for wine samples of the three different wine cultivars, we type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldahist(lda_values[\"x\"].LD1, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the histogram that cultivars 1 and 3 are well separated by the first discriminant function, since the values for the first cultivar are between -6 and -1, while the values for cultivar 3 are between 2 and 6, and so there is no overlap in values.\n",
    "\n",
    "However, the separation achieved by the linear discriminant function on the training set may be an overestimate. To get a more accurate idea of how well the first discriminant function separates the groups, we would need to see a stacked histogram of the values for the three cultivars using some unseen “test set”, that is, using a set of data that was not used to calculate the linear discriminant function.\n",
    "\n",
    "We see that the first discriminant function separates cultivars 1 and 3 very well, but does not separate cultivars 1 and 2, or cultivars 2 and 3, so well.\n",
    "\n",
    "We therefore investigate whether the second discriminant function separates those cultivars, by making a stacked histogram of the second discriminant function’s values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldahist(lda_values[\"x\"].LD2, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the second discriminant function separates cultivars 1 & 2 ; and cultivars 1 & 3 quite well, although there is a little overlap in their values. However, the second discriminant function fails to separate cultivars 2 and 3 as well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scatterplots of the Discriminant Functions\n",
    "\n",
    "We can obtain a scatterplot of the best two discriminant functions, with the data points labelled by cultivar, by typing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(\"LD1\", \"LD2\", lda_values[\"x\"].join(y), hue=\"V1\", fit_reg=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the scatterplot of the first two discriminant functions, we can see that the wines from the three cultivars are well separated in the scatterplot. The first discriminant function (x-axis) separates cultivars 1 and 3 very well, but doesn’t not perfectly separate cultivars 1 and 3, or cultivars 2 and 3.\n",
    "\n",
    "The second discriminant function (y-axis) achieves a fairly good separation of cultivars 1 and 3, and cultivars 2 and 3, although it is not totally perfect.\n",
    "\n",
    "To achieve a very good separation of the three cultivars, it would be best to use both the first and second discriminant functions together, since the first discriminant function can separate cultivars 1 and 3 very well, and the second discriminant function can separate cultivars 1 and 2, and cultivars 2 and 3, reasonably well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Allocation Rules and Misclassification Rate\n",
    "\n",
    "We can calculate the mean values of the discriminant functions for each of the three cultivars using the `printMeanAndSdByGroup()` function (see above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printMeanAndSdByGroup(lda_values[\"x\"], y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that the mean value of the first discriminant function is :\n",
    "    * -3.42248851 for cultivar 1, \n",
    "    * -0.07972623 for cultivar 2, and \n",
    "    * 4.32473717 for cultivar 3. \n",
    "    \n",
    "    The mid-way point between the mean values for cultivars 1 and 2 is (-3.42248851-0.07972623)/2 = -1.751107, and \n",
    "    the mid-way point between the mean values for cultivars 2 and 3 is (-0.07972623+4.32473717)/2 = 2.122505.\n",
    "\n",
    "Therefore, we can use the following allocation rule:\n",
    "* if the first discriminant function is <= -1.751107, predict the sample to be from cultivar 1\n",
    "* if the first discriminant function is > -1.751107 and <= 2.122505, predict the sample to be from cultivar 2\n",
    "* if the first discriminant function is > 2.122505, predict the sample to be from cultivar 3\n",
    "\n",
    "We can examine the accuracy of this allocation rule by using the `calcAllocationRuleAccuracy()` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcAllocationRuleAccuracy(ldavalue, groupvariable, cutoffpoints):\n",
    "    # find out how many values the group variable can take\n",
    "    levels = sorted(set((groupvariable)))\n",
    "    numlevels = len(levels)\n",
    "    confusion_matrix = []\n",
    "    # calculate the number of true positives and false negatives for each group\n",
    "    for i, leveli in enumerate(levels):\n",
    "        levelidata = ldavalue[groupvariable==leveli]\n",
    "        row = []\n",
    "        # see how many of the samples from this group are classified in each group\n",
    "        for j, levelj in enumerate(levels):\n",
    "            if j == 0:\n",
    "                cutoff1 = cutoffpoints[0]\n",
    "                cutoff2 = \"NA\"\n",
    "                results = (levelidata <= cutoff1).value_counts()\n",
    "            elif j == numlevels-1:\n",
    "                cutoff1 = cutoffpoints[numlevels-2]\n",
    "                cutoff2 = \"NA\"\n",
    "                results = (levelidata > cutoff1).value_counts()\n",
    "            else:\n",
    "                cutoff1 = cutoffpoints[j-1]\n",
    "                cutoff2 = cutoffpoints[j]\n",
    "                results = ((levelidata > cutoff1) & (levelidata <= cutoff2)).value_counts()\n",
    "            try:\n",
    "                trues = results[True]\n",
    "            except KeyError:\n",
    "                trues = 0\n",
    "            print(\"Number of samples of group\", leveli, \"classified as group\", levelj, \":\", trues, \"(cutoffs:\", cutoff1, \",\", cutoff2, \")\")\n",
    "            row.append(trues)\n",
    "        confusion_matrix.append(row)\n",
    "    return confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, to calculate the accuracy for the wine data based on the allocation rule for the first discriminant function, we type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = calcAllocationRuleAccuracy(lda_values[\"x\"].iloc[:, 0], y, [-1.751107, 2.122505])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be displayed in a *confusion matrix*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateConfusionMatrix(confusion_matrix, classes_names):\n",
    "    display(pd.DataFrame(confusion_matrix, index=[\"Is group \"+i for i in classes_names], columns=[\"Allocated to group \"+i for i in classes_names]))\n",
    "\n",
    "calculateConfusionMatrix(confusion_matrix, lda.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3+5+1=9 wine samples that are misclassified, out of (56+3+5+65+1+48=) 178 wine samples: 3 samples from cultivar 1 are predicted to be from cultivar 2, 5 samples from cultivar 2 are predicted to be from cultivar 1, and 1 sample from cultivar 2 is predicted to be from cultivar 3. Therefore, the misclassification rate is 9/178, or 5.1%. The misclassification rate is quite low, and therefore the accuracy of the allocation rule appears to be relatively high.\n",
    "\n",
    "However, this is probably an underestimate of the misclassification rate, as the allocation rule was based on this data (this is the *training set*). If we calculated the misclassification rate for a separate *test set* consisting of data other than that used to make the allocation rule, we would probably get a higher estimate of the misclassification rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Python way\n",
    "\n",
    "Python allows to do all the above in a much faster way and providing extended automatic report capabilities by using the `sklearn.metrics` module. The above confusion matrix and reporting typical performance metrics, such as *precision*, *recall*, *f1-score* can be done in python as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "def lda_classify(v, levels, cutoffpoints):\n",
    "    for level, cutoff in zip(reversed(levels), reversed(cutoffpoints)):\n",
    "        if v > cutoff: return level\n",
    "    return levels[0]\n",
    "    \n",
    "y_pred = lda_values[\"x\"].iloc[:, 0].apply(lda_classify, args=(lda.classes_, [-1.751107, 2.122505],)).values\n",
    "y_true = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#example-model-selection-plot-confusion-matrix-py\n",
    "def plotConfusionMatrix(cm, target_names, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(target_names))\n",
    "    plt.xticks(tick_marks, target_names, rotation=45)\n",
    "    plt.yticks(tick_marks, target_names)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "print(metrics.classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(y_true, y_pred)\n",
    "calculateConfusionMatrix(cm, lda.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizedConfusionMatrix = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "plotConfusionMatrix(normalizedConfusionMatrix, lda.classes_, title='Normalized confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links and Further Reading\n",
    "\n",
    "Here are some info and links for further reading.\n",
    "\n",
    "To learn about multivariate analysis I would recommend the following:\n",
    "* [Multivariate Data Analysis](http://www.bookbutler.co.uk/compare?isbn=9781292021904) by Hair et. al.\n",
    "* [Applied Multivariate Data Analysis](http://www.bookbutler.co.uk/compare?isbn=9780340741221) by Everitt and Dunn.\n",
    "\n",
    "\n",
    "To learn about data analysis and data science using the Python ecosystem I would recommend the following:\n",
    "* [Python for Data Analysis](http://shop.oreilly.com/product/0636920023784.do) by Wes McKinney\n",
    "* [Data Science from Scratch](http://shop.oreilly.com/product/0636920033400.do) by Joel Grus\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the benefits of hierarchical clustering is that you don't need to already know the number of clusters k in your data in advance. Sadly, there doesn't seem to be much documentation on how to actually use scipy's hierarchical clustering to make an informed decision and then retrieve the clusters.\n",
    "\n",
    "We'll be learning:\n",
    "    - how to use scipy's hierarchical clustering\n",
    "    - how to plot a nice dendrogram from it\n",
    "    - how to use the dendrogram to select a distance cut-off (aka determining the number of clusters k in your data)\n",
    "    - how to retrieve the k clusters\n",
    "    - how to visualize the clusters (2D case)\n",
    "\n",
    "Naming conventions:\n",
    "Before we start, as i know that it's easy to get lost, some naming conventions:\n",
    "\n",
    "X samples (n x m array), aka data points or \"singleton clusters\"\n",
    "n number of samples\n",
    "m number of features\n",
    "Z cluster linkage array (contains the hierarchical clustering information)\n",
    "k number of clusters\n",
    "\n",
    "So, let's go..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for plots and clean output\n",
    "%matplotlib inline\n",
    "np.set_printoptions(precision=5, suppress=True)  # suppress scientific float notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 2)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# generate two clusters: a with 100 points, b with 50:\n",
    "np.random.seed(4711)  # for repeatability of this tutorial\n",
    "a = np.random.multivariate_normal([10, 0], [[3, 1], [1, 4]], size=[100,])\n",
    "b = np.random.multivariate_normal([0, 20], [[3, 1], [1, 4]], size=[50,])\n",
    "X = np.concatenate((a, b),)\n",
    "print(X.shape)  # 150 samples with 2 dimensions\n",
    "plt.scatter(X[:,0], X[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform the Hierarchical Clustering\n",
    "Now that we have some very simple sample data, let's do the actual clustering on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the linkage matrix\n",
    "Z = linkage(X, 'ward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done. That was pretty simple, wasn't it?\n",
    "\n",
    "Well, sure it was, this is python ;), but what does 'ward' mean there and how does this actually work?\n",
    "\n",
    "As it's explained in the scipy documents, 'ward' is one of the methods that can be used to calculate the distance between newly formed clusters. 'ward' causes linkage() to use the Ward variance minimization algorithm.\n",
    "\n",
    "Although it's a good default choice, but it never hurts to play around with some other common linkage methods like 'single', 'complete', 'average', ... and the different distance metrics like 'euclidean' (default), 'cityblock' aka Manhattan, 'hamming', 'cosine'... if you have the feeling that your data should not just be clustered to minimize the overall intra cluster variance in euclidean space.\n",
    "\n",
    "As you can see there's a lot of choice here and while python and scipy make it very easy to do the clustering, it's you who has to understand and make these choices. \n",
    "\n",
    "Another important concept is check the Cophenetic Correlation Coefficient of your clustering with help of the cophenet() function. This compares the actual pairwise distances of all your samples to those implied by the hierarchical clustering. The closer the value is to 1, the better the clustering preserves the original distances, which in our case is pretty close:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9800148387574268"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.cluster.hierarchy import cophenet\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "c, coph_dists = cophenet(Z, pdist(X))\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No matter what method and metric you pick, the linkage() function will use that method and metric to calculate the distances of the clusters (starting with your n individual samples as singleton clusters) and in each iteration will merge the two clusters which have the smallest distance according the selected method and metric. It will return an array of length n - 1 giving you information about the n - 1 cluster merges which it needs to pairwise merge n clusters. Z[i] will tell us which clusters were merged in the i-th iteration, let's take a look at the first two points that were merged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([52.     , 53.     ,  0.04151,  2.     ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that each row of the resulting array has the format [idx1, idx2, dist, sample_count].\n",
    "\n",
    "In its first iteration the linkage algorithm decided to merge the two clusters (original samples here) with indices 52 and 53, as they only had a distance of 0.04151. This created a cluster with a total of 2 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second iteration the algorithm decided to merge the clusters (original samples here as well) with indices 14 and 79, which had a distance of 0.04914. This again formed another cluster with a total of 2 samples.\n",
    "\n",
    "The indices of the clusters until now correspond to our samples. Remember that we had a total of 150 samples, so indices 0 to 149. Let's have a look at the first 20 iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that until iteration 13 the algorithm only directly merged original samples. We can also observe the monotonic increase of the distance.\n",
    "\n",
    "In iteration 13 the algorithm decided to merge cluster indices 62 with 152. If you paid attention the 152 should astonish you as we only have original sample indices 0 to 149 for our 150 samples. All indices idx >= len(X) actually refer to the cluster formed in Z[idx - len(X)].\n",
    "\n",
    "This means that while idx 149 corresponds to X[149] that idx 150 corresponds to the cluster formed in Z[0], idx 151 to Z[1], 152 to Z[2], ...\n",
    "\n",
    "Hence, the merge iteration 13 merged sample 62 to our samples 33 and 68 that were previously merged in iteration 2 (152 - 2).\n",
    "\n",
    "Let's check out the points coordinates to see if this makes sense:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[[33, 68, 62]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems pretty close, but let's plot the points again and highlight them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = [33, 68, 62]\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(X[:,0], X[:,1])  # plot all points\n",
    "plt.scatter(X[idxs,0], X[idxs,1], c='r')  # plot interesting points in red again\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the 3 red dots are pretty close to each other, which is a good thing.\n",
    "\n",
    "The same happened in iteration 14 where the alrogithm merged indices 41 to 15 and 69:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = [33, 68, 62]\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(X[:,0], X[:,1])\n",
    "plt.scatter(X[idxs,0], X[idxs,1], c='r')\n",
    "idxs = [15, 69, 41]\n",
    "plt.scatter(X[idxs,0], X[idxs,1], c='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing that the 3 yellow dots are also quite close.\n",
    "\n",
    "And so on...\n",
    "\n",
    "We'll later come back to visualizing this, but now let's have a look at what's called a dendrogram of this hierarchical clustering first:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting a Dendrogram\n",
    "A dendrogram is a visualization in form of a tree showing the order and distances of merges during the hierarchical clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate full dendrogram\n",
    "plt.figure(figsize=(25, 10))\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('sample index')\n",
    "plt.ylabel('distance')\n",
    "dendrogram(\n",
    "    Z,\n",
    "    leaf_rotation=90.,  # rotates the x axis labels\n",
    "    leaf_font_size=8.,  # font size for the x axis labels\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(right click and \"View Image\" to see full resolution)\n",
    "\n",
    "If this is the first time you see a dendrogram, it's probably quite confusing, so let's take this apart...\n",
    "\n",
    "On the x axis you see labels. If you don't specify anything else they are the indices of your samples in X.\n",
    "On the y axis you see the distances (of the 'ward' method in our case).\n",
    "Starting from each label at the bottom, you can see a vertical line up to a horizontal line. The height of that horizontal line tells you about the distance at which this label was merged into another label or cluster. You can find that other cluster by following the other vertical line down again. If you don't encounter another horizontal line, it was just merged with the other label you reach, otherwise it was merged into another cluster that was formed earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summarizing:\n",
    "\n",
    "horizontal lines are cluster merges\n",
    "vertical lines tell you which clusters/labels were part of merge forming that new cluster\n",
    "heights of the horizontal lines tell you about the distance that needed to be \"bridged\" to form the new cluster\n",
    "You can also see that from distances > 25 up there's a huge jump of the distance to the final merge at a distance of approx. 180. Let's have a look at the distances of the last 4 merges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z[-4:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such distance jumps / gaps in the dendrogram are pretty interesting for us. They indicate that something is merged here, that maybe just shouldn't be merged. In other words: maybe the things that were merged here really don't belong to the same cluster, telling us that maybe there's just 2 clusters here.\n",
    "\n",
    "Looking at indices in the above dendrogram also shows us that the green cluster only has indices >= 100, while the red one only has such < 100. This is a good thing as it shows that the algorithm re-discovered the two classes in our toy example.\n",
    "\n",
    "In case you're wondering about where the colors come from, you might want to have a look at the color_threshold argument of dendrogram(), which as not specified automagically picked a distance cut-off value of 70 % of the final merge and then colored the first clusters below that in individual colors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dendrogram Truncation\n",
    "As you might have noticed, the above is pretty big for 150 samples already and you probably have way more in real scenarios, so let's spend a few seconds on highlighting some other features of the dendrogram() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Hierarchical Clustering Dendrogram (truncated)')\n",
    "plt.xlabel('sample index')\n",
    "plt.ylabel('distance')\n",
    "dendrogram(\n",
    "    Z,\n",
    "    truncate_mode='lastp',  # show only the last p merged clusters\n",
    "    p=12,  # show only the last p merged clusters\n",
    "    show_leaf_counts=False,  # otherwise numbers in brackets are counts\n",
    "    leaf_rotation=90.,\n",
    "    leaf_font_size=12.,\n",
    "    show_contracted=True,  # to get a distribution impression in truncated branches\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above shows a truncated dendrogram, which only shows the last p=12 out of our 149 merges.\n",
    "\n",
    "First thing you should notice are that most labels are missing. This is because except for X[40] all other samples were already merged into clusters before the last 12 merges.\n",
    "\n",
    "The parameter show_contracted allows us to draw black dots at the heights of those previous cluster merges, so we can still spot gaps even if we don't want to clutter the whole visualization. In our example we can see that the dots are all at pretty small distances when compared to the huge last merge at a distance of 180, telling us that we probably didn't miss much there.\n",
    "\n",
    "As it's kind of hard to keep track of the cluster sizes just by the dots, dendrogram() will by default also print the cluster sizes in brackets () if a cluster was truncated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Hierarchical Clustering Dendrogram (truncated)')\n",
    "plt.xlabel('sample index or (cluster size)')\n",
    "plt.ylabel('distance')\n",
    "dendrogram(\n",
    "    Z,\n",
    "    truncate_mode='lastp',  # show only the last p merged clusters\n",
    "    p=12,  # show only the last p merged clusters\n",
    "    leaf_rotation=90.,\n",
    "    leaf_font_size=12.,\n",
    "    show_contracted=True,  # to get a distribution impression in truncated branches\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see that the right most cluster already consisted of 33 samples before the last 12 merges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eye Candy (prettier dendogram)\n",
    "Even though this already makes for quite a nice visualization, we can add more information like annotating the distances inside the dendrogram by using some of the useful return values dendrogram():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fancyDendrogram(*args, **kwargs):\n",
    "    max_d = kwargs.pop('max_d', None)\n",
    "    if max_d and 'color_threshold' not in kwargs:\n",
    "        kwargs['color_threshold'] = max_d\n",
    "    annotate_above = kwargs.pop('annotate_above', 0)\n",
    "\n",
    "    ddata = dendrogram(*args, **kwargs)\n",
    "\n",
    "    if not kwargs.get('no_plot', False):\n",
    "        plt.title('Hierarchical Clustering Dendrogram (truncated)')\n",
    "        plt.xlabel('sample index or (cluster size)')\n",
    "        plt.ylabel('distance')\n",
    "        for i, d, c in zip(ddata['icoord'], ddata['dcoord'], ddata['color_list']):\n",
    "            x = 0.5 * sum(i[1:3])\n",
    "            y = d[1]\n",
    "            if y > annotate_above:\n",
    "                plt.plot(x, y, 'o', c=c)\n",
    "                plt.annotate(\"%.3g\" % y, (x, y), xytext=(0, -5),\n",
    "                             textcoords='offset points',\n",
    "                             va='top', ha='center')\n",
    "        if max_d:\n",
    "            plt.axhline(y=max_d, c='k')\n",
    "    return ddata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fancyDendrogram(\n",
    "    Z,\n",
    "    truncate_mode='lastp',\n",
    "    p=12,\n",
    "    leaf_rotation=90.,\n",
    "    leaf_font_size=12.,\n",
    "    show_contracted=True,\n",
    "    annotate_above=10,  # useful in small plots so annotations don't overlap\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting a Distance Cut-Off; Determining the Number of Clusters\n",
    "As explained above already, a huge jump in distance is typically what we're interested in if we want to argue for a certain number of clusters. If you have the chance to do this manually, I'd always opt for that, as it allows you to gain some insights into your data and to perform some sanity checks on the edge cases. In our case it is probably safe to say that our cut-off is 50, as the jump is pretty obvious:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set cut-off to 50\n",
    "maxDistance = 50  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize this in the dendrogram as a cut-off line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fancyDendrogram(\n",
    "    Z,\n",
    "    truncate_mode='lastp',\n",
    "    p=12,\n",
    "    leaf_rotation=90.,\n",
    "    leaf_font_size=12.,\n",
    "    show_contracted=True,\n",
    "    annotate_above=10,\n",
    "    max_d=maxDistance,  # plot a horizontal cut-off line\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we (\"surprisingly\") have two clusters at this cut-off.\n",
    "\n",
    "In general for a chosen cut-off value max_d you can always simply count the number of intersections with vertical lines of the dendrogram to get the number of formed clusters. Say we choose a cut-off of maxDistance = 16, we'd get 4 final clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fancyDendrogram(\n",
    "    Z,\n",
    "    truncate_mode='lastp',\n",
    "    p=12,\n",
    "    leaf_rotation=90.,\n",
    "    leaf_font_size=12.,\n",
    "    show_contracted=True,\n",
    "    annotate_above=10,\n",
    "    max_d=16,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Automated Cut-Off Selection (or why you shouldn't rely on this)\n",
    "Now while this manual selection of a cut-off value offers a lot of benefits when it comes to checking for a meaningful clustering and cut-off, there are cases in which you want to automate this.\n",
    "\n",
    "The problem again is that there is no golden method to pick the number of clusters for all cases (which is why you should understand your data and use investigative & backtesting manual methods, if possible). Wikipedia lists a couple of common methods. Reading this, you should realize how different the approaches and how vague their descriptions are.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Inconsistency Method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the default for the fcluster() function in scipy is the  \"inconsistency\" method.\n",
    "\n",
    "The concept behind the inconsistency method is understanding \"what makes a distance jump a jump?\". It answers this by comparing each cluster merge's height h to the average avg and normalizing it by the standard deviation std formed over the depth previous levels:\n",
    "\n",
    "inconsistency= (h−avg) / std\n",
    "\n",
    "The following shows a matrix of the avg, std, count, inconsistency for each of the last 10 merges of our hierarchical clustering with depth = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import inconsistent\n",
    "\n",
    "depth = 5\n",
    "incons = inconsistent(Z, depth)\n",
    "incons[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you might be tempted to say \"yay, let's just pick 5\" as a limit in the inconsistencies, but look at what happens if we set depth to 3 instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = 3\n",
    "incons = inconsistent(Z, depth)\n",
    "incons[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ooops! This should make you realize that the inconsistency values heavily depend on the depth of the tree you calculate the averages over.\n",
    "\n",
    "Another problem in its calculation is that the previous d levels' heights aren't normally distributed, but expected to increase, so you can't really just treat the current level as an \"outlier\" of a normal distribution, as it's expected to be bigger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Elbow Method\n",
    "This method tries to find the clustering step where the acceleration of distance growth is the biggest (the \"strongest elbow\" of the blue line graph below, which is the highest value of the green graph below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last = Z[-10:, 2]\n",
    "last_rev = last[::-1]\n",
    "idxs = np.arange(1, len(last) + 1)\n",
    "plt.plot(idxs, last_rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acceleration = np.diff(last, 2)  # 2nd derivative of the distances\n",
    "acceleration_rev = acceleration[::-1]\n",
    "plt.plot(idxs[:-2] + 1, acceleration_rev)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = acceleration_rev.argmax() + 2  # if idx 0 is the max of this we want 2 clusters\n",
    "print(\"clusters:\", k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this works nicely in our simplistic example (the green line takes its maximum for k=2), it's pretty flawed as well.\n",
    "\n",
    "One issue of this method has to do with the way an \"elbow\" is defined: you need at least a right and a left point, which implies that this method will never be able to tell you that all your data is in one single cluster only.\n",
    "\n",
    "Another problem with this variant lies in the np.diff(Z[:, 2], 2) though. The order of the distances in Z[:, 2] isn't properly reflecting the order of merges within one branch of the tree. In other words: there is no guarantee that the distance of Z[i] is contained in the branch of Z[i+1]. By simply computing the np.diff(Z[:, 2], 2) we assume that this doesn't matter and just compare distance jumps from different branches of our merge tree.\n",
    "\n",
    "If you still don't want to believe this, let's just construct another simplistic example but this time with very different variances in the different clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.random.multivariate_normal([40, 40], [[20, 1], [1, 30]], size=[200,])\n",
    "d = np.random.multivariate_normal([80, 80], [[30, 1], [1, 30]], size=[200,])\n",
    "e = np.random.multivariate_normal([0, 100], [[100, 1], [1, 100]], size=[200,])\n",
    "X2 = np.concatenate((X, c, d, e),)\n",
    "plt.scatter(X2[:,0], X2[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we have 5 clusters now, but they have increasing variances... let's have a look at the dendrogram again and how you can use it to spot the problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z2 = linkage(X2, 'ward')\n",
    "plt.figure(figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fancyDendrogram(\n",
    "    Z2,\n",
    "    truncate_mode='lastp',\n",
    "    p=30,\n",
    "    leaf_rotation=90.,\n",
    "    leaf_font_size=12.,\n",
    "    show_contracted=True,\n",
    "    annotate_above=40,\n",
    "    max_d=170,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at a dendrogram like this and trying to put a cut-off line somewhere, you should notice the very different distributions of merge distances below that cut-off line. Compare the distribution in the cyan cluster to the red, green or even two blue clusters that have even been truncated away. In the cyan cluster below the cut-off we don't really have any discontinuity of merge distances up to very close to the cut-off line. The two blue clusters on the other hand are each merged below a distance of 25, and have a gap of > 155 to our cut-off line.\n",
    "\n",
    "The variant of the \"elbow\" method will incorrectly see the jump from 167 to 180 as minimal and tell us we have 4 clusters:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last = Z2[-10:, 2]\n",
    "last_rev = last[::-1]\n",
    "idxs = np.arange(1, len(last) + 1)\n",
    "plt.plot(idxs, last_rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acceleration = np.diff(last, 2)  # 2nd derivative of the distances\n",
    "acceleration_rev = acceleration[::-1]\n",
    "plt.plot(idxs[:-2] + 1, acceleration_rev)\n",
    "plt.show()\n",
    "k = acceleration_rev.argmax() + 2  # if idx 0 is the max of this we want 2 clusters\n",
    "print(\"clusters:\", k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same happens with the inconsistency metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inconsistent(Z2, 5)[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hope you can now understand why i'm warning against blindly using any of those methods on a dataset you know nothing about. They can give you some indication, but you should always go back in and check if the results make sense, for example with a dendrogram which is a great tool for that (especially if you have higher dimensional data that you can't simply visualize anymore)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve the Clusters\n",
    "Now, let's finally have a look at how to retrieve the clusters, for different ways of determining k. We can use the fcluster function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Knowing max_d:\n",
    "Let's say we determined the max distance with help of a dendrogram, then we can do the following to get the cluster id for each of our samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import fcluster\n",
    "max_d = 50\n",
    "clusters = fcluster(Z, max_d, criterion='distance')\n",
    "clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Knowing k:\n",
    "Another way starting from the dendrogram is to say \"i can see i have k=2\" clusters. You can then use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=2\n",
    "fcluster(Z, k, criterion='maxclust')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using the Inconsistency Method (default):\n",
    "If you're really sure you want to use the inconsistency method to determine the number of clusters in your dataset, you can use the default criterion of fcluster() and hope you picked the correct values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import fcluster\n",
    "fcluster(Z, 8, depth=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Your Clusters\n",
    "If you're lucky enough and your data is very low dimensional, you can actually visualize the resulting clusters very easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(X[:,0], X[:,1], c=clusters, cmap='prism')  # plot points with cluster dependent colors\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further Reading:\n",
    "The scipy hierarchical clustering module: http://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html\n",
    "The scipy distance computation docs: http://docs.scipy.org/doc/scipy/reference/spatial.distance.html\n",
    "The scipy hierarchical clustering module docs often refer to the MATLAB docs saying that a certain function is similar to the MATLAB one. Here's their hierarchical clustering tutorial: http://mathworks.com/help/stats/hierarchical-clustering.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering using Scikit (k-means)\n",
    "\n",
    "#### Automotive Dataset\n",
    "\n",
    "We will be using the auto_mpg data. Dataset contains information about the MPG, acceleration, weight for each car.  However, we don't have logical groupings for these cars.  We can construct these manually using our domain knowledge (e.g. we could put all of the high mpg cars together and all of the low mpg cars together), but we want a more automatic way of grouping these vehicles that can take into account more features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from sklearn.cluster import KMeans # K means model\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from bokeh.sampledata.autompg import autompg as df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>model_year</th>\n",
       "      <th>origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130</td>\n",
       "      <td>3504</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165</td>\n",
       "      <td>3693</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150</td>\n",
       "      <td>3436</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150</td>\n",
       "      <td>3433</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140</td>\n",
       "      <td>3449</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mpg  cylinders  displacement  horsepower  weight  acceleration  \\\n",
       "0  18.0          8         307.0         130    3504          12.0   \n",
       "1  15.0          8         350.0         165    3693          11.5   \n",
       "2  18.0          8         318.0         150    3436          11.0   \n",
       "3  16.0          8         304.0         150    3433          12.0   \n",
       "4  17.0          8         302.0         140    3449          10.5   \n",
       "\n",
       "   model_year  origin  \n",
       "0          70       1  \n",
       "1          70       1  \n",
       "2          70       1  \n",
       "3          70       1  \n",
       "4          70       1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in data\n",
    "data = pd.read_table('auto_mpg.txt', sep='|') # All values range from 0 to 1\n",
    "data.drop('car_name', axis=1, inplace=True) # Drop labels from dataframe\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Let's see how we can implement a k-means clustering algorithm?\n",
    "scikit-learn KMeans documentation for reference:\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rajku\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\rajku\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\base.py:462: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "# Standardize our data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility \n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run KMeans\n",
    "est = KMeans(n_clusters=2, init='random') # Instatiate estimator\n",
    "est.fit(data_scaled) # Fit your data\n",
    "y_kmeans = est.predict(data_scaled) # Make cluster \"predictions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>model_year</th>\n",
       "      <th>origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28.720536</td>\n",
       "      <td>4.15625</td>\n",
       "      <td>115.162946</td>\n",
       "      <td>80.566964</td>\n",
       "      <td>2367.433036</td>\n",
       "      <td>16.443750</td>\n",
       "      <td>77.187500</td>\n",
       "      <td>1.995536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16.413095</td>\n",
       "      <td>7.22619</td>\n",
       "      <td>300.077381</td>\n",
       "      <td>136.339286</td>\n",
       "      <td>3791.119048</td>\n",
       "      <td>14.338095</td>\n",
       "      <td>74.369048</td>\n",
       "      <td>1.017857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         mpg  cylinders  displacement  horsepower       weight  acceleration  \\\n",
       "0  28.720536    4.15625    115.162946   80.566964  2367.433036     16.443750   \n",
       "1  16.413095    7.22619    300.077381  136.339286  3791.119048     14.338095   \n",
       "\n",
       "   model_year    origin  \n",
       "0   77.187500  1.995536  \n",
       "1   74.369048  1.017857  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the data by looking at the means for each cluster\n",
    "data.groupby(y_kmeans).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mpg               23.445918\n",
       "cylinders          5.471939\n",
       "displacement     194.411990\n",
       "horsepower       104.469388\n",
       "weight          2977.584184\n",
       "acceleration      15.541327\n",
       "model_year        75.979592\n",
       "origin             1.576531\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This can be compared to the overall means for each variable\n",
    "data.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.67666, -0.7723 , -0.75829, -0.62178, -0.71925,  0.32752,\n",
       "         0.32832,  0.52083],\n",
       "       [-0.90222,  1.02973,  1.01105,  0.82904,  0.959  , -0.43669,\n",
       "        -0.43776, -0.69444]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can get the coordinates for the center of each cluster\n",
    "centers = est.cluster_centers_\n",
    "centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### VISUALIZING THE CLUSTERS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can create a nice plot to visualize this upon two of the dimensions\n",
    "colors = np.array(['red', 'green', 'blue', 'yellow', 'orange'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(data_scaled[:, 0], data_scaled[:, 5], c=colors[y_kmeans], s=50)\n",
    "plt.xlabel('MPG')\n",
    "plt.ylabel('Acceleration')\n",
    "plt.scatter(centers[:, 0], centers[:, 5], linewidths=3, marker='+', s=300, c='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another important tool to use when doing cluster analysis is \n",
    "# to see which parameters are correlated with each other.\n",
    "# For this purpose we use the scatter plot matrix to see all of\n",
    "# the different dimensions paired\n",
    "\n",
    "pd.plotting.scatter_matrix(data, c=colors[y_kmeans], figsize=(15,15), s = 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DETERMINING THE NUMBER OF CLUSTERS\n",
    "\n",
    "How do you choose k - the righ number of clusters? There is not  set in stone rule, but we can evaluate \n",
    "performance metrics such as the silhouette (clustering) coefficient across values of k.\n",
    "Note:  You also have to take into account practical limitations of choosing k also.  Ten clusters may give the best value, but it might not make sense in the\n",
    "context of your data.\n",
    "scikit-learn Clustering metrics documentation:\n",
    "http://scikit-learn.org/stable/modules/classes.html#clustering-metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bunch of different models\n",
    "k_rng = range(2,15)\n",
    "k_est = [KMeans(n_clusters = k).fit(data) for k in k_rng]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette Coefficient\n",
    "# Generally want SC to be closer to 1, while also minimizing k\n",
    "from sklearn import metrics\n",
    "silhouette_score = [metrics.silhouette_score(data, e.labels_, metric='euclidean') for e in k_est]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "plt.figure()\n",
    "plt.title('Silhouette coefficient for various values of k')\n",
    "plt.plot(k_rng, silhouette_score, 'b*-')\n",
    "plt.xlim([1,15])\n",
    "plt.grid(True)\n",
    "plt.ylabel('Silhouette Coefficient')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run KMeans\n",
    "est = KMeans(n_clusters=6, init='random') # Instatiate estimator\n",
    "est.fit(data_scaled) # Fit your data\n",
    "y_kmeans = est.predict(data_scaled) # Make cluster \"predictions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can get the coordiantes for the center of each cluster\n",
    "centers = est.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(data_scaled[:, 0], data_scaled[:, 5])#, c=colors[y_kmeans], s=50)\n",
    "plt.xlabel('MPG')\n",
    "plt.ylabel('Acceleration')\n",
    "plt.scatter(centers[:, 0], centers[:, 5], linewidths=8, marker='+', s=300, c='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Another example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.cluster.hierarchy as hac\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use('ggplot')\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "data = np.array([[0.1,   2.5],\n",
    "              [1.5,   .4 ],\n",
    "              [0.3,   1  ],\n",
    "              [1  ,   .8 ],\n",
    "              [0.5,   0  ],\n",
    "              [0  ,   0.5],\n",
    "              [0.5,   0.5],\n",
    "              [2.7,   2  ],\n",
    "              [2.2,   3.1],\n",
    "              [3  ,   2  ],\n",
    "              [3.2,   1.3]])\n",
    "plt.scatter(data[:,0],data[:,1], s = 150, linewidths=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import KMeans # K means model\n",
    "classifier = KMeans(n_clusters=6)\n",
    "classifier.fit(data)\n",
    "\n",
    "centroids = classifier.cluster_centers_\n",
    "labels = classifier.labels_\n",
    "\n",
    "# colorsList = [\"green\",'red',\"cyan\", \"black\", \"orange\"]\n",
    "\n",
    "# for i in range(len(data)):\n",
    "#     plt.plot(data[i][0], data[i][1], colorsList[labels[i]],markersize=18)\n",
    "plt.scatter(data[:,0],data[:,1], marker='o')    \n",
    "plt.scatter(centroids[:,0],centroids[:,1], marker = '+', s = 150, linewidths=15)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "                                            \n",
    "                                             \n",
    "                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###### Titanic Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use('ggplot')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Passenger Class</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Siblings and Spouses</th>\n",
       "      <th>Parents and Children</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Port</th>\n",
       "      <th>Lifeboat</th>\n",
       "      <th>Body</th>\n",
       "      <th>Home_Destination</th>\n",
       "      <th>Midpoint age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Allen, Miss. Elisabeth Walton</td>\n",
       "      <td>female</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24160</td>\n",
       "      <td>211.3375</td>\n",
       "      <td>B5</td>\n",
       "      <td>S</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>St Louis, MO</td>\n",
       "      <td>27.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Allison, Master. Hudson Trevor</td>\n",
       "      <td>male</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>113781</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>S</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Montreal, PQ / Chesterville, ON</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>Allison, Miss. Helen Loraine</td>\n",
       "      <td>female</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>113781</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Montreal, PQ / Chesterville, ON</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>Allison, Mr. Hudson Joshua Creighton</td>\n",
       "      <td>male</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>113781</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>135.0</td>\n",
       "      <td>Montreal, PQ / Chesterville, ON</td>\n",
       "      <td>32.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>Allison, Mrs. Hudson J C (Bessie Waldo Daniels)</td>\n",
       "      <td>female</td>\n",
       "      <td>25.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>113781</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Montreal, PQ / Chesterville, ON</td>\n",
       "      <td>27.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Passenger Class Survived                                             Name  \\\n",
       "0                1      Yes                    Allen, Miss. Elisabeth Walton   \n",
       "1                1      Yes                   Allison, Master. Hudson Trevor   \n",
       "2                1       No                     Allison, Miss. Helen Loraine   \n",
       "3                1       No             Allison, Mr. Hudson Joshua Creighton   \n",
       "4                1       No  Allison, Mrs. Hudson J C (Bessie Waldo Daniels)   \n",
       "\n",
       "      Sex      Age  Siblings and Spouses  Parents and Children  Ticket  \\\n",
       "0  female  29.0000                     0                     0   24160   \n",
       "1    male   0.9167                     1                     2  113781   \n",
       "2  female   2.0000                     1                     2  113781   \n",
       "3    male  30.0000                     1                     2  113781   \n",
       "4  female  25.0000                     1                     2  113781   \n",
       "\n",
       "       Fare    Cabin Port Lifeboat   Body                 Home_Destination  \\\n",
       "0  211.3375       B5    S        2    NaN                     St Louis, MO   \n",
       "1  151.5500  C22 C26    S       11    NaN  Montreal, PQ / Chesterville, ON   \n",
       "2  151.5500  C22 C26    S      NaN    NaN  Montreal, PQ / Chesterville, ON   \n",
       "3  151.5500  C22 C26    S      NaN  135.0  Montreal, PQ / Chesterville, ON   \n",
       "4  151.5500  C22 C26    S      NaN    NaN  Montreal, PQ / Chesterville, ON   \n",
       "\n",
       "   Midpoint age  \n",
       "0          27.5  \n",
       "1           2.5  \n",
       "2           2.5  \n",
       "3          32.5  \n",
       "4          27.5  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('TitanicPassengers.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What to do with Categorical Data, missing data?\n",
    "\n",
    "df.drop(['Body','Name'],1,inplace=True)\n",
    "df.convert_objects(convert_numeric=True)\n",
    "df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1309 entries, 0 to 1308\n",
      "Data columns (total 13 columns):\n",
      "Passenger Class         1309 non-null int64\n",
      "Survived                1309 non-null object\n",
      "Sex                     1309 non-null object\n",
      "Age                     1309 non-null float64\n",
      "Siblings and Spouses    1309 non-null int64\n",
      "Parents and Children    1309 non-null int64\n",
      "Ticket                  1309 non-null object\n",
      "Fare                    1309 non-null float64\n",
      "Cabin                   1309 non-null object\n",
      "Port                    1309 non-null object\n",
      "Lifeboat                1309 non-null object\n",
      "Home_Destination        1309 non-null object\n",
      "Midpoint age            1309 non-null float64\n",
      "dtypes: float64(3), int64(3), object(7)\n",
      "memory usage: 133.0+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(   Passenger Class Survived     Sex      Age  Siblings and Spouses  \\\n",
       " 0                1      Yes  female  29.0000                     0   \n",
       " 1                1      Yes    male   0.9167                     1   \n",
       " 2                1       No  female   2.0000                     1   \n",
       " 3                1       No    male  30.0000                     1   \n",
       " 4                1       No  female  25.0000                     1   \n",
       " \n",
       "    Parents and Children  Ticket      Fare    Cabin Port Lifeboat  \\\n",
       " 0                     0   24160  211.3375       B5    S        2   \n",
       " 1                     2  113781  151.5500  C22 C26    S       11   \n",
       " 2                     2  113781  151.5500  C22 C26    S        0   \n",
       " 3                     2  113781  151.5500  C22 C26    S        0   \n",
       " 4                     2  113781  151.5500  C22 C26    S        0   \n",
       " \n",
       "                   Home_Destination  Midpoint age  \n",
       " 0                     St Louis, MO          27.5  \n",
       " 1  Montreal, PQ / Chesterville, ON           2.5  \n",
       " 2  Montreal, PQ / Chesterville, ON           2.5  \n",
       " 3  Montreal, PQ / Chesterville, ON          32.5  \n",
       " 4  Montreal, PQ / Chesterville, ON          27.5  , None)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(), df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_categoricalData(df):\n",
    "    columns = df.columns.values\n",
    "    \n",
    "    for column in columns:\n",
    "        text_digit_vals = {}\n",
    "        def convert_to_int(val):\n",
    "            return text_digit_vals[val]\n",
    "        \n",
    "        if df[column].dtype != np.int64 and df[column].dtype != np.float64:\n",
    "            column_contents = df[column].values.tolist()\n",
    "            unique_elements = set(column_contents)\n",
    "            x = 0\n",
    "            for unique in unique_elements:\n",
    "                if unique not in text_digit_vals:\n",
    "                    text_digit_vals[unique] = x\n",
    "                    x+=1\n",
    "                    \n",
    "            df[column] = list(map(convert_to_int, df[column]))\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Passenger Class</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Siblings and Spouses</th>\n",
       "      <th>Parents and Children</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Port</th>\n",
       "      <th>Lifeboat</th>\n",
       "      <th>Home_Destination</th>\n",
       "      <th>Midpoint age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>763</td>\n",
       "      <td>211.3375</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>111</td>\n",
       "      <td>27.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>761</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>73</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>336</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>761</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>73</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>336</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>761</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>73</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>336</td>\n",
       "      <td>32.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>25.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>761</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>73</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>336</td>\n",
       "      <td>27.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Passenger Class  Survived  Sex      Age  Siblings and Spouses  \\\n",
       "0                1         0    1  29.0000                     0   \n",
       "1                1         0    0   0.9167                     1   \n",
       "2                1         1    1   2.0000                     1   \n",
       "3                1         1    0  30.0000                     1   \n",
       "4                1         1    1  25.0000                     1   \n",
       "\n",
       "   Parents and Children  Ticket      Fare  Cabin  Port  Lifeboat  \\\n",
       "0                     0     763  211.3375     29     2        14   \n",
       "1                     2     761  151.5500     73     2        25   \n",
       "2                     2     761  151.5500     73     2         0   \n",
       "3                     2     761  151.5500     73     2         0   \n",
       "4                     2     761  151.5500     73     2         0   \n",
       "\n",
       "   Home_Destination  Midpoint age  \n",
       "0               111          27.5  \n",
       "1               336           2.5  \n",
       "2               336           2.5  \n",
       "3               336          32.5  \n",
       "4               336          27.5  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = handle_categoricalData(df)\n",
    "df2.head(5)\n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3 = df2\n",
    "df3.drop(['Lifeboat'],1)\n",
    "df3.drop(['Sex'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df3.drop(['Survived'],1).astype(float))\n",
    "X[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocessing.scale(X)\n",
    "X[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2.Survived\n",
    "y = np.array(df3.Survived)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KMeans(n_clusters=2)\n",
    "clf.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check how good is our prediction\n",
    "correct = 0\n",
    "for i in range(len(X)):\n",
    "    predict_me = np.array(X[i].astype(float))\n",
    "    predict_me = predict_me.reshape(-1, len(predict_me))\n",
    "    prediction = clf.predict(predict_me)\n",
    "    if prediction[0] == y[i]:\n",
    "        correct += 1\n",
    "        \n",
    "print(correct/len(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Under the hood - kMeans clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.cluster.hierarchy as hac\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use('ggplot')\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([[0.1,   2.5],\n",
    "              [0.5,   .4 ],\n",
    "              [0.3,   1  ],\n",
    "              [1  ,   .8 ],\n",
    "              [0.5,   0  ],\n",
    "              [0  ,   0.5],\n",
    "              [0.5,   0.5],\n",
    "              [2.7,   2  ],\n",
    "              [2.2,   3.1],\n",
    "              [3  ,   2  ],\n",
    "              [3.2,   0.3]])\n",
    "plt.scatter(data[:,0],data[:,1], s = 150, linewidths=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colorsList = [\"green\",'red',\"cyan\", \"black\", \"orange\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class K_Means:\n",
    "    def __init__(self, k = 2, tol = 0.0001, max_iter = 300):\n",
    "        self.k = k\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "        \n",
    "    def fit(self, data):\n",
    "        self.centroids = {}\n",
    "        for i in range(self.k):\n",
    "            self.centroids[i] = data[i]\n",
    "            \n",
    "        for i in range(self.max_iter):\n",
    "            self.classifications = {}\n",
    "            \n",
    "            for i in range(self.k):\n",
    "                self.classifications[i]=[]\n",
    "                \n",
    "            for featureset in data:\n",
    "                distances = [np.linalg.norm(featureset-self.centroids[centroid]) for centroid in self.centroids]\n",
    "                classification = distances.index(min(distances))\n",
    "                self.classifications[classification].append(featureset)\n",
    "                \n",
    "            prev_centroids = dict(self.centroids)\n",
    "            \n",
    "            for classification in self.classifications:\n",
    "                # pass\n",
    "                self.centroids[classification] = np.average(self.classifications[classification], axis=0)\n",
    "                \n",
    "            optimized = True\n",
    "            \n",
    "            for c in self.centroids:\n",
    "                original_centroid = prev_centroids[c]\n",
    "                current_centroid = self.centroids[c]\n",
    "                if np.sum((current_centroid-original_centroid)/original_centroid*100.0) > self.tol:\n",
    "                    # print(np.sum((current_centroid-original_centroid)/original_centroid*100.0))\n",
    "                    optimized = False\n",
    "                    \n",
    "            if optimized:\n",
    "                break\n",
    "                \n",
    "                \n",
    "    def predict(self, data):\n",
    "        distances = [np.linalg.norm(data-self.centroids[centroid]) for centroid in self.centroids]\n",
    "        classification = distances.index(min(distances))\n",
    "        # print(classification)\n",
    "        return classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = K_Means()\n",
    "clf.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for centroid in clf.centroids:\n",
    "    plt.scatter(clf.centroids[centroid][0], clf.centroids[centroid][1],marker = 'o', color = 'k', s = 150, linewidths=5)\n",
    "    \n",
    "for classification in clf.classifications:\n",
    "    color = colors[classification]\n",
    "    for featureset in clf.classifications[classification]:\n",
    "        plt.scatter(featureset[0], featureset[1], marker='x', color = colorsList[classification], s = 150, linewidths = 5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newData = np.array([[1, 3],\n",
    "                   [1.8,1.9],\n",
    "                   [0.5,0.3],\n",
    "                   [2.01,2.4],\n",
    "                   [2,2.5]\n",
    "                   ,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for newValues in newData:\n",
    "    classification = clf.predict(newValues)\n",
    "    plt.scatter(newValues[0], newValues[1], marker = '*', color = colorsList[classification], s =150, linewidths = 5)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does it do for the Titanic data ?\n",
    "\n",
    "clf = K_Means()\n",
    "clf.fit(X)\n",
    "\n",
    "correct = 0\n",
    "for i in range(len(X)):\n",
    "    predict_me = np.array(X[i].astype(float))\n",
    "    predict_me = predict_me.reshape(-1, len(predict_me))\n",
    "    prediction = clf.predict(predict_me)\n",
    "    if prediction == y[i]:\n",
    "        correct += 1\n",
    "        \n",
    "print(correct/len(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hierarchical Clustering - Using sklearn Mean Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import MeanShift\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import style\n",
    "style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = [[1,1,1], [5, 5,5],[3, 10, 10]]\n",
    "X, _ = make_blobs(n_samples = 100, centers = centers, cluster_std = 1)\n",
    "\n",
    "ms = MeanShift()\n",
    "ms.fit(X)\n",
    "labels = ms.labels_\n",
    "cluster_centers = ms.cluster_centers_\n",
    "print(cluster_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = len(np.unique(labels))\n",
    "print(\"Number of estimated Clusters:\", n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = 10*['r', 'g','b','c','k','y','m']\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "for i in range(len(X)):\n",
    "    ax.scatter(X[i][0],X[i][1],X[i][2], c = colors[labels[i]], marker = 'o')\n",
    "    \n",
    "ax.scatter(cluster_centers[:,0], cluster_centers[:,1], cluster_centers[:,2], marker = 'x', color='k', s= 150, linewidths = 5, zorder = 10)\n",
    "           \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Under the Hood - Self implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.cluster.hierarchy as hac\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use('ggplot')\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "data = np.array([[0.1,   2.5],\n",
    "              [0.5,   .4 ],\n",
    "              [0.3,   1  ],\n",
    "              [1  ,   .8 ],\n",
    "              [0.5,   0  ],\n",
    "              [0  ,   0.5],\n",
    "              [0.5,   0.5],\n",
    "              [2.7,   2  ],\n",
    "              [2.2,   3.1],\n",
    "              [3  ,   2  ],\n",
    "              [3.2,   0.3]])\n",
    "plt.scatter(data[:,0],data[:,1], s = 150, linewidths=5)\n",
    "plt.show()\n",
    "colorsList = 10*[\"green\",'red',\"cyan\", \"black\", \"orange\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mean_Shift:\n",
    "    def __init__(self, radius=2):\n",
    "        self.radius = radius\n",
    "        \n",
    "    def fit(self, data):\n",
    "        centroids = {}\n",
    "        \n",
    "        for i in range(len(data)):\n",
    "            centroids[i]=data[i]\n",
    "            \n",
    "        while True:\n",
    "            new_centroids = []\n",
    "            for i in centroids:\n",
    "                in_bandwidth = [];\n",
    "                centroid = centroids[i]\n",
    "                for featureset in data:\n",
    "                    if np.linalg.norm(featureset-centroid) < self.radius:\n",
    "                        in_bandwidth.append(featureset)\n",
    "                        \n",
    "                new_centroid = np.average(in_bandwidth, axis=0)\n",
    "                new_centroids.append(tuple(new_centroid))\n",
    "                \n",
    "            uniques = sorted(list(set(new_centroids)))\n",
    "            \n",
    "            prev_centroids = dict(centroids)\n",
    "            \n",
    "            centroids = {}\n",
    "            for i in range(len(uniques)):\n",
    "                centroids[i] = np.array(uniques[i])\n",
    "                \n",
    "            optimized = True\n",
    "            \n",
    "            for i in centroids:\n",
    "                if not np.array_equal(centroids[i],prev_centroids[i]):\n",
    "                    optimized = False\n",
    "                    \n",
    "                if not optimized:\n",
    "                    break\n",
    "                    \n",
    "            if optimized:\n",
    "                break\n",
    "                \n",
    "        self.centroids = centroids\n",
    "        \n",
    "    def predict(self, data):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Mean_Shift()\n",
    "clf.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = clf.centroids\n",
    "plt.scatter(data[:,0],data[:,1], s = 150, linewidths=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in centroids:\n",
    "    plt.scatter(centroids[c][0], centroids[c][1], color = 'k', marker = '*', s = 150)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mean_Shift_Dynamic:\n",
    "    def __init__(self, radius=None, radius_norm_step = 100):\n",
    "        self.radius = radius\n",
    "        self.radius_norm_step =radius_norm_step\n",
    "        \n",
    "    def fit(self, data):\n",
    "        \n",
    "        if self.radius == None:\n",
    "            all_data_centroid = np.average(data, axis=0)\n",
    "            all_data_norm = np.linalg.norm(all_data_centroid)\n",
    "            self.radius = all_data_norm/self.radius_norm_step\n",
    "            \n",
    "        \n",
    "        centroids = {}\n",
    "        \n",
    "        for i in range(len(data)):\n",
    "            centroids[i]=data[i]\n",
    "            \n",
    "        weights = [i for i in range(self.radius_norm_step)][::-1]\n",
    "        \n",
    "        while True:\n",
    "            new_centroids = []\n",
    "            for i in centroids:\n",
    "                in_bandwidth = [];\n",
    "                centroid = centroids[i]\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                for featureset in data:\n",
    "                    \n",
    "                    distance = np.linalg.norm(featureset-centroid)\n",
    "                    if distance == 0:\n",
    "                        distance = 0.00001\n",
    "                    weight_index = int(distance/self.radius)\n",
    "                    if weight_index > self.radius_norm_step-1:\n",
    "                        weight_index = self.radius_norm_step-1\n",
    "                        \n",
    "                    to_add = (weights[weight_index]**2)*[featureset]\n",
    "                    # to_add = (weights[weight_index])*[featureset] # faster, not as reliable\n",
    "                    \n",
    "                    in_bandwidth += to_add\n",
    "                    \n",
    "                        \n",
    "                new_centroid = np.average(in_bandwidth, axis=0)\n",
    "                new_centroids.append(tuple(new_centroid))\n",
    "                \n",
    "            uniques = sorted(list(set(new_centroids)))\n",
    "            \n",
    "            to_pop = []\n",
    "            for i in uniques:\n",
    "                for ii in uniques:\n",
    "                    if i == ii:\n",
    "                        pass\n",
    "                    elif np.linalg.norm(np.array(i)-np.array(ii)) <= self.radius:\n",
    "                        to_pop.append(ii)\n",
    "                        break # eliminate duplicates\n",
    "                        \n",
    "            for i in to_pop:\n",
    "                try:\n",
    "                    uniques.remove(i)\n",
    "                except:\n",
    "                    pass\n",
    "                    \n",
    "            \n",
    "            prev_centroids = dict(centroids)\n",
    "            \n",
    "            centroids = {}\n",
    "            for i in range(len(uniques)):\n",
    "                centroids[i] = np.array(uniques[i])\n",
    "                \n",
    "            optimized = True\n",
    "            \n",
    "            for i in centroids:\n",
    "                if not np.array_equal(centroids[i],prev_centroids[i]):\n",
    "                    optimized = False\n",
    "                    \n",
    "                if not optimized:\n",
    "                    break\n",
    "                    \n",
    "            if optimized:\n",
    "                break\n",
    "                \n",
    "        self.centroids = centroids\n",
    "        \n",
    "        self.classifications = {}\n",
    "        \n",
    "        for i in range(len(self.centroids)):\n",
    "            self.classifications[i] = []\n",
    "            \n",
    "        for featureset in data:\n",
    "            distances = [np.linalg.norm(featureset-self.centroids[centroid]) for centroid in self.centroids]\n",
    "            classification = distances.index(min(distances))\n",
    "            self.classifications[classification].append(featureset)\n",
    "            \n",
    "        \n",
    "    def predict(self, data):\n",
    "        distances = [np.linalg.norm(featureset-self.centroids[centroid]) for centroid in self.centroids]\n",
    "        classification = distances.index(min(distances))\n",
    "        return classification\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Mean_Shift_Dynamic()\n",
    "clf.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = clf.centroids\n",
    "plt.scatter(data[:,0],data[:,1], s = 150, linewidths=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in centroids:\n",
    "    plt.scatter(centroids[c][0], centroids[c][1], color = 'k', marker = '*', s = 150)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Mean_Shift_Dynamic()\n",
    "clf.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = clf.centroids\n",
    "\n",
    "for classification in clf.classifications:\n",
    "    color = colorsList[classification]\n",
    "    for featureset in clf.classifications[classification]:\n",
    "        plt.scatter(featureset[0],featureset[1], marker= 'x', color = colorsList[classification], s = 150, linewidths = 5)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in centroids:\n",
    "    plt.scatter(centroids[c][0], centroids[c][1], color = 'k', marker = '*', s = 150)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "\n",
    "# X, y = make_blobs(n_samples =150, centers = 3, n_features = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "randomCenters= random.randrange(2,8)\n",
    "X, y = make_blobs(n_samples =50, centers = randomCenters, n_features = 2)\n",
    "\n",
    "clf = Mean_Shift_Dynamic()\n",
    "clf.fit(X)\n",
    "centroids = clf.centroids\n",
    "\n",
    "for classification in clf.classifications:\n",
    "    color = colorsList[classification]\n",
    "    for featureset in clf.classifications[classification]:\n",
    "        plt.scatter(featureset[0],featureset[1], marker= 'x', color = colorsList[classification], s = 150, linewidths = 15)\n",
    "        \n",
    "for c in centroids:\n",
    "    plt.scatter(centroids[c][0], centroids[c][1], color = 'k', marker = '*', s = 150)\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "*Adapted from [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "Why are we learning about decision trees?\n",
    "\n",
    "- They're useful for both regression and classification problems.\n",
    "- They're popular (for a variety of reasons).\n",
    "- They're the basis for more sophisticated modeling approaches.\n",
    "- They demonstrate a different way of \"thinking\" than the models we have studied so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression trees\n",
    "\n",
    "Let's look at a simple example to get started.\n",
    "\n",
    "Our goal is to **predict a baseball player's Salary** based on **Years** (number of years playing in the major leagues) and **Hits** (number of hits he made in the previous year). Here is the training data, represented visually (low salary is blue/green, high salary is red/yellow):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Salary data](salary_color.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How might you \"stratify\" or \"segment\" the feature space into regions, based on salary?** Here are the rules:\n",
    "\n",
    "- You can only use straight lines, drawn one at a time.\n",
    "- Your line must either be vertical or horizontal.\n",
    "- Your line stops when it hits an existing line.\n",
    "\n",
    "Intuitively, you want to **maximize** the similarity (or \"homogeneity\") within a given region, and **minimize** the similarity between different regions.\n",
    "\n",
    "*Let's take a minute and do this...*\n",
    "\n",
    "Below is a regression tree that has been fit to the data by a computer. (We will talk later about how the fitting algorithm actually works.) Note that  Salary is measured in thousands and has been log-transformed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Salary tree](salary_tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do we make Salary predictions (for out-of-sample data) using a decision tree?**\n",
    "\n",
    "- Start at the top, and examine the first \"splitting rule\" (Years < 4.5).\n",
    "- If the rule is **True** for a given player, follow the **left branch**. If the rule is **False**, follow the **right branch**.\n",
    "- Continue until reaching the bottom. The predicted Salary is the number in that particular \"bucket\".\n",
    "- **Note:** Years and Hits are both integers, but the convention is to label these rules using the midpoint between adjacent values.\n",
    "\n",
    "Example predictions:\n",
    "\n",
    "- Years=3, then predict 5.11 ($\\$1000 \\times e^{5.11} \\approx \\$166000$)\n",
    "- Years=5 and Hits=100, then predict 6.00 ($\\$1000 \\times e^{6.00} \\approx \\$403000$)\n",
    "- Years=8 and Hits=120, then predict 6.74 ($\\$1000 \\times e^{6.74} \\approx \\$846000$)\n",
    "\n",
    "**How did we come up with the numbers at the bottom of the tree?** Each number is just the **mean Salary in the training data** of players who fit that criteria.\n",
    "\n",
    "Here's the same diagram as before, split into the three regions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Salary regions](salary_regions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This diagram is essentially a combination of the two previous diagrams. In $R_1$, the mean log Salary was 5.11. In $R_2$, the mean log Salary was 6.00. In $R_3$, the mean log Salary was 6.74. Thus, those values are used to predict out-of-sample data.\n",
    "\n",
    "Let's introduce some terminology:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Salary tree annotated](salary_tree_annotated.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How might you interpret the \"meaning\" of this tree?**\n",
    "\n",
    "- Years is the most important factor determining Salary, with a lower number of Years corresponding to a lower Salary.\n",
    "- For a player with a lower number of Years, Hits is not an important factor determining Salary.\n",
    "- For a player with a higher number of Years, Hits is an important factor determining Salary, with a greater number of Hits corresponding to a higher Salary.\n",
    "\n",
    "What we have seen so far hints at the advantages and disadvantages of decision trees:\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "- Highly interpretable\n",
    "- Can be displayed graphically\n",
    "- Prediction is fast\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "- Predictive accuracy is not as high as some supervised learning methods\n",
    "- Can easily overfit the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a regression tree by hand\n",
    "\n",
    "How do you build a decision tree? You're going to find out by building one in pairs!\n",
    "\n",
    "Your training data is a tiny dataset of [used vehicle sale prices](vehicles_train.csv). Your goal is to predict **price** for testing data. Here are your instructions:\n",
    "\n",
    "- Read the data into Pandas.\n",
    "- Explore the data by sorting, plotting, or split-apply-combine (aka `group_by`).\n",
    "- Decide which feature is the most important predictor, and use that to make your first split. (Only binary splits are allowed!)\n",
    "- After making your first split, you should actually split your data in Pandas into two parts, and then explore each part to figure out what other splits to make.\n",
    "- Stop making splits once you are convinced that it strikes a good balance between underfitting and overfitting. (As always, your goal is to build a model that generalizes well!)\n",
    "- You are allowed to split on the same variable multiple times!\n",
    "- Draw your tree, making sure to label your leaves with the mean Price for the observations in that \"bucket\".\n",
    "- When you're finished, review your tree to make sure nothing is backwards. (Remember: follow the **left branch** if the rule is **true**, and follow the **right branch** if the rule is **false**.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does a computer build a regression tree?\n",
    "\n",
    "The ideal approach would be for the computer to consider every possible partition of the feature space. However, this is computationally infeasible, so instead an approach is used called **recursive binary splitting:**\n",
    "\n",
    "- Begin at the top of the tree.\n",
    "- For every single predictor, examine every possible cutpoint, and choose the predictor and cutpoint such that the resulting tree has the **lowest possible mean squared error (MSE)**. Make that split.\n",
    "- Repeat the examination for the two resulting regions, and again make a single split (in one of the regions) to minimize the MSE.\n",
    "- Keep repeating this process until a stopping criterion is met, such as **maximum tree depth** or **minimum number of samples in a leaf**.\n",
    "\n",
    "Below is the regression tree for player salaries grown much deeper, and a comparison of the training, test, and cross-validation errors for trees with different numbers of leaves:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Salary unpruned](salary_unpruned.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the **training error** continues to go down as the tree size increases, but the lowest **cross-validation error** occurs for a tree with 3 leaves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a regression tree in scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>year</th>\n",
       "      <th>miles</th>\n",
       "      <th>doors</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22000</td>\n",
       "      <td>2012</td>\n",
       "      <td>13000</td>\n",
       "      <td>2</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14000</td>\n",
       "      <td>2010</td>\n",
       "      <td>30000</td>\n",
       "      <td>2</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13000</td>\n",
       "      <td>2010</td>\n",
       "      <td>73500</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9500</td>\n",
       "      <td>2009</td>\n",
       "      <td>78000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9000</td>\n",
       "      <td>2007</td>\n",
       "      <td>47000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4000</td>\n",
       "      <td>2006</td>\n",
       "      <td>124000</td>\n",
       "      <td>2</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3000</td>\n",
       "      <td>2004</td>\n",
       "      <td>177000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2000</td>\n",
       "      <td>2004</td>\n",
       "      <td>209000</td>\n",
       "      <td>4</td>\n",
       "      <td>truck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3000</td>\n",
       "      <td>2003</td>\n",
       "      <td>138000</td>\n",
       "      <td>2</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1900</td>\n",
       "      <td>2003</td>\n",
       "      <td>160000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2500</td>\n",
       "      <td>2003</td>\n",
       "      <td>190000</td>\n",
       "      <td>2</td>\n",
       "      <td>truck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5000</td>\n",
       "      <td>2001</td>\n",
       "      <td>62000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1800</td>\n",
       "      <td>1999</td>\n",
       "      <td>163000</td>\n",
       "      <td>2</td>\n",
       "      <td>truck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1300</td>\n",
       "      <td>1997</td>\n",
       "      <td>138000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    price  year   miles  doors   type\n",
       "0   22000  2012   13000      2    car\n",
       "1   14000  2010   30000      2    car\n",
       "2   13000  2010   73500      4    car\n",
       "3    9500  2009   78000      4    car\n",
       "4    9000  2007   47000      4    car\n",
       "5    4000  2006  124000      2    car\n",
       "6    3000  2004  177000      4    car\n",
       "7    2000  2004  209000      4  truck\n",
       "8    3000  2003  138000      2    car\n",
       "9    1900  2003  160000      4    car\n",
       "10   2500  2003  190000      2  truck\n",
       "11   5000  2001   62000      4    car\n",
       "12   1800  1999  163000      2  truck\n",
       "13   1300  1997  138000      4    car"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the training data into pandas and print it out\n",
    "import pandas as pd\n",
    "train = pd.read_csv('vehicles_train.csv')\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode car as 0 and truck as 1\n",
    "train['type'] = train.type.map({'car':0, 'truck':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of the feature columns (every column except for the 0th column)\n",
    "feature_cols = train.columns[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define X (features) and y (response)\n",
    "X = train[feature_cols]\n",
    "y = train.price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the relevant class, and instantiate the model (with random_state=1)\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "treereg = DecisionTreeRegressor(random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=1, splitter='best')"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the model object to see the default arguments\n",
    "treereg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4707.250588484563"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use 3-fold cross-validation to estimate the RMSE for this model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "scores = cross_val_score(treereg, X, y, cv=3, scoring='neg_mean_squared_error')\n",
    "np.mean(np.sqrt(-scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning a regression tree\n",
    "\n",
    "Let's see if we can reduce the RMSE by tuning the **max_depth** parameter. One way to search for an optimal value would be to try different values, one by one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DecisionTreeRegressor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-96-77cd3e1231cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# try max_depth=1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtreereg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDecisionTreeRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtreereg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'neg_mean_squared_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DecisionTreeRegressor' is not defined"
     ]
    }
   ],
   "source": [
    "# try max_depth=1\n",
    "treereg = DecisionTreeRegressor(max_depth=1, random_state=1)\n",
    "scores = cross_val_score(treereg, X, y, cv=3, scoring='neg_mean_squared_error')\n",
    "np.mean(np.sqrt(-scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, we could write a loop to try a range of values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a range of values\n",
    "max_depth_range = range(1, 11)\n",
    "\n",
    "# create an empty list to store the average RMSE for each value of max_depth\n",
    "RMSE_scores = []\n",
    "\n",
    "# use cross-validation with each value of max_depth\n",
    "for depth in max_depth_range:\n",
    "    treereg = DecisionTreeRegressor(max_depth=depth, random_state=1)\n",
    "    MSE_scores = cross_val_score(treereg, X, y, cv=3, scoring='neg_mean_squared_error')\n",
    "    RMSE_scores.append(np.mean(np.sqrt(-MSE_scores)))\n",
    "    \n",
    "# print the results\n",
    "RMSE_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the max_depth (x-axis) versus the RMSE (y-axis)\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(max_depth_range, RMSE_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_depth=3 was best, so fit a tree using that parameter\n",
    "treereg = DecisionTreeRegressor(max_depth=3, random_state=1)\n",
    "treereg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the \"Gini importance\" of each feature: the (normalized) total reduction of MSE brought by that feature\n",
    "pd.DataFrame({'feature':feature_cols, 'importance':treereg.feature_importances_})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Creating a tree diagram\n",
    "\n",
    "\n",
    "![Tree for vehicle data](tree_vehicles.png)\n",
    "\n",
    "How do we read this decision tree?\n",
    "\n",
    "**Internal nodes:**\n",
    "\n",
    "- \"samples\" is the number of observations in that node before splitting\n",
    "- \"mse\" is the mean squared error calculated by comparing the actual response values in that node against the mean response value in that node\n",
    "- first line is the condition used to split that node (go left if true, go right if false)\n",
    "\n",
    "**Leaves:**\n",
    "\n",
    "- \"samples\" is the number of observations in that node\n",
    "- \"value\" is the mean response value in that node\n",
    "- \"mse\" is the mean squared error calculated by comparing the actual response values in that node against \"value\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting on test data\n",
    "\n",
    "How good is scikit-learn's regression tree at predicting the price for test observations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>year</th>\n",
       "      <th>miles</th>\n",
       "      <th>doors</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3000</td>\n",
       "      <td>2003</td>\n",
       "      <td>130000</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6000</td>\n",
       "      <td>2005</td>\n",
       "      <td>82500</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12000</td>\n",
       "      <td>2010</td>\n",
       "      <td>60000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   price  year   miles  doors  type\n",
       "0   3000  2003  130000      4     1\n",
       "1   6000  2005   82500      4     0\n",
       "2  12000  2010   60000      2     0"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the test data\n",
    "test = pd.read_csv('vehicles_test.csv')\n",
    "\n",
    "# encode car as 0 and truck as 1\n",
    "test['type'] = test.type.map({'car':0, 'truck':1})\n",
    "\n",
    "# print the data\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define X and y\n",
    "X_test = test[feature_cols]\n",
    "y_test = test.price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on test data\n",
    "y_pred = treereg.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-105-a38bb199bb6b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# calculate RMSE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "# calculate RMSE\n",
    "from sklearn import metrics\n",
    "np.sqrt(metrics.mean_squared_error(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate RMSE for your own tree!\n",
    "y_test = [3000, 6000, 12000]\n",
    "y_pred = [3057, 3057, 16333]\n",
    "np.sqrt(metrics.mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification trees\n",
    "\n",
    "Classification trees are very similar to regression trees. Here is a quick comparison:\n",
    "\n",
    "|regression trees|classification trees|\n",
    "|---|---|\n",
    "|predict a continuous response|predict a categorical response|\n",
    "|predict using mean response of each leaf|predict using most commonly occuring class of each leaf|\n",
    "|splits are chosen to minimize MSE|splits are chosen to minimize Gini index (discussed below)|\n",
    "\n",
    "Here's an **example of a classification tree**, which predicts whether Barack Obama or Hillary Clinton would win the Democratic primary in a particular county in 2008:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Obama-Clinton decision tree](obama_clinton_tree.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A few questions:**\n",
    "\n",
    "- What is the response variable?\n",
    "- What are the features?\n",
    "- What is the most predictive feature?\n",
    "- How would we calculate the total number of counties?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting criteria for classification trees\n",
    "\n",
    "Here are common options for the splitting criteria:\n",
    "\n",
    "- **classification error rate:** fraction of training observations in a region that don't belong to the most common class\n",
    "- **Gini index:** measure of total variance across classes in a region\n",
    "- **cross-entropy:** numerically similar to Gini index\n",
    "\n",
    "The goal when splitting is to increase the \"node purity\", and it turns out that the **Gini index and cross-entropy** are better measures of purity than classification error rate. The Gini index is faster to compute than cross-entropy, so it is generally preferred (and is used by scikit-learn by default)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example calculations of Gini index\n",
    "\n",
    "Let's say that we are predicting survival on the Titanic. At a particular node, there are **25 individuals**, of whom 10 survived and 15 died. Here's how we calculate the Gini index before making a split:\n",
    "\n",
    "$$1 - \\left(\\frac {Survived} {Total}\\right)^2 - \\left(\\frac {Died} {Total}\\right)^2 = 1 - \\left(\\frac {10} {25}\\right)^2 - \\left(\\frac {15} {25}\\right)^2 = 0.48$$\n",
    "\n",
    "The **maximum value** of the Gini index is 0.5, and occurs when the classes are perfectly balanced in a node. The **minimum value** of the Gini index is 0, and occurs when there is only one class represented in a node. Thus, a node with a lower Gini index is said to be more \"pure\".\n",
    "\n",
    "**When deciding between splits**, the decision tree algorithm chooses the split that maximizes the resulting node purity. Let's pretend that gender was the split being considered, and the resulting nodes are as follows:\n",
    "\n",
    "- **Males:** 2 survived, 13 died\n",
    "- **Females:** 8 survived, 2 died\n",
    "\n",
    "To evaluate this split, we calculate the **weighted average of the Gini indices of the resulting nodes:**\n",
    "\n",
    "$$\\text{Males: } 1 - \\left(\\frac {2} {15}\\right)^2 - \\left(\\frac {13} {15}\\right)^2 = 0.23$$\n",
    "$$\\text{Females: } 1 - \\left(\\frac {8} {10}\\right)^2 - \\left(\\frac {2} {10}\\right)^2 = 0.32$$\n",
    "$$\\text{Weighted Average: } 0.23 \\left(\\frac {15} {25}\\right) + 0.32 \\left(\\frac {10} {25}\\right) = 0.27$$\n",
    "\n",
    "Thus, the decrease in Gini index (and gain in purity) from splitting on gender is **0.21**. The decision tree algorithm will choose this split if no other splits result in a larger gain in purity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a classification tree in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll build a classification tree using the [Titanic data](https://www.kaggle.com/c/titanic-gettingStarted/data) provided by Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Moran, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330877</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>McCarthy, Mr. Timothy J</td>\n",
       "      <td>male</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17463</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>E46</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Palsson, Master. Gosta Leonard</td>\n",
       "      <td>male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>349909</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)</td>\n",
       "      <td>female</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>347742</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Nasser, Mrs. Nicholas (Adele Achem)</td>\n",
       "      <td>female</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>237736</td>\n",
       "      <td>30.0708</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "5            6         0       3   \n",
       "6            7         0       1   \n",
       "7            8         0       3   \n",
       "8            9         1       3   \n",
       "9           10         1       2   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "5                                   Moran, Mr. James    male   NaN      0   \n",
       "6                            McCarthy, Mr. Timothy J    male  54.0      0   \n",
       "7                     Palsson, Master. Gosta Leonard    male   2.0      3   \n",
       "8  Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)  female  27.0      0   \n",
       "9                Nasser, Mrs. Nicholas (Adele Achem)  female  14.0      1   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  \n",
       "5      0            330877   8.4583   NaN        Q  \n",
       "6      0             17463  51.8625   E46        S  \n",
       "7      1            349909  21.0750   NaN        S  \n",
       "8      2            347742  11.1333   NaN        S  \n",
       "9      0            237736  30.0708   NaN        C  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the data\n",
    "titanic = pd.read_csv('titanic_train.csv')\n",
    "titanic.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's choose our response variable and a few features, and review **how to handle categorical features**:\n",
    "\n",
    "- **Survived:** This is our response variable, and is already encoded as 0=died and 1=survived.\n",
    "- **Pclass:** These are the passenger class categories (1=first class, 2=second class, 3=third class). They are logically ordered, so we'll leave them as-is. (If the tree splits on this feature, the splits will occur at 1.5 or 2.5.)\n",
    "- **Sex:** This is a binary category, so we should encode it as 0=female and 1=male. (If the tree splits on this feature, the split will occur at 0.5.)\n",
    "- **Age:** This is a numeric feature, but we need to fill in the missing values.\n",
    "- **Embarked:** This is the port they embarked from. There are three unordered categories, so we should create dummy variables and drop one level as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>1</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>0</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>0</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>0</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>1</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Moran, Mr. James</td>\n",
       "      <td>1</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330877</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>McCarthy, Mr. Timothy J</td>\n",
       "      <td>1</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17463</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>E46</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Palsson, Master. Gosta Leonard</td>\n",
       "      <td>1</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>349909</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)</td>\n",
       "      <td>0</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>347742</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Nasser, Mrs. Nicholas (Adele Achem)</td>\n",
       "      <td>0</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>237736</td>\n",
       "      <td>30.0708</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "5            6         0       3   \n",
       "6            7         0       1   \n",
       "7            8         0       3   \n",
       "8            9         1       3   \n",
       "9           10         1       2   \n",
       "\n",
       "                                                Name  Sex        Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    1  22.000000      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...    0  38.000000      1   \n",
       "2                             Heikkinen, Miss. Laina    0  26.000000      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)    0  35.000000      1   \n",
       "4                           Allen, Mr. William Henry    1  35.000000      0   \n",
       "5                                   Moran, Mr. James    1  29.699118      0   \n",
       "6                            McCarthy, Mr. Timothy J    1  54.000000      0   \n",
       "7                     Palsson, Master. Gosta Leonard    1   2.000000      3   \n",
       "8  Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)    0  27.000000      0   \n",
       "9                Nasser, Mrs. Nicholas (Adele Achem)    0  14.000000      1   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  Embarked_Q  Embarked_S  \n",
       "0      0         A/5 21171   7.2500   NaN        S           0           1  \n",
       "1      0          PC 17599  71.2833   C85        C           0           0  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S           0           1  \n",
       "3      0            113803  53.1000  C123        S           0           1  \n",
       "4      0            373450   8.0500   NaN        S           0           1  \n",
       "5      0            330877   8.4583   NaN        Q           1           0  \n",
       "6      0             17463  51.8625   E46        S           0           1  \n",
       "7      1            349909  21.0750   NaN        S           0           1  \n",
       "8      2            347742  11.1333   NaN        S           0           1  \n",
       "9      0            237736  30.0708   NaN        C           0           0  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode female as 0 and male as 1\n",
    "titanic['Sex'] = titanic.Sex.map({'female':0, 'male':1})\n",
    "\n",
    "# fill in the missing values for age with the mean age\n",
    "titanic.Age.fillna(titanic.Age.mean(), inplace=True)\n",
    "\n",
    "# create three dummy variables, drop the first dummy variable, and store the two remaining columns as a DataFrame\n",
    "embarked_dummies = pd.get_dummies(titanic.Embarked, prefix='Embarked').iloc[:, 1:]\n",
    "\n",
    "# concatenate the two dummy variable columns onto the original DataFrame\n",
    "titanic = pd.concat([titanic, embarked_dummies], axis=1)\n",
    "\n",
    "# print the updated DataFrame\n",
    "titanic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of the feature columns\n",
    "feature_cols = ['Pclass', 'Sex', 'Age', 'Embarked_Q', 'Embarked_S']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define X (features) and y (response)\n",
    "X = titanic[feature_cols]\n",
    "y = titanic.Survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pandas.core.frame.DataFrame, pandas.core.series.Series)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X), type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a classification tree with max_depth=3 on all data\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "treeclf = DecisionTreeClassifier(max_depth=3, random_state=1)\n",
    "treeclf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Tree for Titanic data](tree_titanic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Notice the split in the bottom right. The **same class** is predicted in both of its leaves! Why did this split occur?\n",
    "\n",
    "Although that split didn't affect the **classification error rate**, it did increase the **node purity**. Node purity is important because we're interested in the class proportions among the observations in each region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the feature importances\n",
    "pd.DataFrame({'feature':feature_cols, 'importance':treeclf.feature_importances_})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping up decision trees\n",
    "\n",
    "Here are some advantages and disadvantages of decision trees that we haven't yet talked about:\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "- Can be specified as a series of rules, and are thought to more closely approximate human decision-making than other models\n",
    "- Non-parametric (will do better than linear models if relationship between features and response is highly non-linear)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "![Trees versus linear models](tree_vs_linear.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disadvantages:**\n",
    "\n",
    "- Small variations in the data can result in a completely different tree (high variance)\n",
    "- Recursive binary splitting makes \"locally optimal\" decisions that may not result in a globally optimal tree\n",
    "- Can create biased trees if the classes are highly imbalanced\n",
    "\n",
    "Note that there is not just one decision tree algorithm; instead, there are many variations. A few common decision tree algorithms that are often referred to by name are C4.5, C5.0, and CART. (More details are available in the [scikit-learn documentation](http://scikit-learn.org/stable/modules/tree.html#tree-algorithms-id3-c4-5-c5-0-and-cart).) scikit-learn uses an \"optimized version\" of CART."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- scikit-learn documentation: [Decision Trees](http://scikit-learn.org/stable/modules/tree.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Ensembling\n",
    "\n",
    "*Adapted from [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/)*\n",
    "\n",
    "Let's pretend that instead of building a single model to solve a classification problem, you created **five independent models**, and each model was correct 70% of the time. If you combined these models into an \"ensemble\" and used their majority vote as a prediction, how often would the ensemble be correct?\n",
    "\n",
    "Let's simulate it to find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a seed for reproducibility\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 1000 random numbers (between 0 and 1) for each model, representing 1000 observations\n",
    "mod1 = np.random.rand(1000)\n",
    "mod2 = np.random.rand(1000)\n",
    "mod3 = np.random.rand(1000)\n",
    "mod4 = np.random.rand(1000)\n",
    "mod5 = np.random.rand(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each model independently predicts 1 (the \"correct response\") if random number was at least 0.3\n",
    "preds1 = np.where(mod1 > 0.3, 1, 0)\n",
    "preds2 = np.where(mod2 > 0.3, 1, 0)\n",
    "preds3 = np.where(mod3 > 0.3, 1, 0)\n",
    "preds4 = np.where(mod4 > 0.3, 1, 0)\n",
    "preds5 = np.where(mod5 > 0.3, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the predictions together\n",
    "sum_of_preds = preds1 + preds2 + preds3 + preds4 + preds5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensemble predicts 1 (the \"correct response\") if at least 3 models predict 1\n",
    "ensemble_preds = np.where(sum_of_preds >=3 , 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the ensemble's first 20 predictions\n",
    "print(ensemble_preds[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first 20 predictions from each model\n",
    "print(preds1[:20])\n",
    "print(preds2[:20])\n",
    "print(preds3[:20])\n",
    "print(preds4[:20])\n",
    "print(preds5[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how accurate was the ensemble?\n",
    "ensemble_preds.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazing, right?\n",
    "\n",
    "**Ensemble learning (or \"ensembling\")** is simply the process of combining several models to solve a prediction problem, with the goal of producing a combined model that is more accurate than any individual model. For **classification** problems, the combination is often done by majority vote. For **regression** problems, the combination is often done by taking an average of the predictions.\n",
    "\n",
    "For ensembling to work well, the individual models must meet two conditions:\n",
    "\n",
    "- Models should be **accurate** (they must outperform random guessing)\n",
    "- Models should be **independent** (their predictions are not correlated with one another)\n",
    "\n",
    "The idea, then, is that if you have a collection of individually imperfect (and independent) models, the \"one-off\" mistakes made by each model are probably not going to be made by the rest of the models, and thus the mistakes will be discarded when averaging the models.\n",
    "\n",
    "It turns out that as you add more models to the voting process, the probability of error decreases. This is known as [Condorcet's Jury Theorem](http://en.wikipedia.org/wiki/Condorcet%27s_jury_theorem), which was developed by a French political scientist in the 18th century.\n",
    "\n",
    "Anyway, we'll see examples of ensembling below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapping\n",
    "\n",
    "**Some preliminary terminology:** In statistics, \"bootstrapping\" refers to the process of using \"bootstrap samples\" to quantify the uncertainty of a model. Bootstrap samples are simply random samples with replacement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "# create an array of 0 to 9, then sample 10 times with replacement\n",
    "np.random.choice(a=10, size=10, replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging\n",
    "\n",
    "On their own, decision trees are not competitive with the best supervised learning methods in terms of **predictive accuracy**. However, they can be used as the basis for more sophisticated methods that have much higher accuracy!\n",
    "\n",
    "One of the main issues with decision trees is **high variance**, meaning that different splits in the training data can lead to very different trees. **\"Bootstrap aggregation\" (aka \"bagging\")** is a general purpose procedure for reducing the variance of a machine learning method, but is particularly useful for decision trees.\n",
    "\n",
    "What is the bagging process (in general)?\n",
    "\n",
    "- Take repeated bootstrap samples (random samples with replacement) from the training data set\n",
    "- Train our method on each bootstrapped training set and make predictions\n",
    "- Average the predictions\n",
    "\n",
    "This increases predictive accuracy by **reducing the variance**, similar to how cross-validation reduces the variance associated with the test set approach (for estimating out-of-sample error) by splitting many times an averaging the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying bagging to decision trees\n",
    "\n",
    "So how exactly can bagging be used with decision trees? Here's how it applies to **regression trees**:\n",
    "\n",
    "- Grow B regression trees using B bootstrapped training sets\n",
    "- Grow each tree deep so that each one has low bias\n",
    "- Every tree makes a numeric prediction, and the predictions are averaged (to reduce the variance)\n",
    "\n",
    "It is applied in a similar fashion to **classification trees**, except that during the prediction stage, the overall prediction is based upon a majority vote of the trees.\n",
    "\n",
    "**What value should be used for B?** Simply use a large enough value that the error seems to have stabilized. (Choosing a value of B that is \"too large\" will generally not lead to overfitting.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually implementing bagged decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in vehicle data\n",
    "vehicles = pd.read_csv('used_vehicles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert car to 0 and truck to 1\n",
    "vehicles['type'] = vehicles.type.map({'car':0, 'truck':1})\n",
    "\n",
    "# print out data\n",
    "vehicles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the number of rows in vehicles\n",
    "n_rows = vehicles.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a seed for reproducibility\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create three bootstrap samples (will be used to select rows from the DataFrame)\n",
    "sample1 = np.random.choice(a=n_rows, size=n_rows, replace=True)\n",
    "sample2 = np.random.choice(a=n_rows, size=n_rows, replace=True)\n",
    "sample3 = np.random.choice(a=n_rows, size=n_rows, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print samples\n",
    "print(sample1)\n",
    "print(sample2)\n",
    "print(sample3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use sample1 to select rows from DataFrame\n",
    "print(vehicles.iloc[sample1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# grow one regression tree with each bootstrapped training set\n",
    "treereg1 = DecisionTreeRegressor(random_state=123)\n",
    "treereg1.fit(vehicles.iloc[sample1, 1:], vehicles.iloc[sample1, 0])\n",
    "\n",
    "treereg2 = DecisionTreeRegressor(random_state=123)\n",
    "treereg2.fit(vehicles.iloc[sample2, 1:], vehicles.iloc[sample2, 0])\n",
    "\n",
    "treereg3 = DecisionTreeRegressor(random_state=123)\n",
    "treereg3.fit(vehicles.iloc[sample3, 1:], vehicles.iloc[sample3, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in out-of-sample data\n",
    "oos = pd.read_csv('used_vehicles_oos.csv')\n",
    "\n",
    "# convert car to 0 and truck to 1\n",
    "oos['type'] = oos.type.map({'car':0, 'truck':1})\n",
    "\n",
    "# print data\n",
    "oos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select feature columns (every column except for the 0th column)\n",
    "feature_cols = vehicles.columns[1:]\n",
    "\n",
    "# make predictions on out-of-sample data\n",
    "preds1 = treereg1.predict(oos[feature_cols])\n",
    "preds2 = treereg2.predict(oos[feature_cols])\n",
    "preds3 = treereg3.predict(oos[feature_cols])\n",
    "\n",
    "# print predictions\n",
    "print(preds1)\n",
    "print(preds2)\n",
    "print(preds3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average predictions and compare to actual values\n",
    "print((preds1 + preds2 + preds3)/3)\n",
    "print(oos.price.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating out-of-sample error\n",
    "\n",
    "Bagged models have a very nice property: **out-of-sample error can be estimated without using the test set approach or cross-validation!**\n",
    "\n",
    "Here's how the out-of-sample estimation process works with bagged trees:\n",
    "\n",
    "- On average, each bagged tree uses about two-thirds of the observations. **For each tree, the remaining observations are called \"out-of-bag\" observations.**\n",
    "- For the first observation in the training data, predict its response using **only** the trees in which that observation was out-of-bag. Average those predictions (for regression) or take a majority vote (for classification).\n",
    "- Repeat this process for every observation in the training data.\n",
    "- Compare all predictions to the actual responses in order to compute a mean squared error or classification error. This is known as the **out-of-bag error**.\n",
    "\n",
    "**When B is sufficiently large, the out-of-bag error is an accurate estimate of out-of-sample error.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set is a data structure used to identify unique elements\n",
    "print(set(range(14)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only show the unique elements in sample1\n",
    "print(set(sample1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the \"set difference\" to identify the out-of-bag observations for each tree\n",
    "print(sorted(set(range(14)) - set(sample1)))\n",
    "print(sorted(set(range(14)) - set(sample2)))\n",
    "print(sorted(set(range(14)) - set(sample3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we would predict the response for **observation 4** by using tree 1 (because it is only out-of-bag for tree 1). We would predict the response for **observation 5** by averaging the predictions from trees 1, 2, and 3 (since it is out-of-bag for all three trees). We would repeat this process for all observations, and then calculate the MSE using those predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating variable importance\n",
    "\n",
    "Although bagging **increases predictive accuracy**, it **decreases model interpretability** because it's no longer possible to visualize the tree to understand the importance of each variable.\n",
    "\n",
    "However, we can still obtain an overall summary of \"variable importance\" from bagged models:\n",
    "\n",
    "- To compute variable importance for bagged regression trees, we can calculate the **total amount that the mean squared error is decreased due to splits over a given predictor, averaged over all trees**.\n",
    "- A similar process is used for bagged classification trees, except we use the Gini index instead of the mean squared error.\n",
    "\n",
    "(We'll see an example of this below.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "Random Forests is a **slight variation of bagged trees** that has even better performance! Here's how it works:\n",
    "\n",
    "- Exactly like bagging, we create an ensemble of decision trees using bootstrapped samples of the training set.\n",
    "- However, when building each tree, **each time a split is considered**, a random sample of m predictors is chosen as split candidates from the full set of p predictors. **The split is only allowed to use one of those m predictors.**\n",
    "\n",
    "Notes:\n",
    "\n",
    "- A new random sample of predictors is chosen for **every single tree at every single split**.\n",
    "- For **classification**, m is typically chosen to be the square root of p. For **regression**, m is typically chosen to be somewhere between p/3 and p.\n",
    "\n",
    "What's the point?\n",
    "\n",
    "- Suppose there is one very strong predictor in the data set. When using bagged trees, most of the trees will use that predictor as the top split, resulting in an ensemble of similar trees that are \"highly correlated\".\n",
    "- Averaging highly correlated quantities does not significantly reduce variance (which is the entire goal of bagging).\n",
    "- **By randomly leaving out candidate predictors from each split, Random Forests \"decorrelates\" the trees**, such that the averaging process can reduce the variance of the resulting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>name</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>ticket</th>\n",
       "      <th>fare</th>\n",
       "      <th>cabin</th>\n",
       "      <th>embarked</th>\n",
       "      <th>embarked_Q</th>\n",
       "      <th>embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>1</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>0</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>0</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>0</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>1</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Moran, Mr. James</td>\n",
       "      <td>1</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330877</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>McCarthy, Mr. Timothy J</td>\n",
       "      <td>1</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17463</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>E46</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Palsson, Master. Gosta Leonard</td>\n",
       "      <td>1</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>349909</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)</td>\n",
       "      <td>0</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>347742</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Nasser, Mrs. Nicholas (Adele Achem)</td>\n",
       "      <td>0</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>237736</td>\n",
       "      <td>30.0708</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived  pclass                                               name  sex  \\\n",
       "0         0       3                            Braund, Mr. Owen Harris    1   \n",
       "1         1       1  Cumings, Mrs. John Bradley (Florence Briggs Th...    0   \n",
       "2         1       3                             Heikkinen, Miss. Laina    0   \n",
       "3         1       1       Futrelle, Mrs. Jacques Heath (Lily May Peel)    0   \n",
       "4         0       3                           Allen, Mr. William Henry    1   \n",
       "5         0       3                                   Moran, Mr. James    1   \n",
       "6         0       1                            McCarthy, Mr. Timothy J    1   \n",
       "7         0       3                     Palsson, Master. Gosta Leonard    1   \n",
       "8         1       3  Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)    0   \n",
       "9         1       2                Nasser, Mrs. Nicholas (Adele Achem)    0   \n",
       "\n",
       "         age  sibsp  parch            ticket     fare cabin embarked  \\\n",
       "0  22.000000      1      0         A/5 21171   7.2500   NaN        S   \n",
       "1  38.000000      1      0          PC 17599  71.2833   C85        C   \n",
       "2  26.000000      0      0  STON/O2. 3101282   7.9250   NaN        S   \n",
       "3  35.000000      1      0            113803  53.1000  C123        S   \n",
       "4  35.000000      0      0            373450   8.0500   NaN        S   \n",
       "5  29.699118      0      0            330877   8.4583   NaN        Q   \n",
       "6  54.000000      0      0             17463  51.8625   E46        S   \n",
       "7   2.000000      3      1            349909  21.0750   NaN        S   \n",
       "8  27.000000      0      2            347742  11.1333   NaN        S   \n",
       "9  14.000000      1      0            237736  30.0708   NaN        C   \n",
       "\n",
       "   embarked_Q  embarked_S  \n",
       "0           0           1  \n",
       "1           0           0  \n",
       "2           0           1  \n",
       "3           0           1  \n",
       "4           0           1  \n",
       "5           1           0  \n",
       "6           0           1  \n",
       "7           0           1  \n",
       "8           0           1  \n",
       "9           0           0  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the Titanic data\n",
    "titanic = pd.read_csv('titanicForEnsemble.csv')\n",
    "\n",
    "# encode sex feature\n",
    "titanic['sex'] = titanic.sex.map({'female':0, 'male':1})\n",
    "\n",
    "# fill in missing values for age\n",
    "titanic.age.fillna(titanic.age.mean(), inplace=True)\n",
    "\n",
    "# create three dummy variables, drop the first dummy variable, and store this as a DataFrame\n",
    "embarked_dummies = pd.get_dummies(titanic.embarked, prefix='embarked').iloc[:, 1:]\n",
    "\n",
    "# concatenate the two dummy variable columns onto the original DataFrame\n",
    "# note: axis=0 means rows, axis=1 means columns\n",
    "titanic = pd.concat([titanic, embarked_dummies], axis=1)\n",
    "\n",
    "# create a list of feature columns\n",
    "feature_cols = ['pclass', 'sex', 'age', 'embarked_Q', 'embarked_S']\n",
    "\n",
    "# print the updated DataFrame\n",
    "titanic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features=3, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
       "            oob_score=True, random_state=1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import class, instantiate estimator, fit with all data\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfclf = RandomForestClassifier(n_estimators=100, max_features=3, oob_score=True, random_state=1)\n",
    "rfclf.fit(titanic[feature_cols], titanic.survived)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the most important tuning parameters for Random Forests:\n",
    "\n",
    "- **n_estimators:** more estimators (trees) increases performance but decreases speed\n",
    "- **max_features:** cross-validate to choose an ideal value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pclass</td>\n",
       "      <td>0.159183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sex</td>\n",
       "      <td>0.374466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>age</td>\n",
       "      <td>0.424082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>embarked_Q</td>\n",
       "      <td>0.012104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>embarked_S</td>\n",
       "      <td>0.030164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature  importance\n",
       "0      pclass    0.159183\n",
       "1         sex    0.374466\n",
       "2         age    0.424082\n",
       "3  embarked_Q    0.012104\n",
       "4  embarked_S    0.030164"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the feature importances\n",
    "pd.DataFrame({'feature':feature_cols, 'importance':rfclf.feature_importances_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.797979797979798"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the out-of-bag classification accuracy\n",
    "rfclf.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping up ensembling\n",
    "\n",
    "Ensembling is incredibly popular, when the **primary goal is predictive accuracy**. For example, the team that eventually won the $1 million [Netflix Prize](http://en.wikipedia.org/wiki/Netflix_Prize) used an [ensemble of 107 models](http://www2.research.att.com/~volinsky/papers/chance.pdf) early on in the competition.\n",
    "\n",
    "There was a recent paper in the Journal of Machine Learning Research titled \"[Do We Need Hundreds of Classifiers to Solve Real World Classification Problems?](http://jmlr.csail.mit.edu/papers/volume15/delgado14a/delgado14a.pdf)\" (**Spoiler alert:** Random Forests did very well.) In the [comments about the paper](https://news.ycombinator.com/item?id=8719723) on Hacker News, Ben Hamner (Kaggle's chief scientist) said the following:\n",
    "\n",
    "> This is consistent with our experience running hundreds of Kaggle competitions: for most classification problems, some variation on ensembled decision trees (random forests, gradient boosted machines, etc.) performs the best. This is typically in conjunction with clever data processing, feature selection, and internal validation.\n",
    "\n",
    "> One key exception is where the data is richly and hierarchically structured. Text, speech, and visual data falls under this category. In many cases here, variations of neural networks (deep neural nets/CNN's/RNN's/etc.) provide very dramatic improvements.\n",
    "\n",
    "But as you can imagine, ensembling may not often be practical in a real-time environment.\n",
    "\n",
    "**You can also build your own ensembles:** just build a variety of models and average them together! Here are some strategies for building independent models:\n",
    "\n",
    "- using different models\n",
    "- choosing different combinations of features\n",
    "- changing the tuning parameters\n",
    "\n",
    "Note that there is an entire class of well-known ensembling methods that we did not discuss, namely **boosting**. Instead of building independent models and averaging the predictions, the models are built sequentially on repeatedly modified versions of the data. More information is available in the scikit-learn documentation on Ensemble Methods, namely the sections on [AdaBoost](http://scikit-learn.org/stable/modules/ensemble.html#adaboost) and [Gradient Tree Boosting](http://scikit-learn.org/stable/modules/ensemble.html#gradient-tree-boosting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- scikit-learn documentation: [Ensemble Methods](http://scikit-learn.org/stable/modules/ensemble.html)\n",
    "- Quora: [How do random forests work in layman's terms?](http://www.quora.com/How-do-random-forests-work-in-laymans-terms/answer/Edwin-Chen-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "name": "_merged",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
